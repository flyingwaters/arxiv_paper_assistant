title: Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM
abstract: LLM已成为代码生成任务的首选，随着对LLM的训练、开发和使用在特定于代码生成方面呈指数增长。
link: http://arxiv.org/abs/2403.19114v1
publish_time: 2024-03-28

title: ReflectSumm: A Benchmark for Course Reflection Summarization
abstract: ReflectSumm是一个专门设计用于总结学生反思性写作的新型总结数据集，旨在促进开发和评估适用于现实场景、培训数据有限的新型总结技术，对于意见总结领域和教育领域具有潜在影响的实践任务。该数据集涵盖了各种总结任务，并包括了全面的元数据，可用于探索各种研究问题和支持不同的应用。我们通过使用多个最先进的基准模型进行了广泛的评估来展示其实用性。结果为进一步研究提供了基准。
link: http://arxiv.org/abs/2403.19012v1
publish_time: 2024-03-27

title: The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian
abstract: 尽管意大利语在各项指标上都是一种高资源语言，但当前并没有一种专门针对该语言进行预训练的语言模型。这导致意大利语的可用基准测试数量较少，难以评估语言模型在意大利语中的性能。本研究提出了两个新的基准测试，用于评估意大利语中的数学理解和语言理解模型的表现。这些基准测试基于意大利学校教育体系中11到18岁学生所进行的真实测试，并得到了多位教学和教育专家的认可。为了验证这一数据集，我们评估了9种在意大利语写作中表现最佳的语言模型的表现，包括我们自己调整优化的模型。我们发现这是一个具有挑战性的基准测试，目前的语言模型的准确率被限制在60％。我们相信这个数据集的发布为改善未来语言模型在意大利语中的数学和语言理解铺平了道路。
link: http://arxiv.org/abs/2403.18697v1
publish_time: 2024-03-27

title: RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers
abstract: Transformer 的结构在多个应用机器学习领域取得了巨大成功，如自然语言处理（NLP）、计算机视觉（CV）和信息检索（IR）。Transformer 架构的核心机制 -- attention 在训练中需要 $O(n^2)$ 的时间复杂度，在推理过程中需要 $O(n)$ 的时间复杂度。许多工作已经提出来改进 attention 机制的可扩展性，例如 Flash Attention 和 Multi-query Attention。另一方面，一些工作着眼于设计新的机制来替代 attention。最近，一种显著的模型结构 -- Mamba，基于状态空间模型，在多个序列建模任务中实现了与 transformer 等效的性能。在这项工作中，我们通过一个经典的 IR 任务 -- 文档排序，来检验 Mamba 的效能。重新排序模型以查询和文档作为输入，并预测一个标量相关性分数。这项任务需要语言模型理解长篇上下文输入，并捕捉查询和文档标记之间的交互。我们发现 (1) Mamba 模型与具有相同训练方法的 transformer 模型相比具有竞争力的性能；(2) 但与如 Flash Attention 等高效 transformer 实现相比，训练吞吐量较低。希望这项研究可以作为探索 Mamba 模型在其他经典 IR 任务中的起点。我们的代码实现和训练检查点已经公开，以促进可重复性。\url{https://github.com/zhichaoxu-shufe/RankMamba}。
link: http://arxiv.org/abs/2403.18276v1
publish_time: 2024-03-27

title: NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions
abstract: 自动事实检查在数字时代应对不断增长的错误信息问题中引起了极大关注。现有系统主要关注维基百科上的合成声明，并在真实世界声明方面取得了显著进展。我们发布了Numtemp，一个专注于数字声明的多样化、多领域数据集，包括时间、统计和多样化方面，具有细粒度的元数据和不泄露的证据收集。这解决了验证真实世界数字声明的挑战，这些声明往往复杂且缺乏精确信息，而现有工作主要关注合成声明未能解决这个问题。我们评估并量化了现有解决方案在验证数字声明任务中的局限性。我们还评估了基于声明分解的方法、基于数字理解的模型，我们最佳基线达到了58.32的宏F1分数。这表明Numtemp作为一个具有挑战性的数字声明验证评估集。
link: http://arxiv.org/abs/2403.17169v1
publish_time: 2024-03-25

title: A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark
abstract: 本研究探讨了在BERT模型微调过程中，注意力得分是否根据词类别在下游任务中显著变化。通过人类语言处理的概念启发，我们将句子中的词按词类别分类，关注不同类别之间的注意力得分变化。我们的假设是，在强调语义信息的下游任务中，内容词的注意力得分会增强，而在强调句法信息的任务中，功能词的注意力得分会增强。通过对GLUE基准数据集中六个任务的实验验证了我们的假设。此外，我们的额外研究发现，BERT层始终对特定词类别赋予更多偏好，无论任务是什么，突显出了存在任务无关的词类别偏好的现象。
link: http://arxiv.org/abs/2403.16447v1
publish_time: 2024-03-25

title: VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding
abstract: 各国语言的自然语言理解基准的成功，如英语的GLUE、中文的CLUE、韩语的KLUE和印尼语的IndoNLU，有助于评估各种任务中新的NLU模型。我们引入了第一个越南语语言理解评估(VLUE)基准来建立越南语NLU的标准化基准。 VLUE基准包括五个数据集，涵盖不同的NLU任务，包括文本分类、跨度提取和自然语言理解。我们评估了七种最先进的预训练模型，包括多语种和越南语单语种模型，在我们提出的VLUE基准上，以提供对当前越南语NLU状况的深入了解。此外，我们提出了CafeBERT，一个在VLUE基准上的所有任务都取得卓越成绩的新的最先进预训练模型。我们的模型结合了多语种预训练模型的熟练和越南语言知识。 CafeBERT基于XLM-RoBERTa模型开发，采用了额外的预训练步骤，利用大量的越南文本数据来增强其对越南语的适应性。为了未来研究的目的，我们将CafeBERT公开发布供研究使用。
link: http://arxiv.org/abs/2403.15882v1
publish_time: 2024-03-23

title: Construction of a Japanese Financial Benchmark for Large Language Models
abstract: 该研究证实了构建的基准测试在不同难度任务的结合下，可以有效区分各种性能水平的模型，同时确认GPT-4目前处于出色状态。
link: http://arxiv.org/abs/2403.15062v1
publish_time: 2024-03-22

title: Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
abstract: CHARM是首个全面深入评估大型语言模型（LLMs）的常识推理能力的基准，覆盖全球知名和中国特有的常识，通过5种提示策略对7个英文和12个中文导向的LLMs进行评估。我们的研究结果表明，LLM的语言方向和任务领域影响提示策略的有效性，我们发现一些LLMs在记忆中文常识方面存在困难，而其他LLMs在记忆表现相似的情况下在推理方面有差异。我们还评估了LLMs的独立于记忆的推理能力，并分析了典型错误。我们的研究准确识别了LLMs的优势和劣势，并为优化提供了清晰方向，也可成为其他领域研究的参考。我们将在https://github.com/opendatalab/CHARM 上发布CHARM。
link: http://arxiv.org/abs/2403.14112v1
publish_time: 2024-03-21

title: The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data
abstract: 这篇白皮书介绍了NeurIPS 2023机器学习音频研讨会，汇集了来自各个音频领域的机器学习专家，强调了音频领域的数据获取和任务挑战，并提供了几个开源和专有数据集以鼓励研究者利用。
link: http://arxiv.org/abs/2403.14048v1
publish_time: 2024-03-21

title: Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection
abstract: 虚假信息对社会影响严重，可影响公众舆论、机构信心和国家政治前景，因此对于乌尔都假新闻的识别和检查，有必要建立更大规模，可信度高的数据集来提高检测的精准度。
link: http://arxiv.org/abs/2403.14037v1
publish_time: 2024-03-20

title: LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models
abstract: 汉语大型语言模型（LLM）最近在各种自然语言处理基准测试和实际应用中展现出令人印象深刻的能力。然而，目前用于全面评估这些LLM的基准测试仍然不足，特别是在衡量LLM捕捉的知识方面。当前的数据集收集了来自不同科目和教育水平的中国考试题目，以解决这个问题。然而，这些基准测试主要侧重于客观题，如选择题，导致问题类型缺乏多样性。为了解决这个问题，我们在本文中提出了LHMKE，这是一个大规模、全面和多科目知识评估基准测试。LHMKE旨在全面评估中国LLMs的知识获取能力。它包含了10465个问题，涵盖了30个学科的75个任务，从小学到专业认证考试。值得注意的是，LHMKE包括客观和主观题，提供了对LLM知识水平更全面的评估。我们在零样本设置下评估了11个中国LLM，并比较了它们在不同学科下的表现。我们还进行了深入分析，检查GPT-4是否能自动评分主观预测。我们的研究结果表明，LHMKE是一个对中国LLM而言具有挑战性和先进的测试平台。
link: http://arxiv.org/abs/2403.12601v1
publish_time: 2024-03-19

title: AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework
abstract: 机器学习和深度学习算法在股市趋势预测方面取得了显著进展，但缺乏解释性和推理过程，也无法整合文本信息。大型语言模型在文本理解和生成方面具有显着能力，但由于金融训练数据集稀缺，与实时知识整合有限，仍存在幻觉问题。为解决这些挑战，我们发布了AlphaFin数据集，并使用它来评估一种名为Stock-Chain的最先进方法，以有效解决金融分析任务，其中整合了检索增强生成技术。 extensive实验证实了我们框架在金融分析方面的有效性。
link: http://arxiv.org/abs/2403.12582v1
publish_time: 2024-03-19

title: OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety
abstract: 中文大语言模型的快速发展，对于高效评估提出了巨大挑战。目前的评估倡议引入了新的基准或评估平台，用于评估中文大语言模型，但很多只着重于能力，通常忽视了潜在的对齐和安全问题。为了弥补这一不足，我们引入了OpenEval，一个评估测试平台，评估中文大语言模型的能力、对齐性和安全性。
link: http://arxiv.org/abs/2403.12316v1
publish_time: 2024-03-18

title: NovelQA: A Benchmark for Long-Range Novel Question Answering
abstract: 大型语言模型（LLMs）的快速发展在自然语言处理中开辟了一个新领域，尤其是在理解和处理长篇内容信息方面。然而，由于当前基准测试的限制，这些模型的长文本能力评估仍然是一个挑战。为了填补这一空白，我们引入了NovelQA，这是一个专门设计用于测试LLMs在扩展文本中的能力的基准。NovelQA由英语小说构成，提供了复杂性、长度和叙事连贯性的独特组合，是评估LLMs深度文本理解的理想工具。本文介绍了NovelQA的设计和构建，重点介绍了其手动标注和多样的问题类型。我们在NovelQA上对长文本LLMs的评估揭示了对模型表现的重要见解，特别强调了它们在多跳推理、注重细节的问题和超过100,000标记的极长输入方面面临的挑战。结果强调了进一步改进LLMs以提高其长文本理解和计算文学研究的必要性。
link: http://arxiv.org/abs/2403.12766v1
publish_time: 2024-03-18

title: Tur[k]ingBench: A Challenge Benchmark for Web Agents
abstract: 最新的聊天机器人展示出了在原始文本形式中理解和交流的能力，但世界不仅仅是原始文本。人类花费大量时间在网页上，文本与其他形式交织在一起，任务通过各种复杂互动完成。我们引入了TurkingBench，一个任务以包含文本说明和多模态上下文的网页形式的基准。这个基准包含32.2K个示例，分布在158个任务中。我们还开发了一个评估框架，连接聊天机器人的响应和网页上的修改，对语言、视觉、版式和它们的组合模型在这个基准上的表现进行评估。我们的研究表明这些模型明显优于随机，但还有改进的空间。希望这个基准可以促进基于网络的代理的评估和发展。
link: http://arxiv.org/abs/2403.11905v2
publish_time: 2024-03-18

title: m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
abstract: 现实世界中的多模态问题很少能被单一的机器学习模型解决，通常需要涉及到多个模型的多步计算方案。工具增强型LLMs在自动化生成这种计算方案方面有巨大潜力。然而，缺乏用于评估LLMs作为多步多模任务计划器的标准基准，阻碍了对计划设计决策的系统研究。LLMs应该一次性生成完整计划还是逐步生成?它们应该直接使用Python代码调用工具还是通过结构化数据格式如JSON?反馈是否能改善规划?为了回答这些问题，我们引入了 m&m's：一个包含4K+多步多模任务的基准，涉及33个工具，包括多模型、（免费）公共API和图像处理模块。对于每一个任务查询，我们提供了使用这种逼真工具集自动生成的计划。我们还提供了一个高质量子集的1,565个任务计划，经过人工验证并可正确执行。通过m&m's，我们评估了6种流行的LLMs，采用了2种规划策略（多步vs.逐步规划），2种计划格式（JSON vs.代码）和3种反馈类型（解析/验证/执行）。最后，我们总结了我们广泛实验的结果。我们的数据集和代码可在HuggingFace（https://huggingface.co/datasets/zixianma/mnms）和Github（https://github.com/RAIVNLab/mnms）上获得。
link: http://arxiv.org/abs/2403.11085v3
publish_time: 2024-03-17

title: DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages
abstract: 语言技术的评估应该基于其在真实用例中的有效性。
link: http://arxiv.org/abs/2403.11009v1
publish_time: 2024-03-16

title: MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations
abstract: MIntRec2.0是一个大规模的多模态意图识别基准数据集，为研究人机对话互动提供了重要资源。
link: http://arxiv.org/abs/2403.10943v2
publish_time: 2024-03-16

title: BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English
abstract: 本研究介绍了BEnQA数据集，包含了孟加拉国中学和高中水平的平行孟加拉语和英语考试题。我们的数据集包含大约5K个科学科目的问题，涵盖了事实、应用和基于推理的不同类型的问题。我们使用我们的平行数据集对几个大型语言模型（LLMs）进行基准测试，并观察到这些模型在孟加拉语和英语方面存在显著的性能差异。我们还研究了一些提示方法，发现思维链提示方法在推理问题上有益，但在事实问题上并不那么有效。我们还发现添加英语翻译有助于回答孟加拉语问题。我们的发现指向了未来改进LLMs在孟加拉语和更普遍地在低资源语言中性能的有希望的研究方向。
link: http://arxiv.org/abs/2403.10900v1
publish_time: 2024-03-16

title: Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study
abstract: 通过从原始文本中预训练图像表示，使得零样本视觉转移至下游任务成为可能，CLIP等多模态基础模型取得竞争力十足的零样本结果，同时在分类准确度上表现出色并在自然分布转移下弥合了鲁棒性差距。然而，我们的评估结果表明，对于大规模鲁棒性基准测试，特别是在合成分布转移和对抗性攻击下，CLIP相比于监督训练的ImageNet模型存在显著的鲁棒性下降，而数据重叠分析则表明，在自然分布转移下观察到的鲁棒性至少在一定程度上要归功于数据重叠。因此，我们认为需要对零样本多模态模型的鲁棒性进行全面评估并进一步改进。
link: http://arxiv.org/abs/2403.10499v1
publish_time: 2024-03-15

title: EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models
abstract: EXAMS-V是一个新颖的、具有挑战性的多学科、多模态、多语言的考试基准，用于评估视觉语言模型，在各个领域的考试题目，包含文本、图片、表格、图表等多种信息，跨越11种语言，难度高且具备重要意义。
link: http://arxiv.org/abs/2403.10378v1
publish_time: 2024-03-15

title: MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection
abstract: 众多在线信息源中存在的假新闻对公众产生了显著影响，目前已有的中国假新闻检测数据集局限于仅来自微博的新闻。然而，来自多个来源的假新闻在内容和社会背景等方面呈现出多样性。纯粹依靠单一新闻来源训练的方法几乎无法适用于真实场景。为了解决这一局限性，我们构建了首个面向多来源的中文假新闻检测基准数据集MCFEND。此数据集由来自社交平台、消息应用和传统在线新闻网站等多种来源的新闻组成，这些新闻已经通过全球14家权威事实核查机构实事核实。MCFEND作为一个基准数据集旨在推动中文假新闻检测方法在真实场景中的应用。
link: http://arxiv.org/abs/2403.09092v1
publish_time: 2024-03-14

title: LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots
abstract: ChatGPT的突破引起了大型语言模型（LLMs）在研究界的重大关注，随着LLMs的发展，对话模型的文本风格转移问题已成为一个自然的延伸，聊天机器人可能拥有自己的风格甚至角色。LMStyle基准提出了一个创新的评估框架，适用于聊天风格文本风格转移（C-TST），可以自动化和可扩展地衡量LLMs的风格转移质量，并考虑了一系列新颖的指标来衡量适当性，这些指标不仅包括了传统的风格强度指标，还考虑了连贯性、流畅性等隐含因素。我们的实验表明，LMStyle基准引入的新评估方法在适当性方面与人类判断有更高的相关性。基于LMStyle基准，我们展示了流行的LLMs（包括LLaMA、Alpaca和Vicuna）的评估结果，反映了它们的风格特性，如正式度和情感强度，以及它们的适当性。
link: http://arxiv.org/abs/2403.08943v1
publish_time: 2024-03-13

title: DevBench: A Comprehensive Benchmark for Software Development
abstract: 最近发展的大型语言模型(Large Language Models, LLMs)显著增强了它们的编码能力，但现有基准主要专注于编程的简化或孤立方面，如单文件代码生成或存储库问题调试，未能全面评估真实世界编程活动所引发的挑战。因此，我们提出了DevBench，一个全面评估LLMs在软件开发生命周期各阶段的基准，包括软件设计、环境设置、实现、验收测试和单元测试。DevBench涵盖了广泛的编程语言和领域，拥有高质量的数据采集，并为每个任务精心设计和验证了指标。实证研究显示，包括GPT-4-Turbo在内的当前LLMs在解决DevBench中的挑战上存在失败。分析表明，模型在理解存储库中的复杂结构、管理编译过程和掌握高级编程概念方面存在困难。我们的发现为未来LLMs向真实世界编程应用的发展提供了可操作的见解。我们的基准可在https://github.com/open-compass/DevBench 获取。
link: http://arxiv.org/abs/2403.08604v2
publish_time: 2024-03-13

title: FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models
abstract: 为了全面评估大型语言模型（LLMs）的数学推理能力，我们需要仔细策划涵盖不同难度水平和数学概念的评估数据集。FineMath是为了评估中文LLMs而提出的细粒度数学评估基准数据集，覆盖了小学数学中的主要数学概念，进一步划分为17个数学问题类别，可以深入分析LLMs的数学推理能力。
link: http://arxiv.org/abs/2403.07747v1
publish_time: 2024-03-12

title: StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
abstract: 近年来，大型语言模型（LLMs）取得了显著进展，引发了对工具学习的探索，将LLMs与外部工具整合以解决多样化的现实挑战。评估LLMs利用工具的能力需要大规模且稳定的基准测试。然而，先前的工作要么依赖于具有有限规模的手工在线工具，要么依赖于遭受API状态不稳定性的大规模真实在线APIs。为了解决这个问题，我们引入了StableToolBench，这是从ToolBench演变而来的基准测试，提出了虚拟API服务器和稳定评估系统。虚拟API服务器包含缓存系统和API模拟器，可相辅相成地缓解API状态变化。同时，稳定评估系统设计了可解决的通过率和胜率，并使用GPT-4作为自动评估器消除评估过程中的随机性。实验结果表明了StableToolBench的稳定性，并进一步探讨了API模拟器、缓存系统和评估器系统的有效性。
link: http://arxiv.org/abs/2403.07714v2
publish_time: 2024-03-12

title: KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
abstract: 目前，关于大规模视觉-语言模型（LVLMs）的知识编辑研究较少。编辑LVLMs面临挑战，需要有效整合多种模式（图像和文本），同时确保修改一致且与上下文相关。现有基准有三个指标（可靠性、局部性和普适性）用于衡量LVLMs的知识编辑。然而，该基准在评估中使用的生成图像质量不高，且无法评估模型是否有效利用编辑后的知识与相关内容相结合。我们采用不同的数据收集方法构建了一个新的基准，$\textbf{KEBench}$，并扩展了新的度量标准（可移植性）进行全面评估。利用多模式知识图，我们的图像数据展示了明确的实体方向性。这种方向性可以进一步用于提取与实体相关的知识并形成编辑数据。我们在五个LVLMs上进行了不同编辑方法的实验，并彻底分析了这些方法对模型的影响。结果揭示了这些方法的优势和不足之处，并希望为未来研究提供潜在的思路。
link: http://arxiv.org/abs/2403.07350v1
publish_time: 2024-03-12

title: GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method
abstract: 随着自然语言模型（如ChatGPT）在应用和服务中变得越来越普遍，检测它们输出的稳健准确方法的需求至关重要。本文介绍了GPT Reddit数据集（GRiD），这是一个设计用于评估检测模型在识别ChatGPT生成响应方面表现的新颖数据集。该数据集基于Reddit构建，包含人类生成和ChatGPT生成的各种语境提示对。我们对数据集的特征进行了分析，包括语言多样性、语境复杂性和响应质量。为展示数据集的实用性，我们在其上对几种检测方法进行了基准测试，展示它们在区分人类和ChatGPT生成响应方面的有效性。这个数据集为在ChatGPT环境中评估和推进检测技术提供了资源，并为确保互联网上AI驱动的沟通负责和可信的持续努力做出了贡献。最后，我们提出了GpTen，一种基于张量的新型GPT文本检测方法，它是一种半监督的方法，因为它只能访问人类生成文本，并且表现与全监督的基线方法相当。
link: http://arxiv.org/abs/2403.07321v1
publish_time: 2024-03-12

title: $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model
abstract: 最新的强化学习算法专注于提升规模化语言模型的性能，但缺乏成本效益和标准化的测试平台，我们提出了一个通用的$(N,K)$-拼图测试来评估和比较RL算法的效果。
link: http://arxiv.org/abs/2403.07191v1
publish_time: 2024-03-11

title: CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean
abstract: 尽管韩文大型语言模型(LLMs)的快速发展，但韩文文化和语言知识的基准数据集仍然显然缺乏，为填补这一空白，我们引入了文化和语言智力基准数据集CLIcK，对LLMs在韩文文化和语言方面的表现进行了全面的测试和评估。
link: http://arxiv.org/abs/2403.06412v3
publish_time: 2024-03-11

title: No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks
abstract: 大型语言模型的发展推动了金融分析，但目前主要集中在单一语言领域，未发挥中英双语潜力。ICE-PIXIU整合了ICE-INTENT模型和ICE-FLARE基准，为中英双语金融分析提供无缝支持，丰富了双语金融建模的广度和深度。ICE-PIXIU集成了一系列中文任务，以及翻译和原始英文数据集，为双语金融建模提供了丰富的资源。这一研究强调融入双语数据集的优势，尤其在翻译任务和利用原始英文数据方面，提升了金融文本分析的灵活性和准确性。ICE-INTENT在双语环境中得到显著改进，凸显了强大的双语数据对金融NLP准确性和有效性的深远影响。
link: http://arxiv.org/abs/2403.06249v1
publish_time: 2024-03-10

title: A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
abstract: 大型语言模型在自动化实际任务中展现出卓越能力，但在医疗领域如病程简要(BHC)合成方面的应用尚未被证明。本研究引入了一个新的基准，评估了不同LLM在BHC合成中的性能，结果显示LLM在临床信息和BHC合成方面表现出高质量的总结能力。
link: http://arxiv.org/abs/2403.05720v1
publish_time: 2024-03-08

title: ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
abstract: 大型语言模型在各种应用中取得了前所未有的性能，但其评估仍然是一个关键问题。我们提出利用现有的关系数据库构建基准测试是一种有前途的方法，因为它们通过函数依赖提供了准确的知识描述。ERBench可以自动将任何关系数据库转换为基于实体-关系模型的基准测试，并支持持续评估、多模态问题和各种提示工程技术。
link: http://arxiv.org/abs/2403.05266v1
publish_time: 2024-03-08

title: CommitBench: A Benchmark for Commit Message Generation
abstract: 写提交信息是许多软件开发人员的烦人日常任务，往往被忽视。自动化这一任务有潜力节省时间，同时确保信息丰富。一个高质量的数据集和客观的评估标准对于实现这一目标至关重要。
link: http://arxiv.org/abs/2403.05188v1
publish_time: 2024-03-08

title: Speech Robust Bench: A Robustness Benchmark For Speech Recognition
abstract: 随着自动语音识别（ASR）模型变得越来越普遍，确保它们在物理世界和数字世界中存在的各种干扰下能够做出可靠的预测变得至关重要。我们提出了语音鲁棒性基准（SRB），这是一个用于评估ASR模型对各种干扰的鲁棒性的综合基准。 SRB由69种输入扰动组成，旨在模拟ASR模型可能在物理世界和数字世界中遇到的各种干扰。我们使用SRB评估了几种最先进的ASR模型的鲁棒性，并观察到模型大小和某些建模选择（如离散表示和自我训练）似乎有助于提高鲁棒性。我们扩展了这一分析，以测量ASR模型在各种人口亚组（如英语和西班牙语说话者、男性和女性）的数据上的鲁棒性，并观察到了不同群体之间模型鲁棒性的显着差异。我们相信SRB将促进未来关于鲁棒ASR模型的研究，使全面且可比较的鲁棒性评估变得更加容易。
link: http://arxiv.org/abs/2403.07937v1
publish_time: 2024-03-08

title: NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems
abstract: 随着基于transformer架构的发展，我们看到自然语言预处理工具的兴起，能够解决NLP任务中的初步问题，而无需外部语言指导。现有的NLP预处理评估方法存在一些缺陷，因此我们提出了一种可靠且公正的评估和性能报告方法。这个基于语言的基准系统启用了全面的多个NLPre工具的持续评估，同时可靠地跟踪它们的性能。我们在这个基准系统的基础上对多种波兰NLPre系统进行了广泛评估，以促进其他语言的基准环境的构建，我们确保了公开发布的基准系统源代码的完全定制化。
link: http://arxiv.org/abs/2403.04507v2
publish_time: 2024-03-07

title: A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain
abstract: 本工作旨在引入一个全面的基准，专门针对阿拉伯语电话会话的挑战进行定制。阿拉伯语以其丰富的方言多样性和语音复杂性而著称，这给自动语音识别系统带来了许多独特的挑战。电话通话领域中这些挑战被进一步放大，其中音频质量、背景噪音和对话风格的影响消极地影响了识别准确性。我们的工作旨在建立一个健壮的基准，不仅囊括广泛的阿拉伯方言，而且模拟呼叫通信的真实世界条件。通过融入多样的方言表达，并考虑呼叫录音的可变质量，这个基准旨在为开发和评估能够应对阿拉伯语电话交流复杂性的语音识别系统提供严格的测试平台。此外，本工作还尝试利用最先进的语音识别技术建立一个基线性能评估。
link: http://arxiv.org/abs/2403.04280v1
publish_time: 2024-03-07

title: PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion
abstract: 大型语言模型（LLMs）在完成用户指导中的不断依赖，需要全面了解它们对真实世界复杂任务完成的稳健性。我们提出了PowerPoint任务完成稳健性基准（PPTC-R）来衡量LLMs对用户PPT任务指导和软件版本的稳健性。我们构建了对抗性用户指导，在句子、语义和多语言级别攻击用户指导。为了评估语言模型对软件版本的稳健性，我们改变提供的API数量，模拟最新版本和早期版本设置。我们使用涵盖这些稳健性设置的基准测试了3个闭源和4个开源LLMs，旨在评估偏差如何影响LLMs的API调用以完成任务。我们发现GPT-4在我们的基准测试中表现最好，特别是在版本更新和多语言设置中表现强劲。然而，当同时面临多个挑战（如多轮对话）时，我们发现所有LLMs都会失去稳健性，导致性能显著下降。我们进一步分析了LLMs在我们基准测试中的稳健性行为和错误原因，为研究人员理解LLMs在任务完成中的稳健性提供了有价值的见解，并开发更稳健的LLMs和代理。我们在\url{https://github.com/ZekaiGalaxy/PPTCR}发布了代码和数据。
link: http://arxiv.org/abs/2403.03788v1
publish_time: 2024-03-06

title: Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese
abstract: 通过基于GPT-4的自我指导方法，我们成功构建了高质量的日语指示数据和评估基准，显著提高了大语言模型在非英语语言上的表现。
link: http://arxiv.org/abs/2403.03690v1
publish_time: 2024-03-06

title: Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem
abstract: 大型语言模型在各种自然语言处理任务中非常有效，但在模糊环境中容易产生不可靠的猜测，称为幻觉。本文提出了一种新方法，通过无法回答的数学应用问题（MWP）评估LLM在问答中的幻觉。我们开发了一个数据集称为无法回答的数学应用问题（UMWP），包含5200个问题跨越五个类别。我们开发了一种评估方法，结合文本相似度和数学表达式检测，以确定LLM是否认为问题无法回答。对包括GPT-3、InstructGPT、LLaMA和Claude在内的31个LLM进行了广泛实验，结果表明，在上下文学习和强化学习与人类反馈（RLHF）训练的帮助下，模型能够显著提高避免幻觉的能力。我们展示利用MWP是评估幻觉的可靠和有效方法。我们的代码和数据可以在https://github.com/Yuki-Asuuna/UMWP找到。
link: http://arxiv.org/abs/2403.03558v1
publish_time: 2024-03-06

title: CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models
abstract: 近期研究的焦点是开发具有强大长文本能力的大型语言模型，导致长文本语言模型在中文方面取得显著进展。然而，由于缺乏基准测试，这些模型的评估仍然不完善。为了弥补这一空白，我们提出了 CLongEval，一个用于评估长文本语言模型的中文基准测试。CLongEval 具有三个关键特点：(1)充足的数据量，包括7个不同任务和7,267个示例；(2)广泛适用性，适用于上下文窗口尺寸从1K到100K的模型；(3)高质量，除了自动构建的标签外，还有超过2,000个手动注释的问题-答案对。借助 CLongEval，我们对6个开源长文本语言模型和2个既具有长文本能力又在中文方面表现突出的主流商业模型进行了全面评估。我们还基于实证结果提供了深入分析，试图揭示长文本环境中的关键能力所面临的挑战。数据集、评估脚本和模型输出将被发布。
link: http://arxiv.org/abs/2403.03514v1
publish_time: 2024-03-06

title: DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
abstract: 社交媒体文本的立场检测是识别在有争议话题上持有相反意见的用户群体的下游任务中的一个关键组成部分，例如疫苗接种以及争论。
link: http://arxiv.org/abs/2403.03334v1
publish_time: 2024-03-05

title: The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
abstract: 白宫关于人工智能的行政命令强调了大型语言模型给发展生物、网络和化学武器的恶意行为带来的风险，为了评估这些恶意使用的风险，政府机构和主要人工智能实验室正在开发对LLMs中危险能力的评估。然而，目前的评估是私密的，限制了进一步研究如何减轻风险。此外，它们只关注了少数高度具体的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准测试，这是一个由4,157个多项选择题组成的数据集，作为生物安全、网络安全和化学安全中有害知识的代理测量。WMDP由一组学术界人士和技术顾问开发，严格筛选，消除了敏感信息后才公开发布。WMDP扮演两个角色：首先，作为对LLMs中有害知识的评估，其次，作为去学习方法的基准，以消除这些有害知识。为了指导去学习的进展，我们开发了CUT，一种基于控制模型表示的最新去学习方法。CUT减少了WMDP上模型的性能，同时保持了在生物学和计算机科学等领域的一般能力，这表明去学习可能是减少LLMs恶意使用的一条具体途径。我们在https://wmdp.ai上公开发布了我们的基准测试和代码。
link: http://arxiv.org/abs/2403.03218v2
publish_time: 2024-03-05

title: Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation
abstract: 大型语言模型(Large Language Models, LLMs)已成为推动文本到SQL任务的强大工具，显著胜过传统方法。然而，作为一个新兴的研究领域，还没有就最佳提示模板和设计框架达成共识。此外，现有基准测试不充分探索LLMs在文本到SQL过程中各个子任务的表现，这妨碍了评估LLMs的认知能力和LLM基于解决方案的优化。为了解决上述问题，我们首先构建了一个旨在减少LLMs过拟合风险的新数据集。然后我们制定了五项评估任务，全面评估各种LLMs在整个文本到SQL过程中的表现。我们的研究突显了LLMs之间的性能差距，并提出了针对每个任务量身定制的最佳上下文学习解决方案。这些发现为增强LLM基于文本到SQL系统的发展提供了宝贵的见解。
link: http://arxiv.org/abs/2403.02951v2
publish_time: 2024-03-05

title: InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents
abstract: 最近的工作将LLM作为代理体现出来，使他们可以访问工具，执行操作，并与外部内容互动。然而，外部内容引入了间接提示注入（IPI）攻击的风险，恶意指令被嵌入在LLM处理的内容中，旨在操纵这些代理执行对用户有害的操作。鉴于这些攻击可能带来的严重后果，建立用于评估和缓解这些风险的基准是至关重要的。我们介绍了InjecAgent，一个旨在评估LLM代理对IPI攻击的脆弱性的基准。InjecAgent包括1,054个测试用例，涵盖17种不同的用户工具和62种攻击者工具。我们将攻击意图分为两种主要类型：直接危害用户和窃取私人数据。我们评估了30种不同的LLM代理，并表明代理容易受到IPI攻击，ReAct-prompted GPT-4受攻击的成功率为24%。进一步研究增强设置，攻击者指令得到黑客指令的支持后，成功率进一步提高，几乎使对ReAct-prompted GPT-4的攻击成功率翻倍。我们的研究结果对LLM代理的广泛部署提出了疑问。我们的基准可在https://github.com/uiuc-kang-lab/InjecAgent上获得。
link: http://arxiv.org/abs/2403.02691v2
publish_time: 2024-03-05

title: Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground
abstract: 最近关注了语言模型( LM )的心灵理论( ToM )能力的评估, 许多现有的基准测试依赖于合成数据, 这可能导致实验结果与人类行为不符。我们引入了第一个基于自然发生的口语对话的 ToM 数据集 Common-ToM, 并表明 LM 在展示 ToM 方面存在困难。我们随后展示了将信念的简单显式表示集成到 LM 中可以改善 Common-ToM 上的性能。
link: http://arxiv.org/abs/2403.02451v1
publish_time: 2024-03-04

title: SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis
abstract: 近期大型语言模型（LLMs）的突破彻底改变了自然语言理解和生成，引发了对将这些技术运用于科学文献分析领域的兴趣激增。然而，现有的基准评估不足以评估LLMs在科学文献分析中的熟练程度，特别是在涉及复杂理解和多模态数据的场景中。为此，我们推出了SciAssess，一个专为深入分析科学文献而量身定制的基准，旨在全面评估LLMs的效能。SciAssess侧重评估LLMs在科学文献分析环境中记忆、理解和分析能力。它包括来自多个科学领域（如一般化学、有机材料和合金材料）的代表性任务。严格的质量控制措施确保了它在正确性、匿名化和版权合规性方面的可靠性。SciAssess评估了领先的LLMs，包括GPT-4、GPT-3.5和Gemini，鉴别了它们的优势和改进方面，支持LLMs在科学文献分析中应用的持续发展。SciAssess及其资源可在https://sci-assess.github.io免费获取，为推动LLMs在科学文献分析中的能力提供了宝贵的工具。
link: http://arxiv.org/abs/2403.01976v2
publish_time: 2024-03-04

title: NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models
abstract: 该研究引入了一个动态基准测试NPHardEval4V，旨在评估MLLMs的纯推理能力，发现不同类型模型在推理能力上存在显著差异，提出了一个更新频率为每月的基准测试以更真实、细致地评估模型，有助于指导MLLMs推理能力的进一步发展。
link: http://arxiv.org/abs/2403.01777v2
publish_time: 2024-03-04

title: KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations
abstract: 我们介绍了KorMedMCQA，这是源自韩国医疗专业人员执照考试的第一个韩语多选题答题（MCQA）基准，涵盖了2012年至2023年。这个数据集包括了医生、护士和药剂师执照考试中的一些问题，涵盖多种学科。我们对各种大型语言模型进行了基线实验，包括专有/开源、多语言/韩语附加预训练和临床背景预训练模型，突显了进一步增强的潜力。我们将我们的数据公开在HuggingFace（https://huggingface.co/datasets/sean0042/KorMedMCQA），并通过LM-Harness提供了一个评估脚本，邀请更多人在韩国医疗环境中进行探索和进步。
link: http://arxiv.org/abs/2403.01469v2
publish_time: 2024-03-03

title: MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models
abstract: 近年来，预训练的计算语言模型在利用被认为是人类独有的语言能力方面取得了显著进展。这些模型的成功引起了人们对这些模型是否像人类一样表示和处理语言的兴趣。为了回答这个问题，本文提出了MulCogBench，一个从母语为中文和英文的参与者那里收集的多模式认知基准数据集。它包含各种认知数据，包括主观语义评分、眼动、功能性磁共振成像（fMRI）和脑磁图（MEG）。为了评估语言模型与认知数据之间的关系，我们进行了一种相似性编码分析，根据文本嵌入的模式相似性来解码认知数据。结果表明，语言模型与人类认知数据存在显著相似性，并且相似性模式受数据模态和刺激复杂性的调节。具体而言，具有上下文感知能力的模型在语言刺激复杂性增加时表现出色，浅层上下文感知模型更好地与高时间分辨率的MEG信号对齐，而深层则更多地与高空间分辨率的fMRI表现出相似性。这些结果表明，语言模型与大脑语言表示存在微妙的关系。此外，中英文之间的结果高度一致，表明这些发现在不同语言之间具有普适性。
link: http://arxiv.org/abs/2403.01116v1
publish_time: 2024-03-02

title: Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks
abstract: 多模态大型语言模型（MLLMs）在需要复杂推理和语言理解的任务中已被证明有效。然而，由于除英语之外的其他语言缺乏高质量的多模态资源，MLLMs的成功仍相对局限于基于英语的环境。这给开发其他语言的可比较模型带来了巨大挑战，甚至包括像阿拉伯语这样拥有庞大说话人口的语言。为了缓解这一挑战，我们介绍了一套全面的阿拉伯语MLLMs家族，命名为“Peacock”，具有强大的视觉和语言能力。通过全面的定性和定量分析，我们展示了我们的模型在各种视觉推理任务上的出色表现，并进一步展示了它们新兴的方言潜力。此外，我们推出了一个专门设计用于评估与阿拉伯文化相关方面的MLLMs的新基准~“Henna”，为有文化意识的阿拉伯语MLLMs奠定了第一块石头。 Peacock项目的GitHub存储库可在\url {https://github.com/UBC-NLP/peacock}找到。
link: http://arxiv.org/abs/2403.01031v1
publish_time: 2024-03-01

title: DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
abstract: 大型语言模型在近年取得了显著成功，但幻觉问题仍然是一个挑战，许多基准被提出来检测幻觉。然而，一些基准不是自然生成的，而是被有意诱导出来的。许多基准仅关注事实性幻觉，却忽略了忠实性幻觉。此外，尽管对话模式在LLM时代更为广泛使用，当前的基准仅集中在句子级和段落级幻觉。在本研究中，我们提出了DiaHalu，据我们所知是第一个对话级幻觉评估基准。初步我们将收集的主题整合到系统提示中，促进两个ChatGPT3.5之间的对话。随后，我们手动修改违反人类语言规范的内容，然后让LLM重新生成，模拟真实的人机交互场景。最后，专业学者对数据集中的所有样本进行注释。DiaHalu涵盖了四个常见的多轮对话领域和五种幻觉亚型，从事实性和忠实性幻觉延伸出来。在数据集上通过一些知名的LLM和检测方法的实验表明，DiaHalu是一个具有挑战性的基准，对进一步的研究具有重要价值。
link: http://arxiv.org/abs/2403.00896v1
publish_time: 2024-03-01

title: Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs
abstract: Benchmarking是评估LLMs的事实标准，由于其速度、可复制性和低成本。然而，最近的研究指出，今天的大多数开源基准测试数据已经被污染或泄露到LLMs中，意味着LLMs在预训练和/或微调过程中可以访问测试数据。这引发了对迄今为止进行的基准研究的有效性以及未来使用基准进行评估的严重关注。为了解决这个问题，我们提出私密基准测试，这是一个解决方案，其中测试数据集被保密，模型在评估过程中不会向模型透露测试数据。我们描述了各种场景（取决于对模型所有者或数据集所有者的信任），并提出了避免数据污染的私密基准测试解决方案。对于需要保持模型权重私密的情况，我们描述了保密计算和密码学技术可以帮助私密基准测试的解决方案。最后，我们提出解决基准数据集审计问题的解决方案，以确保私密基准测试具有足够高的质量。
link: http://arxiv.org/abs/2403.00393v1
publish_time: 2024-03-01

title: Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance
abstract: 我们研究了基于LLM的零样本立场检测在推特上的表现，发现零样本方法可以与甚至优于经过微调的模型。
link: http://arxiv.org/abs/2403.00236v1
publish_time: 2024-03-01

title: Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap
abstract: 我们提出了一个框架，用于鲁棒评估语言模型的推理能力，使用基准测试的功能变体。
link: http://arxiv.org/abs/2402.19450v1
publish_time: 2024-02-29

title: GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers
abstract: 大型语言模型在各种数学推理基准测试中取得了令人印象深刻的表现，但越来越多的争论是关于这些模型是真正理解和应用数学知识，还是仅仅依赖于数学推理的捷径。这种争论的一个重要证据是，当数学问题稍微改变时，LLMs可能表现不准确。这促使我们通过测试各种问题变化来评估LLMs数学推理能力的稳健性。我们引入了对抗性小学数学(\datasetname)数据集，该数据集是在GSM8K基础上增加了各种数学扰动。我们对25个LLMs和4种提示技术进行实验表明，虽然LLMs表现出不同水平的数学推理能力，但它们的性能远非稳健。特别是，即使是在GSM8K中已经解决的问题，LLMs在添加新陈述或修改问题目标时也可能出错。我们还探讨了是否可以通过组合现有提示方法来实现更稳健的性能，我们尝试了一种迭代方法，根据其推理目标和计算结果生成并验证每个中间思路。代码和数据可在\url{https://github.com/qtli/GSM-Plus}获取。
link: http://arxiv.org/abs/2402.19255v1
publish_time: 2024-02-29

title: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark
abstract: 当前大语言模型（LLMs）研究的焦点和热点是如何更好地评价其能力。通过引入CDQA（一个包含与中国互联网最新新闻相关的问题-答案对的中国动态问答基准），我们评估和分析了主流和先进的中文LLMs，并发现CDQA具有挑战性，值得进一步研究。我们相信我们提供的基准将成为未来改进中文LLMs问答能力的关键数据资源之一。
link: http://arxiv.org/abs/2402.19248v2
publish_time: 2024-02-29

title: Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study
abstract: 综合总结对心理健康咨询的连续性至关重要，但手动总结的挑战导致专家的注意力从核心咨询过程转移。这项研究评估了最新的大型语言模型在通过基于方面的总结选择性总结治疗会话各个组成部分的有效性，旨在基准他们的表现。我们引入了MentalCLOUDS，一个由191个咨询会话组成的由三个独特咨询分量（即咨询方面）为焦点的总结数据集。此外，我们评估了11种最先进的LLMs在处理咨询中组件导向总结任务方面的能力。生成的总结通过标准总结度量定量评估，并由心理健康专业人士进行质量验证。我们的研究结果表明，专门任务的LLMs（如MentalLlama、Mistral和MentalBART）在所有咨询分量方面的标准量化指标（如Rouge-1、Rouge-2、Rouge-L和BERTScore）上表现出优异的性能。此外，专家评估显示，Mistral在情感态度、负担、道德性、连贯性、机会成本和感知有效性六个参数上超越了MentalLlama和MentalBART。然而，这些模型在机会成本和感知有效性指标方面均存在改进的潜力。
link: http://arxiv.org/abs/2402.19052v1
publish_time: 2024-02-29

title: FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability
abstract: FoFo是一个用于评估大型语言模型在遵循复杂领域特定格式能力的开创性基准，弥补了现有基准评估不足，为选择领域特定AI代理提供了指导。
link: http://arxiv.org/abs/2402.18667v1
publish_time: 2024-02-28

title: A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
abstract: 尽管大规模视觉语言模型（LVLMs）取得了一定成功，但它们在认知能力方面缺乏全面测试。我们提出了一种新的评估基准，通过使用含有丰富语义的图像来评估LVLMs的高级认知能力，结果表明LVLMs与人类之间在认知能力上仍存在很大差距。
link: http://arxiv.org/abs/2402.18409v2
publish_time: 2024-02-28

title: Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions
abstract: LLMs在回答医学问题方面表现出色，如通过医学执照考试。然而，医学委员会考试问题或一般临床问题无法捕捉实际临床案例的复杂性。缺乏参考解释意味着我们无法轻易评估模型决策的推理，这是支持医生做出复杂医学决策的关键组成部分。为了应对这些挑战，我们构建了两个新的数据集：JAMA临床挑战和Medbullets。JAMA临床挑战包含基于具有挑战性的临床案例的问题，而Medbullets包含USMLE第2和第3步风格的临床问题。这两个数据集都按照多选题回答任务的结构，每个问题都有专家撰写的解释。我们使用各种提示在这两个数据集上评估了四个LLMs。实验表明我们的数据集比以前的基准更难。模型生成的解释的自动评估和人为评估之间的不一致突出了需要开发新指标来支持未来可解释的医学问答研究。
link: http://arxiv.org/abs/2402.18060v3
publish_time: 2024-02-28

title: Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
abstract: 量化推理是分析数据的关键技能，但对这种能力的评估仍然有限。为了填补这一空白，我们引入了Quantitative Reasoning with Data（QRData）基准，旨在评估大型语言模型在统计和因果推理方面的能力。该基准包含一个精心构建的数据集，包括来自教科书、在线学习材料和学术论文的411个问题及相关数据表。为了比较模型在数据和文本上的量化推理能力，我们还在基准中加入了290个纯文本问题，即QRText。我们评估了自然语言推理、基于程序的推理和代理推理方法，包括“思维链”、“思维程序”、“ReAct”和代码解释器助手在各种模型上的表现。最强的模型GPT-4的准确率为58%，仍有很大的提升空间。在开源模型中，训练了2T令牌的代码LLM预训练模型Deepseek-coder-instruct获得了最高的37%准确率。分析表明，模型在数据分析和因果推理方面遇到困难，并且在同时使用因果知识和提供的数据时也有困难。代码和数据在https://github.com/xxxiaol/QRData。
link: http://arxiv.org/abs/2402.17644v1
publish_time: 2024-02-27

title: From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions
abstract: 本文介绍了一种新的基准YTSeg，聚焦于口语内容这一更加无序和多元化的文本领域，并提出了一种高效的层次分割模型MiniSeg，超越了现有基准模型。同时，将文本分割的概念拓展到更加实用的“智能章节”任务，包括对无序内容的分割、生成有意义的段落标题以及模型的潜在实时应用。
link: http://arxiv.org/abs/2402.17633v1
publish_time: 2024-02-27

title: OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
abstract: 虚拟代理人的自主技术代表着自动化许多繁琐任务的激动人心的一步，将赋予技术程度有限的用户完全利用计算机系统的可能性，并能通过最少人为干预实现效率提升。
link: http://arxiv.org/abs/2402.17553v2
publish_time: 2024-02-27

title: Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies
abstract: 大型语言模型（LLMs）彻底改变了自然语言处理领域，其能够通过在大规模文本语料库上获得的知识，以最少（甚至没有）调整步骤，在各种下游任务上重复使用，同时也显示了缺乏系统化概括的特点。
link: http://arxiv.org/abs/2402.17396v1
publish_time: 2024-02-27

title: KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark
abstract: 本研究介绍了KoDialogBench，一个旨在评估语言模型在韩语对话中的会话能力的基准。利用这一基准，我们对各种语言模型进行了广泛评估和分析，以衡量其对韩语对话的基本理解能力。实验结果表明，模型的会话能力仍有很大改进空间，并且我们跨不同语言模型的深入比较突显了最近的培训技术在提高会话熟练度方面的有效性。我们期待KoDialogBench将促进朝着会话感知的韩语语言模型的进步。
link: http://arxiv.org/abs/2402.17377v1
publish_time: 2024-02-27

title: Benchmarking Data Science Agents
abstract: 在数据驱动决策的时代，数据分析的复杂性需要先进的数据科学专业知识和工具，即使对专家来说也面临着重大挑战。大型语言模型(LLMs)作为数据科学代理人已经出现，可以帮助人类进行数据分析和处理。然而，它们在实际应用中的效力仍受到各种需求和复杂分析过程的限制。本文介绍了DSEval — 一种新颖的评估范式，以及一系列针对整个数据科学生命周期评估这些代理性能的创新基准。我们采用一种新颖的自举注释方法，简化数据集准备，提高评估覆盖范围，扩展基准的全面性。我们的研究发现了普遍存在的障碍，并提供了关键见解，以指导未来在该领域的进步。
link: http://arxiv.org/abs/2402.17168v1
publish_time: 2024-02-27

title: Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset
abstract: 本研究通过解释性和公平性探讨了法律判决预测模型中的例外性的重要性，利用了瑞士判决预测数据集，评估了最新的单语和多语BERT模型的解释性能，并引入了新的评估框架来揭示当前模型的偏见。
link: http://arxiv.org/abs/2402.17013v1
publish_time: 2024-02-26

title: Benchmarking LLMs on the Semantic Overlap Summarization Task
abstract: Semantic Overlap Summarization (SOS)是一个约束性的多文档总结任务，其约束是捕捉两个备选叙述之间的共同/重叠信息。最近大型语言模型（LLMs）在许多总结任务中取得了卓越性能，但尚未对LLMs在SOS任务上进行基准研究。由于LLMs的响应对提示设计的轻微变化敏感，进行这样的基准研究的主要挑战是在得出可靠结论之前系统地探索各种提示。幸运的是，最近提出了TELeR分类法，可用于为LLMs设计和探索各种提示。本文利用这个TELeR分类方法和15种流行的LLMs全面评估LLMs在SOS任务中的能力，评估它们从多个备选叙述中总结重叠信息的能力。为评估，我们在两种不同的备选叙述数据集上报告了诸如ROUGE、BERTscore和SEM-F1的成熟度量标准。最后，我们通过分析各种LLMs在捕捉重叠信息方面的能力的优势和局限性来总结本文。本研究使用的代码和数据集可在https://anonymous.4open.science/r/llm_eval-E16D 上获得。
link: http://arxiv.org/abs/2402.17008v1
publish_time: 2024-02-26

title: HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization
abstract: 大型语言模型在从文本提示中生成代码方面取得了显著进展，但现有的基准主要集中在将英语提示翻译成多语言代码，或者限制在非常有限的自然语言中。在评估多语言语言模型时，这些基准忽视了广阔的大规模多语言自然语言到多语言代码的领域，留下了一个关键的空白。我们引入HumanEval-XL，这是一个专门设计来解决这个缺陷的大规模多语言代码生成基准。HumanEval-XL建立了23种自然语言和12种编程语言之间的联系，包括22,080个提示，平均每个提示有8.33个测试用例。通过确保在多个自然语言和编程语言之间的平行数据，HumanEval-XL为多语言语言模型提供了全面的评估平台，允许评估对不同自然语言的理解。我们的工作是填补在多语言代码生成领域中对自然语言泛化评估的空白的开创性一步。我们将我们的评估代码和数据公开发布在https://github.com/FloatAI/humaneval-xl。
link: http://arxiv.org/abs/2402.16694v2
publish_time: 2024-02-26

title: MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property
abstract: 大型语言模型在各种自然语言处理任务中展现出令人印象深刻的表现，但在特定领域（如知识产权领域）中的表现仍有限。本文提出了一个新的基准，即第一个面向知识产权的多语言问答基准（MoZIP），用于评估LLMs在知识产权领域的表现。MoZIP基准包括三个挑战性任务：知识产权多项选择题（IPQuiz）、知识产权问答（IPQA）和专利匹配（PatentMatch）。此外，我们还开发了一个新的IP定向多语言大型语言模型（称为MoZi），这是一个经过监督微调于多语言知识产权相关文本数据的BLOOMZ基模型。我们在MoZIP基准上评估了我们提出的MoZi模型和四个知名LLMs（即BLOOMZ、BELLE、ChatGLM和ChatGPT）。实验结果表明，MoZi在表现上明显优于BLOOMZ、BELLE和ChatGLM，但与ChatGPT相比得分较低。值得注意的是，目前LLMs在MoZIP基准上的表现有很大改进空间，即使是最强大的ChatGPT也未达到及格水平。我们的源代码、数据和模型可在 \url{https://github.com/AI-for-Science/MoZi} 上找到。
link: http://arxiv.org/abs/2402.16389v1
publish_time: 2024-02-26

title: HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs
abstract: 由于幻觉对于大型语言模型（LLMs）的可靠性和对齐性构成重大挑战，限制了它们在聊天机器人应用之外的普遍接受。尽管已经做出努力，但幻觉仍然是LLMs中普遍存在的挑战。检测幻觉本身也是一项艰巨的任务，通常需要手动标记或受限评估。本文介绍了一个自动可扩展的框架，结合了LLMs幻觉倾向的基准测试和高效的幻觉检测。我们利用LLMs生成与假设现象相关的具有挑战性的任务，随后将它们作为高效幻觉检测的代理。该框架是与领域无关的，允许在任何领域中使用任何语言模型来创建或评估基准。我们介绍了公开可用的HypoTermQA基准数据集，其中最先进模型的性能在3%至11%之间，评估代理展示了6%的误差率。该提议的框架提供了测试和改进LLMs的机会。此外，它还有潜力生成针对特定领域的基准数据集，如法律、健康和金融。
link: http://arxiv.org/abs/2402.16211v1
publish_time: 2024-02-25

title: EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings
abstract: EHRNoteQA是一个专为评估大型语言模型在临床环境中的新颖患者特定问题回答基准而定制的。通过MIMIC-IV电子健康记录，一个由三名医学专业人士组成的团队策划了包含962个独特问题的数据集，每个问题都与特定患者的电子健康记录临床笔记相关联。与现有EHR基准的不同之处在于：首先，这是第一个采用多选问题回答格式的数据集，这个设计选择有效评估LLM在自动评估背景下的可靠性分数，相比其他格式。其次，它需要分析多个临床笔记来回答一个问题，体现了现实世界临床决策的复杂性，医生需要审查大量患者历史记录。我们对各种大型语言模型的全面评估表明，它们在EHRNoteQA上的得分与它们在临床医生评估的真实医疗问题上的表现更密切相关，而不是它们在其他LLM基准上的得分。这突显了EHRNoteQA在评估医学应用中LLM的重要性，并强调了它在促进LLM集成到医疗系统中的关键作用。该数据集将在PhysioNet凭证访问下向公众开放，促进这一重要领域的进一步研究。
link: http://arxiv.org/abs/2402.16040v2
publish_time: 2024-02-25

title: PST-Bench: Tracing and Benchmarking the Source of Publications
abstract: 研究论文来源的查找是研究人员面临的一项基本而具有挑战性的任务，而在论文之间数十亿级的引用关系阻碍了研究人员高效地理解科学的演变。到目前为止，仍然缺乏由专业研究人员构建的准确且可扩展的数据集，以识别他们研究的论文的直接来源，基于此可以开发自动算法来扩展科学演化知识。本文研究了论文来源追踪（PST）问题，并在计算机科学中构建了一个高质量和不断增长的数据集PST-Bench。基于PST-Bench，揭示了一些有趣的发现，如各种主题之间不同的演化模式。各种方法的探索突出了PST-Bench的困难性，指出了这一领域的潜在方向。数据集和代码已经在https://github.com/THUDM/paper-source-trace上可用。
link: http://arxiv.org/abs/2402.16009v1
publish_time: 2024-02-25

title: SportQA: A Benchmark for Sports Understanding in Large Language Models
abstract: 深入了解体育领域对于提升自然语言处理至关重要，而SportQA作为一个专门设计用来评估LLMs在体育理解方面表现的基准测试，填补了现有专业基准测试的空白，为评估和提升LLMs在体育理解方面的能力提供了重要工具。
link: http://arxiv.org/abs/2402.15862v1
publish_time: 2024-02-24

title: Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method
abstract: 在谈判中，买方要比卖方更难，提出了一个名为OG-Narrator的新方法来提高买方的交易成功率。
link: http://arxiv.org/abs/2402.15813v2
publish_time: 2024-02-24

title: OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
abstract: 随着科学文献的快速增长，多才多艺的学术知识服务越来越依赖于全面的学术图挖掘。
link: http://arxiv.org/abs/2402.15810v1
publish_time: 2024-02-24

title: GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
abstract: LVLMs在图像感知和语言理解方面展现出了巨大的能力，但现有的多模态基准主要关注基本感知能力和常识知识，无法全面反映LVLMs的综合能力。我们提出了以中国高考为基础的多模态基准GAOKAO-MM，包括8个科目和12种图片类型，如图表、函数图、地图和照片。GAOKAO-MM源自中国本土背景，为模型的能力设定了人类水平的要求，包括感知、理解、知识和推理。我们评估了10个LVLMs发现它们的准确率都低于50%，其中GPT-4-Vison（48.1%）、Qwen-VL-Plus（41.2%）和Gemini-Pro-Vision（35.1%）排名前三。我们的多维分析结果表明，LVLMs朝着人工通用智能（AGI）还有一定距离，为多语种LVLMs的发展提供了启示。
link: http://arxiv.org/abs/2402.15745v1
publish_time: 2024-02-24

title: API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs
abstract: 为了有效地使用工具和外部应用程序编程接口（APIs）来规划和完成任务，对大型语言模型（LLMs）的需求不断增加。因此，对于能够获取足够数量涉及工具/API调用的训练和测试数据的方法引起了巨大兴趣，而两种主要研究方向已经出现以解决这一挑战。其中一种方法专注于合成数据生成技术，另一种方法涉及策划与任务相关的数据集，这些数据集可以转化为基于API/工具的任务。本文重点关注识别、策划和转化现有数据集的任务，并推出API-BLEND，一个用于培训和系统测试工具增强型LLMs的大型语料库。这些数据集模仿涉及API任务的真实场景，例如API/工具检测、插槽填充以及检测到的API的序列排列。我们展示了API-BLEND数据集在培训和基准测试方面的实用性。
link: http://arxiv.org/abs/2402.15491v1
publish_time: 2024-02-23

title: ToMBench: Benchmarking Theory of Mind in Large Language Models
abstract: ToM理论是一种认知能力，能够理解和归因于自己和他人的心智状态。最近的研究引发了关于大型语言模型（LLMs）是否表现出ToM形式的辩论。然而，现有的ToM评估面临着受限范围、主观判断和意外污染等挑战，导致评估不足。为了填补这一空白，我们引入了ToMBench，具有三个关键特征：一个系统化评估框架，涵盖社会认知中的8个任务和31种能力，支持自动化和公正评估的多项选择题格式，以严格避免数据泄漏的双语库。基于ToMBench，我们进行了大量实验，评估了10个流行LLMs在任务和能力方面的ToM表现。我们发现，即使是最先进的LLMs如GPT-4也落后于人类表现超过10个百分点，表明LLMs尚未达到人类水平的心智理论。我们的目标是通过ToMBench实现对LLMs ToM能力的高效有效评估，从而促进具有固有社会智能的LLMs的发展。
link: http://arxiv.org/abs/2402.15052v1
publish_time: 2024-02-23

title: CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean
abstract: 本文探讨了预训练语言模型（PLMs）中面向方面情感分类（ABSC）所面临的挑战，特别关注上下文化和虚幻问题。为了解决这些挑战，我们引入了CARBD-Ko（一种针对韩文的面向方面情感分类的上下文注释评估数据集），该数据集包含方面和双标极性，以区分方面特定和方面不可知情感分类。数据集包括用特定方面、方面极性、方面不可知极性和方面强度注释的句子。为了解决双标签方面极性的问题，我们提出了一种采用孪生网络的新方法。我们的实验结果突显了准确预测双极性的固有困难，并凸显了上下文化情感分析模型的重要性。CARBD-Ko数据集为未来在面向方面情感分类方面的研究工作提供了宝贵资源。
link: http://arxiv.org/abs/2402.15046v1
publish_time: 2024-02-23

title: Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
abstract: 本文探讨了在遥远监督框架下的命名实体识别（NER），其中主要挑战在于由于伪阳性、伪阴性和阳性类型错误等固有错误导致标签质量受损。我们批判性评估了当前DS-NER方法的有效性，使用名为QTL的真实基准数据集，揭示它们的性能通常不符合预期。为了解决标签噪声普遍问题，我们引入了一种简单而有效的方法，基于课程的正-未标记学习CuPUL，该方法在训练过程中从“简单”和更清洁的样本开始，以增强模型对嘈杂样本的韧性。我们的实证结果突出了CuPUL降低噪声标签影响和胜过现有方法的能力。QTL数据集和我们的代码均可在GitHub上获取。
link: http://arxiv.org/abs/2402.14948v2
publish_time: 2024-02-22

title: CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
abstract: 大型语言模型（LLMs）的批判和修正推理能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文介绍了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正推理能力的综合基准。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并包含了三个LLM家族的响应。利用CriticBench，我们评估并解析了17个LLMs在生成、批判和纠正推理（即GQC推理）中的表现。我们的研究结果揭示：（1）GQC能力存在线性关系，批判性训练显著提升了性能；（2）纠正有效性存在任务依赖性差异，逻辑导向任务更容易进行纠正；（3）随着模型规模增加，GQC知识的不一致性降低；（4）模型之间存在有趣的相互批判动态，更强大的模型更擅长批评较弱的模型，而较弱的模型可能在自我批评方面出乎意料地超越更强大的模型。我们希望这些对LLMs微妙的批判-纠正推理的见解能够促进进一步研究LLM批判和自我改进。
link: http://arxiv.org/abs/2402.14809v2
publish_time: 2024-02-22

title: MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues
abstract: 大型语言模型(Large Language Models, LLMs)的出现显著增强了对话系统的能力，但对LLMs进行全面评估仍然是一个挑战。以往的基准主要集中在单轮对话上，或提供对多轮对话的粗略和不完整的评估，忽略了实际对话的复杂性和细微差别。为了解决这个问题，我们引入了MT-Bench-101，专门设计用于评估LLMs在多轮对话中的细微能力。通过对真实多轮对话数据的详细分析，我们构建了一个包含13个不同任务中1388个多轮对话中的4208个轮次的三层次能力分类体系。然后我们根据MT-Bench-101对21个流行的LLMs进行评估，从能力和任务角度进行全面分析，观察到LLMs在各个任务内对话轮次的表现趋势不同。进一步分析表明，既不使用常用的对齐技术，也不使用特定于聊天的设计并未明显提升LLMs的多轮对话能力。广泛的案例研究表明，我们设计的任务准确评估了相应的多轮对话能力。
link: http://arxiv.org/abs/2402.14762v1
publish_time: 2024-02-22

title: ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
abstract: ConceptMath是一个双语（英文和中文），细粒度的基准，用来评估大型语言模型（LLMs）在数学推理方面的概念性能。
link: http://arxiv.org/abs/2402.14660v2
publish_time: 2024-02-22

title: Vygotsky Distance: Measure for Benchmark Task Similarity
abstract: 评估在现代自然语言处理中扮演着重要角色，本文提出了“维果茨基距离”来计算基准任务之间的相似性，可显著减少评估任务数量同时保持高验证质量，提高了未来NLP模型的泛化潜力。
link: http://arxiv.org/abs/2402.14890v2
publish_time: 2024-02-22

title: Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark
abstract: 预训练和大型语言模型在一般领域中的摘要能力已得到广泛验证，但它们在涉及复杂句子和专业知识的科学语料库中的使用尚未得到充分评估。该论文通过概念和实验分析科学摘要，强调传统评估方法（如$n$-gram、嵌入比较和问答）在提供解释、理解科学概念或确定关键内容方面的不足。随后，我们引入了 Facet-aware Metric（FM），利用LLMs进行高级语义匹配来根据不同方面评估摘要。这种面向方面的方法通过将评估任务分解为更简单的子任务，为摘要提供了全面评估。鉴于该领域没有评估基准，我们创建了一个基于Facet的科学摘要数据集（FD），带有方面级别的注释。我们的研究结果证实了FM提供了更合乎逻辑的评估科学摘要的方法。此外，在科学领域中，经过微调的较小模型可以与LLMs竞争，而LLMs在学习科学领域的上下文信息方面存在局限性。这提示了LLMs未来的增强方向。
link: http://arxiv.org/abs/2402.14359v1
publish_time: 2024-02-22

title: INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models
abstract: 检索器通常只优先考虑查询信息而不深入了解用户的搜索意图，而增强检索器理解用户意图和偏好的能力，类似语言模型指令，有可能产生更符合用户需求的搜索结果。Prior研究将指导性应用于信息检索的任务描述格式中，忽略了多样化和不断演变的搜索场景的更广泛背景。此外，现有的用于评估的基准缺乏明确定制以评估遵循指导的能力，从而阻碍了该领域的进展。为了解决这些限制，我们提出了一个新的基准，INSTRUCTIR，专门设计用于评估信息检索任务中遵循指导的能力。我们的方法侧重于针对每个查询实例量身定制的与用户符合的指导，反映了真实搜索场景中固有的多样化特征。通过实验分析，我们发现，对Fine-tune以遵循任务风格指令的检索器，如INSTRUCTOR，可能表现不及其非遵循指令调整的对手。这强调了构建受现有指导感知检索数据集训练的检索器时潜在的过度拟合问题。
link: http://arxiv.org/abs/2402.14334v1
publish_time: 2024-02-22

title: MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
abstract: 社交媒体平台是多模态信息交流的中心，涵盖文本、图像和视频，这使得机器很难理解在线空间中互动的信息或情感。多模态大型语言模型（MLLMs）已经成为解决这些挑战的有希望的解决方案，然而，它们在准确解释人类情感和复杂内容（如错误信息）方面仍面临困难。本文介绍了MM-Soc，一个设计用于评估MLLMs对多模态社交媒体内容理解的综合基准。MM-Soc整合了知名的多模态数据集，并加入了一个新的大规模YouTube标记数据集，针对从检测错误信息、仇恨言论到社交语境生成等一系列任务。通过对四个开源MLLMs的十个不同规模变体进行详尽评估，我们发现了显著的性能差异，凸显了模型对社交理解能力的提升需求。我们的分析显示，在零样例设置中，各种类型的MLLMs普遍在处理社交媒体任务时存在困难。然而，MLLMs在微调后表现出了性能提升，为改进提供了潜在途径。
link: http://arxiv.org/abs/2402.14154v1
publish_time: 2024-02-21

title: BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
abstract: BIRCO是一个评估信息检索系统在多方面用户目标下检索文档能力的基准，其复杂性和紧凑大小适合评估基于大型语言模型的信息检索系统。我们提出一个模块化框架来研究可能影响语言模型在检索任务上性能的因素，并确定一个简单的基准模型，它能够匹配或优于现有方法和更复杂的选择。没有一种方法在所有基准任务上取得令人满意的表现，这表明需要更强大的模型和新的检索协议来满足复杂用户需求。
link: http://arxiv.org/abs/2402.14151v1
publish_time: 2024-02-21

title: OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
abstract: 最新的进展表明大型语言模型（LLMs）和大型多模态模型（LMMs）在各种任务中超越了一般人类的能力，接近了人类专家在多个领域上的熟练水平。传统基准对这些模型变得不再具有挑战性，新的严格挑战是必不可少的，以评估它们的先进能力。我们提出了奥林匹亚级双语多模态科学基准OlympiadBench，其中包含来自奥林匹亚级数学和物理竞赛以及中国高考的8952个问题。每个问题都详细注释了逐步推理的专家水平。通过在OlympiadBench上评估顶尖模型，我们实施了一种综合评估方法，精确评估模型响应。值得注意的是，表现最佳的模型GPT-4V在OlympiadBench上获得了平均得分17.23％，在物理学中仅获得了11.28％，突显了基准的严格性和物理推理的复杂性。我们对GPT-4V进行的分析指出了幻觉、知识遗漏和逻辑谬误等普遍问题。我们希望我们的挑战性基准可以成为帮助未来通用人工智能研究努力的宝贵资源。
link: http://arxiv.org/abs/2402.14008v1
publish_time: 2024-02-21

title: CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models
abstract: MLLMs在结合视觉和语言的各种任务中表现出有希望的结果，但在使用上下文信息增强视觉理解方面仍存在不足，需要进一步提升。
link: http://arxiv.org/abs/2402.13607v2
publish_time: 2024-02-21

title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge
abstract: 为了使大型语言模型（LLMs）能够在特定国家有效部署，它们必须具有对该国文化和基础知识的理解。我们引入了国家对齐概念，通过社会价值对齐和基础知识对齐两个方面来衡量LLM与目标国家之间的对齐情况。社会价值对齐评估了模型对特定国家社会价值的理解程度，而基础知识对齐则检查了模型对与国家相关的基础知识的掌握程度。我们构建了KorNAT，这是第一个用于衡量与韩国对齐的基准测试。我们通过涉及6174名韩国参与者的大规模调查获得了社会价值数据集的真实标签。对于基础知识数据集，我们基于韩国教科书和GED参考资料构建了样本。KorNAT包含社会价值和基础知识两个方面分别有4K和6K个多项选择题。我们的数据集创建过程经过了精心设计，基于统计抽样理论，并通过多轮人工审核进行了优化。对七种LLMs的实验结果显示，只有少数模型达到了我们的参考分数，表明存在进一步提升的潜力。KorNAT经过了政府审批，通过了一个致力于评估数据集质量的政府机构的评估。我们的数据集样本和详细评估协议可在https://selectstar.ai/ko/papers-national-alignment 找到。
link: http://arxiv.org/abs/2402.13605v4
publish_time: 2024-02-21

title: OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models
abstract: OMGEval是全球首个多语言生成测试集，可评估LLM在不同语言环境下的能力，对提高LLM的多语言能力具有重要参考价值。
link: http://arxiv.org/abs/2402.13524v1
publish_time: 2024-02-21

title: DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain
abstract: 生物医学领域的自然语言处理(NLP)受到了广泛关注，预训练语言模型(PLMs)取得了重大进展，但由于不同模型之间评估协议的差异，比较这些模型变得具有挑战性。将多样化的下游任务整合到基准中，可以从各种角度评估PLMs的内在质量，这是一个公平的解决方案。虽然仍受限于少数语言，但在生物医学领域已经开展了这项倡议，尤其是英语和中文。现有的法语生物医学模型评估要么在少数任务上使用非标准化的协议进行评估，要么使用一般的下游任务进行评估，这也限制了对最新法语生物医学模型的评估。为了弥补这一研究空白并考虑法语的独特敏感性，我们提出了首个公开发布的法语生物医学语言理解基准DrBenchmark。该基准包括20个多样化任务，包括命名实体识别、词性标注、问答、语义文本相似度和分类等。我们评估了8个最先进的预训练掩蔽语言模型(MLMs)在一般和生物医学特定数据上的表现，以及英语特定的MLMs，以评估它们的跨语言能力。我们的实验结果显示，没有一种单一模型在所有任务中表现出色，一般性模型有时仍然具有竞争力。
link: http://arxiv.org/abs/2402.13432v1
publish_time: 2024-02-20

title: PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods
abstract: 我们提出了波兰信息检索基准（PIRB），一个全面评估框架，包括了41个针对波兰文本信息检索任务。该基准包括现有数据集以及10个涵盖医学、法律、商业、物理和语言学等多样主题的新数据集，我们对20多种密集和稀疏检索模型进行了广泛评估，并引入了三步训练高效语言特定检索器的过程。通过密集模型的优化，超过了迄今为止最好的解决方案，并且混合方法的使用进一步提高了性能。
link: http://arxiv.org/abs/2402.13350v2
publish_time: 2024-02-20

title: Benchmarking Retrieval-Augmented Generation for Medicine
abstract: 尽管大型语言模型在广泛的医学问答任务中取得了最新的表现，但它们仍然面临幻觉和过时知识的挑战。检索增强生成（RAG）是一种有前途的解决方案，已被广泛采用。通过提出医学信息检索增强生成评估（MIRAGE）基准，我们进行了大规模实验，发现各种医学语料库和检索器的组合可以获得最佳性能。我们相信我们的全面评估可以作为实施医学RAG系统的实用指南。
link: http://arxiv.org/abs/2402.13178v2
publish_time: 2024-02-20

title: TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning
abstract: 最新提出的TreeEval评估方法通过由性能高的LLM主持一次不可复制的评估会话，避免了基准测试中的数据泄露问题，并采用树状规划策略实现了评价过程的完整性和效率。
link: http://arxiv.org/abs/2402.13125v1
publish_time: 2024-02-20

title: CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models
abstract: LLM在处理中文任务上存在显著性能差距，需要发展更具文化智慧和语言多样性的模型。
link: http://arxiv.org/abs/2402.13109v1
publish_time: 2024-02-20

title: GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models
abstract: 我们研究了预训练语言模型（LMs）和预训练视觉-语言模型（VLMs）对物体可供性知识的了解。
link: http://arxiv.org/abs/2402.12881v1
publish_time: 2024-02-20

title: The FinBen: An Holistic Financial Benchmark for Large Language Models
abstract: LLM在不同领域展现出巨大潜力，然而在金融领域的潜力尚未充分挖掘，这主要是由于缺乏彻底的评估和金融任务的复杂性。随着LLM的快速发展，迫切需要一个系统化的金融评估基准。本文介绍了FinBen，这是第一个广泛开放源代码的评估基准，专门设计用于全面评估LLM在金融领域的能力。FinBen包括23个财务任务的35个数据集，根据Cattell-Horn-Carroll理论的启发，组织成三个难度谱，并评估LLM的认知能力，包括归纳推理、联想记忆、数量推理、结晶智力等。我们对包括GPT-4、ChatGPT和最新的Gemini在内的15个代表性LLM进行评估，揭示了它们在金融领域内的优势和局限。研究结果表明，GPT-4在数量化、提取、数值推理和股票交易方面领先，而Gemini在生成和预测方面表现出色，然而在复杂提取和预测方面均存在困难，显示出有必要有针对性地进行增强。指导调优可以提升简单任务的表现，但在改进复杂推理和预测能力方面仍有不足。FinBen致力于不断评估金融领域中的LLM，通过定期更新任务和模型，促进人工智能的发展。
link: http://arxiv.org/abs/2402.12659v1
publish_time: 2024-02-20

title: CausalGym: Benchmarking causal interpretability methods on linguistic tasks
abstract: 语言模型（LMs）在心理语言学研究中被证明是强大的工具，但大多数先前的工作集中在纯行为度量（例如，surprisal比较）。与此同时，模型可解释性研究已经开始揭示塑造LM行为的抽象因果机制。为了帮助将这些研究领域更紧密地联系在一起，我们引入了CausalGym。我们改编并扩展了SyntaxGym测试套件的任务，以评估解释性方法对因果影响模型行为的能力。为了说明CausalGym如何被使用，我们研究了python模型（14M-6.9B）并评估了一系列解释性方法包括线性探测和分布式对齐搜索（DAS）的因果功效。我们发现DAS胜过其他方法，因此我们使用它来研究pythia-1b中两种困难的语言现象的学习轨迹：负极性项授权和填充-间隙依赖关系。我们的分析表明实现这两项任务的机制是分阶段学习的，而不是逐渐的。
link: http://arxiv.org/abs/2402.12560v1
publish_time: 2024-02-19

title: AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies
abstract: 人类经常进行类比思维，将个人经历与当前情况联系起来（$X$类似于$Y$是因为$Z$）。类比思维使人类能够以创造性的方式解决问题，理解难以理解的概念，并更有效地表达思想。语言模型（LMs）是否也能做到这一点呢？为了回答这个问题，我们提出了一个叫做ANALOBENCH的基准测试，以确定LMs中的类比推理能力。我们的基准测试方法专注于两个在人类中常见的这种能力方面：（i）从大量信息中回忆相关经验，以及（ii）将类比推理应用于复杂和冗长的情景中。我们测试了一系列专有模型（例如GPT系列，Claude V2）和开源模型，如LLaMA2。与之前的结果一样，放大LMs会带来一些性能提升。令人惊讶的是，在类比涉及冗长情景或从大量信息中回想相关情景时，规模提供的收益较小，这个过程类似于大海捞针。我们希望这些观察结果能够促进这一领域的进一步研究。
link: http://arxiv.org/abs/2402.12370v1
publish_time: 2024-02-19

title: Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark
abstract: Text-to-SQL的重要性在于将自然语言翻译成结构化查询语言，使得非专家也能广泛访问结构化数据库。然而，设计这类任务的模型具有挑战性，因为存在许多因素，包括模糊问题和句法错误等“噪音”。这项研究深入分析了广泛使用的BIRD-Bench基准中噪音的分布和类型，以及噪音对模型的影响。我们发现问题和黄金查询中的噪音在数据集中普遍存在，在不同领域之间数量不均，并且不同类型的噪音分布不均。错误的黄金SQL查询会产生错误的黄金答案，严重影响基准的可靠性。令人惊讶的是，在订正的SQL查询上评估模型时，零-shot基线超过了最先进的提示方法的性能。我们得出结论，信息噪音标签和可靠的基准对于开发能处理不同类型噪音的新Text-to-SQL方法至关重要。所有数据集、注释和代码均可访问https://github.com/niklaswretblad/the-effects-of-noise-in-text-to-SQL。
link: http://arxiv.org/abs/2402.12243v4
publish_time: 2024-02-19

title: RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning
abstract: 近期在大型语言模型（LLMs）和大型多模态模型（LMMs）方面的最新进展表明在各种医疗应用中具有潜力，如智能医疗诊断。虽然取得了令人印象深刻的成果，但我们发现现有基准测试不反映真实医疗报告的复杂性和专业的深入推理能力。在本研究中，我们引入了RJUA-MedDQA，这是医疗专业领域的一项综合基准测试，提出了几项挑战：全面解释各种具有挑战性布局的图像内容，具有数值推理能力来识别异常指标，并展示基于医学背景提供疾病诊断、状态和建议陈述的临床推理能力。我们精心设计了数据生成管道，并提出了高效的结构恢复标注（ESRA）方法，旨在恢复医疗报告图像中的文本和表格内容。该方法显著提高了注释效率，使每个注释者的生产率翻倍，并使准确性提高了26.8%。我们进行了广泛的评估，包括对能够解决中文医学问答任务的5个LMMs的少量评估。为了进一步调查当前LLMs的限制和潜力，我们使用ESRA方法生成的图像文本对一组强大的LLMs进行了比较实验。我们报告了基线的性能并提出了几点观察：（1）现有LMMs的整体性能仍然有限；然而，与LLMs相比，LMMs更能够应对质量低下和结构各异的图像。 （3）跨上下文和图像内容的推理提出了重大挑战。我们希望这一基准测试能帮助社区在多模态医疗文档理解这些具有挑战性的任务上取得进展，并促进其在医疗保健中的应用。
link: http://arxiv.org/abs/2402.14840v1
publish_time: 2024-02-19

title: Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
abstract: 随着自然语言处理（NLP）领域的不断发展，使用SGD和Adam等一阶优化器对预训练的大型语言模型（LLMs）进行微调已成标准。然而，随着LLMs规模的扩大，来自一阶梯度计算的反向传播（BP）的显著内存开销提出了一个重要挑战。解决这一问题至关重要，尤其对于内存效率至关重要的应用，如设备上的训练。这篇论文提出了将微调LLMs的优化方向转向BP-free、零阶（ZO）优化，以降低内存成本。我们的工作不同于传统的ZO-SGD方法，通过对五个LLM族（Roberta、OPT、LLaMA、Vicuna、Mistral）、三种任务复杂度和五种微调方案的全面、首次研究，拓展了对ZO优化技术的探索。我们的研究揭示了先前被忽视的优化原则，强调了任务对齐的重要性、正向梯度方法的作用，以及算法复杂度与微调性能之间的平衡。我们进一步引入了对ZO优化的新型增强，包括块状下降、混合训练和梯度稀疏性。我们的研究为实现更加内存高效的LLM微调提供了有希望的方向。我们所有实验的代码都可以在https://github.com/ZO-Bench/ZO-LLM 中重现。
link: http://arxiv.org/abs/2402.11592v2
publish_time: 2024-02-18

title: Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation
abstract: 近年来，大型语言模型的发展取得了实质性进展，在各种任务中取得了显著的性能。我们认为使用固定问题或有限的改写作为查询来评估语言模型不够可靠和全面，因为语言模型对提示很敏感。因此，我们引入了一个名为知识边界的新概念，包括语言模型内部的提示不可知和提示敏感知识。知识边界可以避免语言模型评估中的提示敏感性，使其更加可靠和稳健。为了探索给定模型的知识边界，我们提出了带有语义约束的梯度下降投影方法，这是一种旨在识别每个知识片段最佳提示的新算法。实验表明，与现有方法相比，我们的算法在计算知识边界方面表现卓越。此外，我们还评估了多个语言模型在几个领域中的知识边界能力。
link: http://arxiv.org/abs/2402.11493v1
publish_time: 2024-02-18

title: MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing
abstract: 多模态知识编辑是提升多模态大型语言模型能力的关键进展，当前的基准测试主要关注粗粒度知识，而对于细粒度多模态实体知识的细节部分仍然未被充分探索。这一差距构成了一个重要挑战，因为细粒度实体识别对于在各种真实场景中实际部署和效果的提升至关重要。为了填补这一差距，我们引入了MIKE，一个专门设计用于细粒度多模态实体知识编辑的全面基准和数据集。MIKE包括一系列任务，评估不同视角，包括普通名称回答、实体级标题、复杂场景识别。此外，引入了一种新形式的知识编辑，多步编辑，以评估编辑效率。通过我们的广泛评估，我们展示目前的最先进方法在应对我们提出的基准测试时面临着重大挑战，突显了MLLMs中细粒度知识编辑的复杂性。我们的发现强调了这一领域需要新方法的紧急性，为未来研究和开发工作设定了明确的议程。
link: http://arxiv.org/abs/2402.14835v1
publish_time: 2024-02-18

title: Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation
abstract: 本文提出了一个基准自进化框架，动态评估迅速发展的大型语言模型（LLMs），旨在更准确评估它们的能力和局限性。我们利用一个多代理系统来操作原始示例的上下文或问题，重新构建新的进化示例，对现有基准进行动态扩展并具有高信心。为了实现更可扩展、更稳健和更精细的评估，我们实施了六种重新构建操作，构建演化实例，测试LLMs对各种查询、数据噪声和探究它们的问题解决子能力。通过这个框架，我们扩展了四个任务的基准数据集。实验结果显示，大多数LLMs在我们的可扩展和稳健评估下普遍表现下降，同时在我们的精细评估下，更准确地反映了模型的能力。此外，我们的框架扩大了不同模型之间以及同一模型在各种任务中的性能差距，有助于为特定任务进行更明智的模型选择。
link: http://arxiv.org/abs/2402.11443v1
publish_time: 2024-02-18

title: Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning
abstract: 在实际情景中，欺骗检测因其重要性而受到越来越多的关注，但数据稀缺阻碍了该领域的发展。本文提出了一种利用GPT-4模拟角色扮演以解决数据稀缺问题的新数据收集管道，同时将传统的欺骗检测任务扩展到欺骗推理，为欺骗行为提供更多证据，为当前大型语言模型的复杂推理能力提供评估，并成为未来研究的推理基准。
link: http://arxiv.org/abs/2402.11432v1
publish_time: 2024-02-18

title: Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models
abstract: 通过Asclepius这一全面且严格的评估标准，我们对Medical Multi-Modal Large Language Models (Med-MLLMs)的能力有了更深入的了解，为未来评估和安全部署这些模型在临床环境中奠定了先例。
link: http://arxiv.org/abs/2402.11217v1
publish_time: 2024-02-17

title: RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations
abstract: 社会规范的违反会导致潜在冲突，为了让交互式AI系统具备纠正违规的能力，我们提供了一个包含多轮对话的大规模语料库ReNoVi，通过定义一系列任务来帮助理解和纠正违规行为。我们利用ChatGPT生成合成数据来缓解训练数据的稀缺性，并评估LLM和人类在社会规范意识方面的对齐性。通过质量控制协议确保数据质量，实验结果展示了纠正规范违规在社会文化对话中的重要性，以及合成数据带来的性能提升。
link: http://arxiv.org/abs/2402.11178v1
publish_time: 2024-02-17

title: M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection
abstract: 大型语言模型（LLMs）的出现带来了机器生成文本（MGT）在多种渠道的激增，这引发了对其潜在滥用和社会影响的合理关切。从识别和区分这种内容与真正人类生成的文本的需求在打击虚假信息、维护教育和科学领域的诚信以及保持人们对交流的信任中至关重要。新的基准M4GT-Bench将多语言、多领域和多生成器的MGT检测问题引入，以解决这一问题。对于任务二的人工评估显示低于随机猜测的表现，展示了区分独特LLMs的挑战。在训练和测试数据在同一领域或生成器内分布时，通常会出现有希望的结果。
link: http://arxiv.org/abs/2402.11175v1
publish_time: 2024-02-17

title: When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models
abstract: 最近，大型语言模型（LLMs）在语言理解和生成方面取得了显著进展，引发了各种度量LLMs各种能力的基准的兴起。我们通过提出一个包含狡猾问题的FaLlacy Understanding Benchmark（FLUB）来挑战LLMs的推理和理解能力，希望促进社区改进LLMs理解谬论的能力。
link: http://arxiv.org/abs/2402.11100v1
publish_time: 2024-02-16

title: PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering
abstract: 现有关于时间问答（TQA）的研究主要集中在与特定时间戳或事件相关的问题上（例如：“1970年谁是美国总统？”）。很少有研究关注相对于当前时间的时间背景的问题（例如：“之前的美国总统是谁？”）。我们将这个问题称为“以现在时为锚点的时间问答（PATQA）”。PATQA面临着独特的挑战：（1）大型语言模型（LLMs）可能存在过时知识，（2）复杂的时间关系（如“之前”，“前任”）难以推理，（3）可能需要多层推理，（4）基准测试的黄金答案必须不断更新。为了解决这些挑战，我们引入了PAT-Questions基准测试，其中包括单跳和多跳时间问题。PAT-Questions中的答案可以通过在知识图上重新运行SPARQL查询来自动刷新。我们通过直接提示和检索增强生成（RAG）在PAT-Questions上评估了几种最先进的LLMs和一种最先进的时间推理模型（TEMPREASON-T5）。结果突出了现有PATQA解决方案的局限性，并促使需要新方法来改进PATQA的推理能力。
link: http://arxiv.org/abs/2402.11034v1
publish_time: 2024-02-16

title: Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts
abstract: 纵观当前研究发现，结构化文本方面的基于方面的摘要已取得显著进展，但摘要无序、大规模文本（如社交媒体和客户反馈）仍是一项重要挑战。当前研究主要针对结构化文本中预定义的方面，忽视了动态和无序环境的复杂性。本研究介绍了Disordered-DABS，一种针对无结构文本定制的动态基于方面的摘要新基准。通过改编现有数据集以提高成本效率和可扩展性，我们的全面实验和详细人工评估显示，Disordered-DABS对当代摘要模型，包括GPT-3.5等最先进的语言模型，提出了独特挑战。
link: http://arxiv.org/abs/2402.10554v1
publish_time: 2024-02-16

title: GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving
abstract: 当前大型语言模型(LLMs)和多模态模型(MMs)的最新进展展示了它们在问题解决方面的显著能力，然而，它们在处理几何数学问题方面的熟练程度尚未得到充分评估。GeoEval基准测试的引入填补了这一空白，这一综合收集包括一个主要子集共2000个问题、一个重点关注反向推理的750个问题子集、一个增强的2000个问题子集和一个困难的300个问题子集，这一基准测试有助于深入研究LLMs和MMs在解决几何数学问题上的表现。我们在这些不同子集上对十个LLMs和MMs进行评估，发现WizardMath模型表现突出，在主要子集上实现了55.67%的准确率，但在具有挑战性的子集上只有6.00%的准确率。这突显了对模型进行在未经预训练的数据集上进行测试的重要性。此外，我们的研究结果表明，GPT系列模型在重新表述后能够更有效地解决问题，这为提高模型能力提供了一个有希望的方法。
link: http://arxiv.org/abs/2402.10104v1
publish_time: 2024-02-15

title: Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence
abstract: 大语言模型(LLMs)的迅速普及引发了公众对不同LLM进行评估和比较的好奇心，许多研究人员开始提出自己的LLM基准。在注意到这些基准的初步不足后，我们展开了一项研究，通过人、流程和技术的镜头，以功能性和安全性为支柱，使用我们的创新统一评估框架来对23个最先进的LLM基准进行批判性评估。我们的研究揭示了重大限制，包括偏见、难以测量真正的推理能力、适应性、实现不一致性、工程复杂性、评估者多样性，以及在一个综合评估中忽视文化和意识形态规范。我们的讨论强调了在人工智能（AI）进步方面，包括提倡从静态基准向动态行为描述转变，以准确捕捉LLM的复杂行为和潜在风险。我们的研究凸显了LLM评估方法的范式转变的必要性，强调了为开发普遍接受的基准和增强AI系统融入社会的协作努力的重要性。
link: http://arxiv.org/abs/2402.09880v1
publish_time: 2024-02-15

title: AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
abstract: AQA-Bench是一个用于评估大型语言模型在算法背景下的顺序推理能力的新型基准，其关键特点在于采用交互式评估协议。
link: http://arxiv.org/abs/2402.09404v1
publish_time: 2024-02-14

title: Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking
abstract: 预训练大型语言模型在许多应用中已经带来革命，但仍面临与文化偏见和缺乏文化常识知识相关的挑战，这对引导跨文化交流和互动至关重要。本文介绍了一种新颖的大规模跨文化知识获取方法，以解决现有方法在捕捉丰富多样的世界文化方面的局限性。我们的方法通过策略性地从关于文化主题的密集信息性维基百科文档导航到一个庞大的链接页面网络。利用这一宝贵的数据收集来源，我们构建了CultureAtlas数据集，涵盖了广泛的次国家级地理区域和民族语言群，通过数据清理和预处理确保文本断言句的自包容性，并提取细粒度的文化概况信息。我们的数据集不仅有助于评估语言模型在文化多样环境中的表现，还为开发具有文化敏感性和意识的语言模型提供了基础工具。我们的工作标志着AI领域文化差异的更深入理解和弥合差距的重要一步，促进了数字领域对全球文化更具包容性和平衡性的呈现。
link: http://arxiv.org/abs/2402.09369v1
publish_time: 2024-02-14

title: Concept-1K: A Novel Benchmark for Instance Incremental Learning
abstract: 通过实验结果，我们发现十亿参数规模的PLMs仍然会遭受灾难性遗忘，而且遗忘程度受模型规模、预训练和缓冲区大小的影响。现有的增量学习方法和流行的微调技术LoRA都未能达到令人满意的性能，我们的研究为未来探讨PLMs的灾难性遗忘提供了新的场景，并鼓励设计更强大的技术来缓解PLMs的遗忘问题。
link: http://arxiv.org/abs/2402.08526v1
publish_time: 2024-02-13

title: EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages
abstract: 形式数学是将数学转化为编程语言的学科，在这种语言中，任何陈述都可以被计算机明确地检查。数学家和计算机科学家花费数十年的时间进行繁琐的形式化工作，开发了诸如Coq、HOL和Lean等语言。机器学习研究已经收敛到这些形式数学语料库，并产生了一系列方法来帮助交互式和自动定理证明。然而，这些论文主要集中在一种方法、一个证明任务、一个语言上。本文介绍了EvoGPT-f：一种新颖的进化框架，用于对五种形式数学语料库（Lean 3、Lean 4、Coq、HOL 4、HOL Light）的不同机器学习可学习性进行首次系统定量分析，使用四种记号化方法（字符、词级、字节对编码和StarCoder记号化器）。本文并未解决关于“最佳”或“最容易”学习的问题。相反，这个框架和初步发现开始揭示这些语言的不同机器学习可学习性，为跨社区进行更系统的定量和定性比较研究奠定了基础。
link: http://arxiv.org/abs/2402.16878v1
publish_time: 2024-02-12

title: Mercury: An Efficiency Benchmark for LLM Code Synthesis
abstract: Mercury是首个旨在评估LLM代码合成任务效率的基准测试，其Beyond@K指标鼓励生成功能正确且计算高效的代码，揭示了LLM在效率输出上仍存在较大差距，为LLM研究与发展打开新的领域。
link: http://arxiv.org/abs/2402.07844v1
publish_time: 2024-02-12

title: AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension
abstract: 近年来，指令遵循的音频语言模型引起了人类音频交互的广泛关注，但由于缺乏能够评估以音频为中心交互能力的基准，阻碍了该领域的进展。AIR-Bench是第一个旨在评估LALM能力的基准，旨在评估LALM模型对各种类型的音频信号（包括人类语音、自然声音和音乐）的理解能力，以及其与人类在文本格式中交互的能力。
link: http://arxiv.org/abs/2402.07729v1
publish_time: 2024-02-12

title: Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy
abstract: 评估生成文本视觉语言模型是一项具有挑战性但至关重要的工作，我们的研究通过解决现有视觉问答(VQA)基准的局限性并提出创新的评估方法，旨在推动我们对这些模型能力的理解。
link: http://arxiv.org/abs/2402.07270v1
publish_time: 2024-02-11

title: OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
abstract: 神经心智理论 (N-ToM) 是机器理解并跟踪他人心智状态的关键，对于发展具有社交智能的代理人至关重要。 然而，目前的N-ToM基准存在一些缺陷，包括故事具有模糊和人为性的叙述，缺乏个性特征和偏好，缺乏涉及人物心理状态的问题以及提问的多样性有限。 针对这些问题，我们构建了新的基准 OpenToM，用来评估N-ToM，具备(1) 更长更清晰的叙事故事，(2) 具有明确个性特征的角色，(3) 由角色意图触发的动作，以及(4) 旨在挑战LLMs对于建模人物在物理和心理世界心智状态能力的问题。通过使用OpenToM，我们发现最先进的LLMs在模拟物理世界心智状态中的某些方面表现出色，但在追踪人物在心理世界的心智状态方面则存在短板。
link: http://arxiv.org/abs/2402.06044v2
publish_time: 2024-02-08

title: Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset
abstract: 传统自然语言处理(NLP)在医疗领域的应用主要集中在以患者为中心的服务上，增强患者互动和护理交付，比如医疗对话系统。然而，NLP对于帮助经验不足的医生，特别是在沟通医学辅导等领域的潜力仍然未被充分探索。我们介绍了“ChatCoach”，一个集成人工智能合作框架。在这个框架里，患者代理和辅导代理共同支持医学学习者在诊断中练习医学沟通技能。与传统对话系统不同，“ChatCoach”提供了一个模拟环境，医生可以在这里与患者代理进行医学对话。同时，辅导代理会给医生实时反馈。为构建“ChatCoach”系统，我们开发了一个数据集，并集成了ChatGPT和Llama2等大型语言模型，旨在评估它们在沟通医学辅导任务中的有效性。我们的比较分析表明，经过指导优化的Llama2在表现上显著优于ChatGPT的提示性方法。
link: http://arxiv.org/abs/2402.05547v1
publish_time: 2024-02-08

title: SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
abstract: 在大型语言模型（LLMs）快速发展的领域中，确保健全的安全措施至关重要。为了满足这一关键需求，我们提出了专门用于评估LLMs、攻击和防御方法的安全基准“SALAD-Bench”。通过其广泛性、丰富多样性、跨越三个级别的复杂分类和多功能性，SALAD-Bench超越了传统基准。通过一系列从标准查询到攻击和防御修改和多选题的问题精心设计，以有效管理固有的复杂性，我们引入了一种创新的评估器：基于LLM的MD-Judge用于具有攻击增强查询的QA对，确保了无缝、可靠的评估。以上组件将SALAD-Bench从标准LLM安全评估扩展到LLM攻击和防御方法评估，确保了联合目的的实用性。我们的广泛实验揭示了LLMs抵御新兴威胁的韧性以及当代防御策略的有效性。数据和评估器可在https://github.com/OpenSafetyLab/SALAD-BENCH 上获得。
link: http://arxiv.org/abs/2402.05044v3
publish_time: 2024-02-07

title: MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
abstract: 近期，多模态大型语言模型（MLLMs）引起了广泛关注，展现了在人工智能领域具有显著潜力。然而，评估MLLMs的效用存在很大挑战，主要是由于缺乏与人类偏好相一致的多模态基准。本文受LLM-as-a-Judge在LLMs中的启发，引入了一个新的基准，称为MLLM-as-a-Judge，用于评估MLLMs在辅助评审者方面的能力，包括三个不同的任务：评分评估、配对比较和批量排名。我们的研究表明，虽然MLLMs在配对比较中展现了Remarkable human-like discernment，但在评分评估和批量排名任务中与人类偏好存在显著分歧。此外，MLLMs在判断中仍面临挑战，包括多样化的偏见、幻觉性反应和不一致性，即使对于像GPT-4V这样的先进模型。这些发现突显了进一步研究努力和改进MLLMs作为可靠评估者的紧迫性。源代码和数据集可在https://github.com/Dongping-Chen/MLLM-as-a-Judge获得。
link: http://arxiv.org/abs/2402.04788v1
publish_time: 2024-02-07

title: SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark
abstract: SceMQA是一个新颖的科学多模态问答基准，专注于高中到预大学阶段的教育环节，涵盖数学、物理、化学和生物等核心科学学科，提供多种选择和自由回答格式，并为每个问题提供具体知识点和详细解释，通过相同背景但不同问题的方式促进推理能力评估，并显示出现有多模式大语言模型在该基准测试中尚需进一步发展的空间。
link: http://arxiv.org/abs/2402.05138v1
publish_time: 2024-02-06

title: LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K
abstract: 该论文介绍了LV-Eval，一个拥有五个不同长度级别（16k，32k，64k，128k和256k）、包含11个双语数据集的具有挑战性的长上下文基准。 LV-Eval的设计包括三个关键技术，即混淆事实插入、关键词和短语替换以及基于关键词回忆的度量设计。LV-Eval的优势在于可以控制不同上下文长度的评估、具有困难的测试实例、减轻知识泄漏以及更客观的评估。通过对10个LLMs在LV-Eval上的评估和对LV-Eval构建中使用的技术进行消融研究，发现商用LLMs在短于其声称的上下文长度级别内的评估中通常优于开源LLMs，但在更长的上下文长度方面，它们的综合性能被开源LLMS超越。超长上下文的LLMs（如Yi-6B-200k）在性能下降方面表现相对平缓，但绝对性能未必比上下文长度较短的LLMs更高。在存在混淆信息的情况下，LLMs的性能可能会显著下降。LV-Eval有助于缓解与知识泄漏和不准确度量相关的问题，减少评估中的偏见。
link: http://arxiv.org/abs/2402.05136v1
publish_time: 2024-02-06

title: EffiBench: Benchmarking the Efficiency of Automatically Generated Code
abstract: 代码生成模型越来越成为软件开发中不可或缺的辅助工具，提供代码补全、调试和代码翻译等任务的帮助。虽然当前研究已经彻底研究了代码生成模型生产的代码的正确性，但一个重要方面——生成代码的效率经常被忽视。本文介绍了EffiBench，一个包含1,000个效率关键的编程问题，用于评估代码生成模型生成的代码的效率。EffiBench包含了各种LeetCode编程问题，每个问题都配有可执行的人工编写的规范解决方案。通过EffiBench，我们经验性地检验了21个大型语言模型（13个开源和8个闭源）在生成高效代码方面的能力。结果显示，GPT-4-turbo生成的代码效率最高，显著优于Palm-2-chat-bison、Claude-instant-1、Gemini-pro、GPT-4和GPT-3.5。然而，它的代码效率仍然比人工编写的规范解决方案的效率差。具体来说，GPT-4-turbo生成的代码的平均执行时间和最差执行时间分别是规范解决方案的1.69倍和45.49倍。
link: http://arxiv.org/abs/2402.02037v2
publish_time: 2024-02-03

title: TravelPlanner: A Benchmark for Real-World Planning with Language Agents
abstract: 尽管目前的语言代理人还不能很好地处理复杂的旅行规划任务，但它们有潜力去探索更复杂的问题，这是非常有意义的进步。TravelPlanner为未来语言代理人提供了一个具有挑战性的测试平台。
link: http://arxiv.org/abs/2402.01622v2
publish_time: 2024-02-02

title: When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
abstract: 基于基准排名的大型语言模型（LLM）排行榜经常用来指导从业人员进行模型选择，但往往排行榜的排名被简单接受，我们表明这是一个（潜在的昂贵）错误。在现有的排行榜下，LLM的相对性能对（常常微小的）细节非常敏感。我们展示了对于流行的多项选择问题基准（例如MMLU），微小的扰动比如改变选择的顺序或答案选取方法，会导致排名发生8个位置的变化。我们通过对三种广泛的基准扰动进行系统实验并确定这种行为的来源来解释这一现象。我们的分析得出了几项最佳实践建议，包括采用混合评分方法进行答案选择的优势。我们的研究强调了依赖简单基准评估的危险，并为现有基准提供更加健壮的评估方案。
link: http://arxiv.org/abs/2402.01781v1
publish_time: 2024-02-01

title: A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains
abstract: 复杂推理任务中，促使语言模型提供逐步回答是主要方法，最近文献讨论了验证推理的自动方法，但缺乏细粒度的步骤级数据集阻碍了此方向的进展。REVEAL数据集引入了细致评估复杂思维链的自动验证器的基准，结果显示验证器在验证逻辑正确性和检测矛盾方面存在困难。
link: http://arxiv.org/abs/2402.00559v3
publish_time: 2024-02-01

title: I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench
abstract: 大型语言模型（LLMs）是否表现出类似于人类的意识形态？本文介绍了AwareBench，这是一个旨在评估LLMs中意识的基准。根据心理学和哲学理论，我们将LLMs中的意识定义为理解自己作为AI模型的能力和展现社会智能。随后，我们将LLMs中的意识分为五个维度，包括能力、任务、情感、文化和视角。基于这种分类法，我们创建了一个名为AwareEval的数据集，其中包含二进制、多选和开放式问题，用于评估LLMs对特定意识维度的理解。我们对13个LLMs进行的实验表明，其中大多数在完全认识自己的能力和使命时存在困难，同时展现出体面的社会智能。我们结论强调LLMs的意识与AI的对齐和安全的关联，强调它对LLMs值得信赖和道德发展的重要性。我们的数据集和代码可在https://github.com/HowieHwong/Awareness-in-LLM 上找到。
link: http://arxiv.org/abs/2401.17882v2
publish_time: 2024-01-31

title: Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data
abstract: 本研究建立了一个全面的基准，评估VLMs在场景理解、定位和计数以及变化检测任务中的能力，发现尽管像GPT-4V这样的尖端VLMs在开放性任务上表现出色，但由于其较差的空间推理能力，在对象定位和计数任务上的效用受到限制。
link: http://arxiv.org/abs/2401.17600v1
publish_time: 2024-01-31

title: Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios
abstract: 大语言模型(LLMs)作为工具代理在实际应用中的最新趋势凸显了对其能力的全面评估的必要性，特别是在涉及规划、创建和使用工具的复杂情景中。UltraTool是一个新颖的基准测试，旨在改进和评估LLMs在真实世界情景中利用工具的能力。UltraTool强调了从规划和创建到在复杂任务中应用工具的整个过程，要求准确的多步规划以有效解决问题。
link: http://arxiv.org/abs/2401.17167v2
publish_time: 2024-01-30

title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models
abstract: RAG通过使用外部知识源增强大型语言模型的能力，克服了LLM的一些局限，但评估RAG系统却具有挑战性，需要构建更全面的基准测试，分析各组件在不同RAG应用场景中的影响，为不同情境优化RAG技术提供有益见解。
link: http://arxiv.org/abs/2401.17043v2
publish_time: 2024-01-30

title: MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models
abstract: Large language models (LLMs) are essential for complex multi-turn conversations in various real-world applications, but existing benchmarks mainly focus on single-turn evaluations, neglecting LLMs' abilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark to assess multi-turn conversational skills, categorizing interaction patterns and creating multi-turn queries to study factors influencing performance. Our evaluation of 11 LLMs reveals that closed-source models generally outperform open-source ones, but specific open-source models exceed GPT-3.5-Turbo in certain tasks. Performance degradation in multi-turn settings is observed across most models, not reflecting their core capabilities, with distance to relevant content and error propagation susceptibility identified as key factors. MT-Eval is publicly available to encourage further research on developing more robust conversational models.
link: http://arxiv.org/abs/2401.16745v1
publish_time: 2024-01-30

title: VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models
abstract: Visually Impaired Assistance (VIA)利用计算机视觉和自然语言处理帮助视障者处理日常活动，但当前状态的大型模型在这方面存在一些局限。
link: http://arxiv.org/abs/2402.01735v2
publish_time: 2024-01-29

title: E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models
abstract: 大型语言模型(LLMs)的快速发展使得许多LLMs开始在中国K-12教育领域中被应用。LLMs与教育的整合日益紧密，然而目前还没有专注于中国K-12教育领域的LLMs评估基准。因此，迫切需要一个全面的自然语言处理基准来准确评估不同LLMs在中国K-12教育领域的能力。为此，我们介绍了E-EVAL，这是专门针对中国K-12教育领域设计的第一个全面评估基准。E-EVAL包括了4351道涵盖了中文、英文、政治、历史、伦理、物理、化学、数学和地理等广泛学科的多项选择题，涵盖了小学、初中和高中不同层次。我们对先进的LLMs进行了全面评估，包括英文主导和中文主导模型。研究结果显示，与英文主导模型相比，中文主导模型表现良好，许多甚至得分超过了GPT 4.0。然而，几乎所有模型在数学等复杂学科中表现不佳。我们还发现，大部分中文主导LLMs在小学阶段的得分并没有比在中学阶段更高。我们观察到，模型对更高阶知识的掌握并不一定意味着也能掌握更低阶知识。此外，实验结果表明，连锁思维(CoT)技术仅对具有挑战性的科学学科有效，而少样本提示对文科学科更有益。通过E-EVAL，我们旨在分析LLMs在教育应用中的优势和局限，并为中国K-12教育和LLMs的进步和发展做出贡献。
link: http://arxiv.org/abs/2401.15927v1
publish_time: 2024-01-29

title: PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models
abstract: 近期，大量的大型代码生成模型(LCGMs)被提出，展示了在辅助开发人员处理复杂编程任务方面的巨大潜力。 PPAM完全使用我们的工具在两个广泛使用的数据集上，并使用八种代码生成模型将其与九种基准方法进行比较。结果表明，与基准方法相比，我们的工具在生成更具挑战性、多样化和自然的编程问题方面高效。
link: http://arxiv.org/abs/2401.15545v1
publish_time: 2024-01-28

title: MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries
abstract: 本文介绍了一种新的数据集MultiHop-RAG，旨在解决现有RAG系统在回答多跳查询方面的不足，提供支持证据并对多跳查询进行推理。我们通过两个实验展示了MultiHop-RAG数据集的基准测试效用，结果表明现有RAG方法在处理多跳查询时表现不佳。希望MultiHop-RAG能够成为开发有效RAG系统的宝贵资源，进一步促进LLM在实践中的广泛应用。
link: http://arxiv.org/abs/2401.15391v1
publish_time: 2024-01-27

title: Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs
abstract: 作者对问题回答的归因是为了支持生成的陈述提供引用，吸引了广泛的研究关注。当前用于自动评估归因的方法通常基于大型语言模型（LLMs），仍然不足，特别是在识别归因之间的微妙差别和引用与陈述之间的复杂关系方面。为了比较这些归因评估方法并开发新的方法，我们引入了一组精细分类（支持性、不足、矛盾和不相关）用于测量归因，并通过利用知识图谱（KGs）开发了一个复杂的带归因的问题回答（CAQA）基准，以自动生成不同类别的问题-回答对的归因。我们的分析表明，现有的评估器在精细归因设置下表现不佳，并在复杂的引用-陈述推理中存在缺陷。经过人工注释验证的CAQA基准出现为选择和开发LLM归因评估器的有希望的工具。
link: http://arxiv.org/abs/2401.14640v1
publish_time: 2024-01-26

title: Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline
abstract: ASR系统识别准确性与用户友好度之间存在权衡关系，需要综合考虑语音级和文本级，提出错误可解释基准（EEB）数据集，以更全面了解系统缺陷并改善用户体验。
link: http://arxiv.org/abs/2401.14625v1
publish_time: 2024-01-26

title: K-QA: A Real-World Medical Q&A Benchmark
abstract: 在临床环境中，确保大型语言模型提供的回答准确性至关重要，可能直接影响患者健康。通过构建包含1,212个患者问题的K-QA数据集，并使用内部医生团队回答并手动分解一部分数据，我们评估了几种最先进的模型，并研究了上下文学习和医学导向的信息检索方案的影响。结果显示，上下文学习提高了模型的全面性，而信息检索方法有效降低了幻觉。我们将K-QA数据集共享给社区，以促进医学准确的自然语言处理应用研究。
link: http://arxiv.org/abs/2401.14493v1
publish_time: 2024-01-25

title: LongHealth: A Question Answering Benchmark with Long Clinical Documents
abstract: 最新的大型语言模型在医疗领域具有潜在益处，但目前对于处理现实世界中复杂长篇临床数据的评估还不够全面。
link: http://arxiv.org/abs/2401.14490v1
publish_time: 2024-01-25

title: CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning
abstract: 多模态大型语言模型（MLLMs）取得了显著的进展，展示了强大的知识理解和推理能力。然而，领域特定知识的掌握继续是一个挑战。当前多模态领域特定知识评估的基准主要集中在英语，限制了评估的全面性。因此，我们引入了CMMU，一个针对中文多模态和多类型问题理解和推理的新基准。
link: http://arxiv.org/abs/2401.14011v2
publish_time: 2024-01-25

title: SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval
abstract: 科学多模态信息检索（SciMMIR）是一个迅速发展的领域，通过先进的表示学习和跨模态对齐研究，在图像-文本配对方面取得了重要进展。
link: http://arxiv.org/abs/2401.13478v1
publish_time: 2024-01-24

title: Benchmarking LLMs via Uncertainty Quantification
abstract: 开源大型语言模型的普及突显了对全面评估方法的迫切需求，然而目前的评估平台忽视了重要的因素 -- 不确定性，这对于深入评估大型语言模型至关重要。为弥补这一差距，我们引入了一种结合不确定性量化的新的大型语言模型基准评估方法。我们的研究涉及了八种涵盖了五种代表性自然语言处理任务的大型语言模型 (LLM 系列)。此外，我们引入了一种考虑了预测准确性和预测不确定性的不确定性感知评估度量，UAcc。我们的研究结果显示：I) 准确性更高的大型语言模型可能表现出较低的确定性；II) 规模更大的大型语言模型可能与规模较小的对应模型相比有更大的不确定性；III) 指导微调可能增加大型语言模型的不确定性。通过考虑不确定性，我们的新的 UAcc 度量可以放大或缩小一个大型语言模型相对于另一个的改进，并且甚至可以改变两种大型语言模型的相对排名。这些结果强调了在大型语言模型评估中纳入不确定性的重要性。
link: http://arxiv.org/abs/2401.12794v1
publish_time: 2024-01-23

title: CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
abstract: 随着大型多模型模型（LMMs）的能力不断提升，评估LMMs的表现成为一个迫切的需求。同时，在非英语环境（如中文）中评估LMMs的高级知识和推理能力存在更大的差距。我们推出了CMMMU，一个新的中文大规模多学科多模态理解基准，旨在评估LMMs在中国背景下对需要大学水平学科知识和深思熟虑推理的任务的表现。CMMMU受到MMM的启发，并严格遵循MMM的注释和分析模式。CMMMU包括来自大学考试、测验和教科书的1.2万个手动收集的多模态问题，涵盖艺术与设计、商业、科学、健康医学、人文社会科学和技术工程六个核心学科，30个学科，其中包括39种高度异质的图像类型，如图表、图表、地图、表格、音乐谱和化学结构等。CMMMU侧重于使用中文背景下领域特定知识的复杂感知和推理。我们评估了11个开源LLMs和一个专有的GPT-4V(ision)。即使GPT-4V也只能达到42%的准确率，表明还有很大的改进空间。CMMMU将推动社区构建面向专家人工智能的下一代LMMs，并通过提供多样化的语言背景促进LMMs的民主化。
link: http://arxiv.org/abs/2401.11944v2
publish_time: 2024-01-22

title: Benchmarking Large Multimodal Models against Common Corruptions
abstract: 本技术报告旨在填补对大型多模型模型（LMMs）评估中的不足，特别是通过针对常见损坏情况对其输出的自一致性进行检查。我们研究文本、图像和语音之间的跨模态交互，包括四种基本生成任务：文本到图像、图像到文本、文本到语音和语音到文本。我们创建了一个名为MMCBench的综合基准，涵盖了100多种流行的LMMs（总共超过150个模型检查点）。在常见损坏情况下进行彻底评估对于实际部署至关重要，并有助于更好地了解最前沿LMMs的可靠性。基准代码可在https://github.com/sail-sg/MMCBench 上找到。
link: http://arxiv.org/abs/2401.11943v1
publish_time: 2024-01-22

title: SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese
abstract: 我们介绍了SuperCLUE-Math6（SC-Math6），一个用于评估中文语言模型数学推理能力的新基准数据集。SC-Math6是设计为升级版的GSM8K数据集，增加了难度、多样性和应用范围。它包含超过2000个需要多步推理并提供自然语言解决方案的数学问题。我们提出了一种创新方案，基于不同推理步骤的问题表现来量化大模型的推理能力。对13个代表性中文模型的实验表明，推理水平有明显分层，像GPT-4这样的顶级模型表现出色。SC-Math6填补了中文数学推理基准测试的空白，并为提升中文语言模型的智能提供了全面的测试平台。
link: http://arxiv.org/abs/2401.11819v2
publish_time: 2024-01-22

title: ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution
abstract: 通过词汇替换发现给定目标词在上下文句子中的合适替代词，但该任务未考虑与目标词同等或更高水平的替代词，这对于想要提升写作水平的语言学习者可能是有益的。我们提出了一个新任务，语言能力导向的词汇替换，并引入了一个新的基准测试ProLex，用于评估系统生成合适替代词的能力，同时展示了更好语言水平的替代词。我们还提出了可以自动执行新任务的模型，展示了我们的最佳模型——使用任务特定合成数据微调的Llama2-13B模型，在F分数上比ChatGPT平均提高了3.2％，在ProLex上取得了与GPT-4相当的结果。
link: http://arxiv.org/abs/2401.11356v2
publish_time: 2024-01-21

title: Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences
abstract: 多模态大型语言模型（MLLMs）已表现出在处理各种视觉-语言任务方面的熟练能力，但当前的MLLM基准主要设计用于评估基于单个图像的静态信息的推理，而现代MLLM对于从图像序列中推理的能力，对于理解我们快速变化的世界至关重要，却得到了较少的研究。为解决这一挑战，本文介绍了Mementos，一个新的基准，用于评估MLLM的序列图像推理能力。Mementos包含4,761个不同长度的多样化图像序列。我们还采用了一个GPT-4辅助方法来评估MLLM的推理表现。通过对包括GPT-4V和Gemini在内的九个最近的MLLM在Mementos上进行仔细评估，我们发现它们难以准确描述给定图像序列的动态信息，经常导致出现对象及其对应行为的幻觉/错误描述。我们的定量分析和案例研究确定了影响MLLM的序列图像推理的三个关键因素：对象和行为幻觉之间的相关性，同时发生行为的影响，以及行为幻觉的叠加影响。我们的数据集可在https://github.com/umd-huang-lab/Mementos获得。
link: http://arxiv.org/abs/2401.10529v2
publish_time: 2024-01-19

title: R-Judge: Benchmarking Safety Risk Awareness for LLM Agents
abstract: 大型语言模型在实际应用中展示了巨大的潜力，但在交互环境中操作时，这些模型可能存在意外的安全风险。本研究着眼于评估LLM代理在不同环境中的行为安全，引入了R-Judge基准，旨在评估LLM在判断和识别安全风险方面的能力，结果显示LLM在风险意识方面存在很大的提升空间。
link: http://arxiv.org/abs/2401.10019v2
publish_time: 2024-01-18

title: RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning
abstract: 工具学习作为大型语言模型（LLMs）与物理世界之间互动的重要手段，已经引起了广泛关注。当前的研究主要强调LLMs在结构良好的环境中利用工具的能力，但忽视了当面对真实世界中不可避免的噪音时它们的稳定性。为了填补这一缺口，我们介绍了RoTBench，一个用于评估LLMs在工具学习中鲁棒性的多级基准测试。具体而言，我们建立了五个外部环境，每个环境都具有不同程度的噪音（即干净、轻微、中等、沉重和联合），对模型在工具选择、参数识别和内容填充三个关键阶段的韧性进行了深入分析。涉及六个广泛使用的模型的实验凸显了在工具学习中增强LLMs鲁棒性的紧迫性。例如，即使在手动准确度没有实质性变化的情况下，GPT-4的性能也从80.00下降到58.10。更令人惊讶的是，GPT家族固有的噪音校正能力反而阻碍了它在轻微噪音面前的适应能力。基于这些发现，我们提出了RoTTuning策略，通过丰富训练环境的多样性来增强LLMs在工具学习中的鲁棒性。源代码和数据可在https://github.com/Junjie-Ye/RoTBench 上获得。
link: http://arxiv.org/abs/2401.08326v2
publish_time: 2024-01-16

title: Anchor function: a type of benchmark functions for studying language models
abstract: 理解基于变压器的语言模型越来越关键，特别是它们在推进人工通用智能方面发挥关键作用。然而，语言模型研究面临重大挑战，尤其是对于资源受限的学术研究团体。提出了"锚定函数"的概念，可以为学术研究提供有用的研究问题。
link: http://arxiv.org/abs/2401.08309v1
publish_time: 2024-01-16

title: AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception
abstract: 通过集体努力，多模态大型语言模型（MLLMs）正在蓬勃发展。然而，它们在图像美感感知方面的表现仍然不确定，并且在实际应用中受到高度需求。主要障碍在于缺乏专门的基准来评估MLLMs在美学感知方面的有效性。这种盲目的试探可能会阻碍更先进带有美学感知能力的MLLMs的进一步发展。为了解决这一困境，我们提出了AesBench，这是一个专家基准，旨在通过跨双重方面的精心设计全面评估MLLMs的美学感知能力。我们构建了一个Expert-labeled Aesthetics Perception Database（EAPD），它具有多样化的图像内容和由专业美学专家提供的高质量注释。我们提出了一组综合标准，以从感知（AesP）、共情（AesE）、评估（AesA）和解释（AesI）四个方面衡量MLLMs的美学感知能力。广泛的实验结果表明，当前的MLLMs仅具有基本的美学感知能力，而MLLMs与人类之间仍然存在显著差距。我们希望这项工作能激发社区对MLLMs美学潜力进行更深入的探索。源数据将在https://github.com/yipoh/AesBench 上提供。
link: http://arxiv.org/abs/2401.08276v1
publish_time: 2024-01-16

title: MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception
abstract: 最近的多模态大型语言模型（MLLMs）在视觉感知和理解方面表现出了出色的能力，但这些模型也存在幻觉问题，限制了它们作为人工智能系统的可靠性。我们认为这些幻觉部分是由于模型在理解图像中可以和不可以感知的内容时所遇到的困难所致，我们称之为感知中的自我意识。尽管这一方面的重要性被之前的研究忽视了，但我们在本文中旨在定义和评估MLLMs在感知中的自我意识。我们首先介绍了知识四分法感知中的知识象限，它有助于定义MLLMs对图像的了解与不了解。利用这一框架，我们提出了一个特别设计用于评估这一能力的基准，即MLLMs的感知自我意识（MM-SAP）。我们对各种流行的MLLMs应用MM-SAP，进行了对其自我意识的全面分析，提供了详细的见解。实验结果显示当前的MLLMs具有有限的自我意识能力，指出未来在可信赖的MLLMs开发中有必要重点关注的一个关键领域。代码和数据可在https://github.com/YHWmz/MM-SAP获取。
link: http://arxiv.org/abs/2401.07529v2
publish_time: 2024-01-15

title: PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics Capabilities
abstract: LLM在理解语义方面表现出卓越能力，但在理解语用学方面经常遇到困难。通过发布一个包含14个任务的语用理解基准(PUB)数据集，我们证明了这一事实。我们在每个任务中精心筛选了高质量的测试集，包括多项选择题答案(MCQA)。PUB总共包含28k个数据点，其中6.1k由我们创建，其余来自现有数据集。我们评估了九种模型，包括参数数量和训练类型的不同。我们的研究表明，微调指令跟随和对话对较小语言模型的语用能力有显著提升。然而，对于较大模型，基础版本与对话调整后的版本表现相当。此外，模型的能力与人类的能力之间存在明显的性能差距。此外，与人类在各种任务中保持一致的表现不同，模型展示出在熟练度上的变化，由于不同的提示和相同数据集内任务的复杂性，性能水平有所波动。总的来说，这一基准旨在全面评估LLM处理需要语用推理的真实语言任务的能力。
link: http://arxiv.org/abs/2401.07078v1
publish_time: 2024-01-13

title: OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models
abstract: 当前评估框架在功能性编程中忽视了面向对象编程，为了推动自动编程的发展，我们引入了一项面向对象编程的基准，并提出了新的评估指标pass@o，结果显示大型语言模型在面向对象编程方面仍需改进。
link: http://arxiv.org/abs/2401.06628v2
publish_time: 2024-01-12

title: Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study
abstract: 大型语言模型在编程语言分析中得到广泛应用以提高人类生产力，然而，它们的可靠性可能受到各种代码分布变化的影响，导致输出结果不一致。概率方法通过不确定性校准和估计已被证明可以缓解这种影响，但在语言领域的应用相对于图像任务仍未得到充分探索。本研究首先引入了一个大规模基准数据集，涵盖了三种不同强度的实际代码分布变化模式。然后，我们对最先进的概率方法在CodeLlama上应用这些变化的代码片段进行了深入研究。我们观察到这些方法通常提高了CodeLlama的不确定性意识，提高了校准质量和更高的不确定性估计精度。然而，我们的研究进一步揭示了在不同标准（如校准误差与误分类检测）之间的表现动态变化，以及在效力和效率之间的权衡，强调了针对具体情境的必要方法选择。
link: http://arxiv.org/abs/2402.05939v1
publish_time: 2024-01-12

title: LEGOBench: Scientific Leaderboard Generation Benchmark
abstract: LEGOBench是一个用于评估生成科学榜单系统的基准，展示了自动榜单生成领域的最新研究进展和较大的性能差距。
link: http://arxiv.org/abs/2401.06233v2
publish_time: 2024-01-11

title: Hallucination Benchmark in Medical Visual Question Answering
abstract: 最近在视觉问答（VQA）方面取得的大型语言和视觉模型的成功，尤其是在医学领域（Med-VQA）的应用，展示了实现有效医疗视觉助手的巨大潜力。
link: http://arxiv.org/abs/2401.05827v1
publish_time: 2024-01-11

title: Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems
abstract: 大型语言模型(LLMs)在解决多样化自然语言处理任务方面具有强大的能力，然而LLM系统的安全和安全问题已成为广泛应用的主要障碍。许多研究已广泛调查了LLM系统中的风险，并制定了相应的缓解策略。顶尖企业如OpenAI、Google、Meta和Anthropic也已在负责任的LLMs上做出了大量努力。因此，有必要组织现有研究并为社区建立全面的分类体系。在本文中，我们深入探讨了LLM系统的四个基本模块，包括用于接收提示的输入模块，基于大规模语料库训练的语言模型，用于开发和部署的工具链模块，以及用于导出LLM生成内容的输出模块。基于此，我们提出了一个全面的分类体系，系统分析了LLM系统每个模块可能涉及的潜在风险，并讨论了相应的缓解策略。此外，我们还审查了流行的基准，旨在促进LLM系统的风险评估。我们希望本文能帮助LLM参与者以系统的角度构建他们的负责任LLM系统。
link: http://arxiv.org/abs/2401.05778v1
publish_time: 2024-01-11

title: REBUS: A Robust Evaluation Benchmark of Understanding Symbols
abstract: 本文提出了一个新的基准评估多模态大型语言模型在谜题中的表现，数据集包含333个原始的基于图像的文字游戏示例，涵盖了13个类别，如电影、作曲家、主要城市和食物。为了在识别提示的单词或短语上取得良好的表现，模型必须结合图像识别和字符串操作，还需要假设检验、多步推理和人类认知的理解，使得成为一个复杂的多模态能力评估。我们发现专有模型如GPT-4V和Gemini Pro明显胜过所有其他测试模型。然而，即使是最好的模型最终准确率仅为24％，突显出推理方面需要大幅改进。此外，模型很少能理解谜题的所有部分，几乎总是无法事后解释出正确答案。因此，我们的基准可以用于识别多模态大型语言模型知识和推理方面的主要缺陷。
link: http://arxiv.org/abs/2401.05604v1
publish_time: 2024-01-11

title: ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain
abstract: 此论文介绍了一种中文多项选择题评估基准ANGO，通过关键点分类标准和问题难度分级，提高了评估结果的可解释性和训练指导性，利用独特的抽样策略和评估框架减少数据泄漏影响并充分发挥ANGO的创新特性，实验证明ANGO对模型提出更强挑战，比现有基准更详细地揭示了评估结果。
link: http://arxiv.org/abs/2401.04898v2
publish_time: 2024-01-10

title: TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models
abstract: 大语言模型（LLMs）和多模态大语言模型（MLLMs）展现出出色的通用能力，甚至在许多专业领域如法律、经济、交通和医学中表现出高度的适应性。现在，许多领域特定的基准已经被提出来验证（M）LLMs在特定领域的表现。在各种领域中，交通在现代社会中起着至关重要的作用，因为它影响着经济、环境和数十亿人类的生活质量。然而，目前还不清楚（M）LLMs到底具有多少交通知识，以及它们能否可靠地执行与交通相关的任务。为了弥补这一空白，我们提出了TransportationGames，这是一个精心设计且全面评估（M）LLMs在交通领域的评估基准。通过全面考虑现实场景中的应用，并参考布鲁姆分类法中的前三个层次，我们通过所选任务测试各种（M）LLMs在记忆、理解和应用交通知识方面的表现。实验结果显示，尽管一些模型在某些任务中表现出色，但整体上仍有很大的改进空间。我们希望发布TransportationGames可以成为未来研究的基础，从而加速在交通领域中（M）LLMs的实施和应用。
link: http://arxiv.org/abs/2401.04471v1
publish_time: 2024-01-09

title: Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark
abstract: 人工智能在多个领域取得了显著进展，像ChatGPT这样的大型语言模型因其人类化的文本生成能力而备受关注。尽管取得了这些成就，空间推理对这些模型仍然是一个重要挑战。StepGame等基准评估AI的空间推理，ChatGPT在其中表现不尽人意，但基准中的模板错误会影响评估结果。因此，如果解决了这些模板错误，ChatGPT有潜力表现更好，从而对其空间推理能力进行更准确的评估。本研究对StepGame基准进行了细化，提供了更准确的数据集用于模型评估。我们分析了GPT在修正后的基准上的空间推理表现，在将自然语言文本映射到空间关系方面表现出熟练，但在多跳推理方面存在局限。我们通过将模板映射与基于逻辑的推理相结合，提供了无误的解决方案，展示了在StepGame上进行定性推理的熟练性而无任何错误。然后，我们解决了GPT模型在空间推理方面的局限性。我们采用了链式思维和思维树提示策略，揭示了GPT的“认知过程”，在准确性方面取得了显著的提升。我们的研究不仅揭示了模型的不足之处，还提出了改进方案，有助于提高AI的更强大的空间推理能力。
link: http://arxiv.org/abs/2401.03991v1
publish_time: 2024-01-08

title: PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
abstract: 通过对Python代码生成的两个流行基准测试的人类评估，发现存在编程概念的偏见和简单任务的普遍现象，为此我们提出了PythonSaga基准测试以解决这些问题。
link: http://arxiv.org/abs/2401.03855v2
publish_time: 2024-01-08

title: German Text Embedding Clustering Benchmark
abstract: 本研究介绍了一个评估在不同领域中对德语文本嵌入进行聚类性能的基准。这个基准是由聚类神经文本嵌入在需要对文本进行分组的任务（如主题建模）中越来越多的使用以及现有基准中对德语资源的需求所驱动的。我们对一系列经过预训练的单语和多语模型在不同聚类算法结果上的表现进行了初步分析。结果包括表现强劲的单语和多语模型。减少嵌入的维度可以进一步改善聚类效果。此外，我们对继续对德语BERT模型进行预训练的实验，以评估这种额外训练的好处。我们的实验表明，在短文本中可能会有显著的性能提升。所有代码和数据集均已公开。
link: http://arxiv.org/abs/2401.02709v1
publish_time: 2024-01-05

title: CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation
abstract: 最近，大型语言模型的出现彻底改变了生成式代理。其中，角色扮演对话代理引起了广泛关注，因为它们具有引起用户情绪的能力。然而，在这一领域中缺乏全面的基准测试阻碍了进展。为了弥补这一差距，我们介绍了CharacterEval，一个用于全面评估RPCA的中文基准，配备了一个定制的高质量数据集。
link: http://arxiv.org/abs/2401.01275v2
publish_time: 2024-01-02

title: BIBench: Benchmarking Data Analysis Knowledge of Large Language Models
abstract: 大型语言模型在许多领域展现出令人印象深刻的能力，但在数据分析领域的熟练度和可靠性尚不确定。为了弥合这一差距，我们引入了BIBench，一个旨在评估LLMs数据分析能力的综合基准。
link: http://arxiv.org/abs/2401.02982v3
publish_time: 2024-01-01

title: Benchmarking Large Language Models on Controllable Generation under Diversified Instructions
abstract: 通过新的基准测试 CoDI-Eval，我们系统地评估了大型语言模型对具有各种约束的指令的响应能力，发现代表性的LLMs在遵循具体约束的指令上存在限制，且开源和商业闭源LLMs之间存在显著差距，这一基准测试将促进改善LLMs对指令的可控性的研究。
link: http://arxiv.org/abs/2401.00690v1
publish_time: 2024-01-01

title: RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models
abstract: 快速发展的大型语言模型需要有效的基准来评估它们的角色知识，这对于与现实世界建立联系和提供更沉浸式互动至关重要。RoleEval是一个双语基准，旨在评估角色知识的记忆、利用和推理能力。RoleEval由RoleEval-Global（包括国际知名人物）和RoleEval-Chinese（包括中国热门人物）组成，涵盖300位来自名人、动漫、漫画、电影、电视剧、游戏和小说等领域的影响力人物和虚构角色，提出6000个中英文平行多选题，涵盖基础知识和多跳推理能力，旨在系统地探究角色的个人信息、关系、能力和经历等各个方面。对于保持高标准，我们进行了自动和人工验证相结合的混合质量检查流程，确保问题多样、具有挑战性和有鉴别性。我们通过在各种开源和专有大型语言模型上对RoleEval进行广泛评估，发现了有见地的结果。值得注意的是，尽管GPT-4在RoleEval-Global上表现优于其他模型，但中国大型语言模型在RoleEval-Chinese上表现出色，突显了显著的知识分布差异。我们期望RoleEval能够强调在不同语言和文化环境中评估大型语言模型的角色知识的重要性。
link: http://arxiv.org/abs/2312.16132v2
publish_time: 2023-12-26

title: DocMSU: A Comprehensive Benchmark for Document-level Multimodal Sarcasm Understanding
abstract: 通过引入文档级多模态讽刺理解（DocMSU），我们填补了现有多模态讽刺理解基准和方法在新闻领域的空白，使得研究者更好地应对真实世界情境下的文档级讽刺理解挑战。
link: http://arxiv.org/abs/2312.16023v1
publish_time: 2023-12-26

title: NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes
abstract: LLM的推理能力是其重要特征之一，对于复杂决策任务至关重要，NPHardEval基准旨在评估LLM在广泛算法问题中的推理能力。
link: http://arxiv.org/abs/2312.14890v4
publish_time: 2023-12-22

title: Computational Semantics and Evaluation Benchmark for Interrogative Sentences via Combinatory Categorial Grammar
abstract: 在组合范畴语法（CCG）框架内提出了各种极性疑问句和疑问句的组合语义，通过引入一个特别设计用于评估疑问句语义的问答数据集QSEM来评估我们提出的分析的解释能力。我们利用现有的CCG解析器实施了我们的分析，并使用数据集进行评估。通过评估，我们已为QSEM中约一半的样本获得了带有CCG树和语义表示的注释数据。此外，我们讨论了CCG的理论能力与现有CCG解析器的能力之间的差距。
link: http://arxiv.org/abs/2312.14737v1
publish_time: 2023-12-22

title: EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models
abstract: EmphAssess 是一个用于评估语音到语音模型编码和重现语调强调能力的基准，可用于语音重构和语音到语音翻译任务，其中包括 EmphaClass 模型用于强调分类。
link: http://arxiv.org/abs/2312.14069v1
publish_time: 2023-12-21

title: Text2Analysis: A Benchmark of Table Question Answering with Advanced Data Analysis and Unclear Queries
abstract: 表格数据分析在各个领域至关重要，而大型语言模型在这一领域展现出了巨大的潜力。然而，目前的研究大多集中在基础任务，如Text2SQL和TableQA，而忽视了像预测和图表生成这样的高级分析。为了填补这一空白，我们开发了Text2Analysis基准测试，包括超越SQL兼容操作，需要更深入分析的高级分析任务。我们还开发了五种创新和有效的注释方法，利用大型语言模型的能力来提高数据质量和数量。此外，我们还包括类似真实用户问题的不清晰查询，以测试模型对这些挑战的理解和处理能力。最后，我们收集了2249个查询结果对和347个表格。我们使用三种不同的指标评估了五种最先进的模型，结果显示我们的基准测试为表格数据分析领域提出了相当大的挑战，为更多先进研究机会铺平了道路。
link: http://arxiv.org/abs/2312.13671v1
publish_time: 2023-12-21

title: How to Prune Your Language Model: Recovering Accuracy on the "Sparsity May Cry'' Benchmark
abstract: BERT类模型的修剪已成为标准压缩基准，现存方法被SMC基准质疑，但我们提出了成功修剪的一般性指导方针，可以取得最先进的结果。
link: http://arxiv.org/abs/2312.13547v1
publish_time: 2023-12-21

title: Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
abstract: 将大型语言模型（LLMs）与外部内容整合，使LLMs能够实现更为及时和广泛的应用，例如微软的Copilot。然而，这种整合也使LLMs面临间接提示注入攻击的风险，攻击者可以在外部内容中嵌入恶意指令，破坏LLM的输出并导致响应偏离用户预期。我们引入了第一个间接提示注入攻击的基准标准BIPIA，以评估此类攻击的风险。基于评估，我们对攻击成功的根本原因进行了重要分析，即LLMs无法区分指令和外部内容，也意识不到不应执行外部内容中的指令。基于这一分析，我们开发了基于提示学习的两种黑盒方法和基于对抗性训练的白盒防御方法。实验结果表明，黑盒防御在减少这些攻击方面非常有效，而白盒防御将攻击成功率降至接近零。总体而言，我们通过引入基准标准、分析攻击成功的根本原因以及开发一套初步防御方法，系统地研究了间接提示注入攻击。
link: http://arxiv.org/abs/2312.14197v3
publish_time: 2023-12-21

title: Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised Learning for Biomedical Knowledge Curation: a focused study on chemical entities of biological interest
abstract: 自动化知识答题对于生物医学本体的维护至关重要，以确保其保持全面、高质量和更新。在基础语言模型时代，本研究比较和分析了三种自然语言处理范式：上下文学习（ICL），微调（FT）和监督学习（ML）在策划任务中的表现。ICL模型中，GPT-4在任务1和3中表现出色，在小于6,000个三元组的情况下超过ML/FT。
link: http://arxiv.org/abs/2312.12989v1
publish_time: 2023-12-20

title: CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models
abstract: 本文提出了CORECODE，一个包含丰富常识知识的数据集，用于评估中文大型语言模型(LLMs)在现实场景中的常识推理和常识冲突检测能力。我们将日常对话中的常识知识分为实体、事件和社会互动三个维度，并通过众包收集了19,700个对话中的76,787个常识知识注释。我们建立了一系列对话级别的推理和检测任务，包括常识填充、常识生成、常识冲突短语检测、领域识别、插槽识别和事件因果推断等。实验结果表明，现有的中文LLMs在CORECODE数据集上无法准确预测丰富的推理内容，甚至ChatGPT在零样本设置下对领域识别和插槽识别任务的准确率仅为0.275和0.084。我们在https://github.com/danshi777/CORECODE发布了CORECODE的数据和代码，以促进对日常对话环境下LLMs的常识推理评估和研究。
link: http://arxiv.org/abs/2312.12853v1
publish_time: 2023-12-20

title: MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models
abstract: 众多医学领域的大型语言模型（LLMs）的出现凸显了对统一评估标准的需求，手动评估LLMs证明是耗时且劳动密集的。为解决这一问题，我们引入了MedBench，这是一个涵盖40,041个问题的中文医学领域综合基准，来源于真实考试练习和各种医学报告。MedBench由四个关键组成部分构成，包括中华人民共和国医师资格考试、住院医师标准化培训考试、主治医师资格考试以及涵盖检查、诊断和治疗等真实临床案例。MedBench复制了中国大陆医生的教育进程和临床实践经验，因此确立自身作为评估医学语言学习模型知识掌握和推理能力的可信基准。我们进行了广泛实验和深入分析，得出以下结论：（1）中国医学LLMs在这一基准测试中表现不佳，突显了在临床知识和诊断准确性方面需要有重大进展。 （2）一些通用领域的LLMs出人意料地具有相当的医学知识。这些发现阐明了在MedBench背景下LLMs的能力和限制，旨在帮助医学研究社区。
link: http://arxiv.org/abs/2312.12806v1
publish_time: 2023-12-20

title: ALMANACS: A Simulatability Benchmark for Language Model Explainability
abstract: ALMANACS是一个语言模型可解释性基准测试，评估解释方法的模拟性，结果显示目前没有一种解释方法能胜过无解释控制组，表明在ALMANACS上开发能帮助模拟性的解释方法仍然是一个挑战。
link: http://arxiv.org/abs/2312.12747v1
publish_time: 2023-12-20

title: Paloma: A Benchmark for Evaluating Language Model Fit
abstract: Paloma使用多样化的文本领域进行语言模型评估，结果显示预训练所使用的数据对不同领域的适应性存在差异。
link: http://arxiv.org/abs/2312.10523v1
publish_time: 2023-12-16

title: Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models
abstract: 通过研究LLMs在二进制代码理解方面的潜力，我们提出了BinSum综合基准和数据集，并引入了一种新的提示综合和优化方法，以更准确地衡量LLM的性能。我们的评估揭示了10个关键见解，揭示了LLMs在该领域的变革潜力以及尚需克服的挑战。
link: http://arxiv.org/abs/2312.09601v1
publish_time: 2023-12-15

title: BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering
abstract: Med-VQA是医疗行业中非常重要的任务，可以通过医学图像回答自然语言问题。BESTMVQA系统通过自动生成医学VQA数据集和在统一实验设置中评估SOTAs来解决数据不足和再现性问题，为用户提供了一个方便的工具进行全面实证研究。
link: http://arxiv.org/abs/2312.07867v1
publish_time: 2023-12-13

title: SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models
abstract: 当前用于不想要社会偏见审计的数据集仅限于研究种族和性别等受保护的人口特征。本研究引入了一个全面的基准，旨在捕捉生成性语言模型中通过污名来放大社会偏见。 我们从社会科学研究中获得灵感，首先列出一个包含93种美国为中心的污名的记录清单，并策划了一个涉及简单社交情境的问答(QA)数据集。我们的基准，SocialStigmaQA，包含大约10K条提示，具有各种提示风格，经过精心设计，旨在系统地测试社会偏见和模型的稳健性。我们使用两种开源生成性语言模型对SocialStigmaQA进行了结果展示，发现在各种解码策略和提示风格中，产生社会偏见输出的比例介于45%到59%之间。我们展示了我们基准中模板的刻意设计（例如在提示中添加偏见文字或使用不同动词改变回答以指示偏见）如何影响模型生成社会偏见输出的倾向。此外，通过手动评估，我们发现生成的思维链输出中存在从微妙偏见到缺乏推理的问题模式。警告：本文包含有毒、偏见和潜在有害的文本示例。
link: http://arxiv.org/abs/2312.07492v4
publish_time: 2023-12-12

title: EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models
abstract: EQ-Bench是一项新颖的基准测试，旨在评估大型语言模型（LLMs）中情感智力的各个方面，通过要求它们预测对话中角色情绪状态的强度，我们发现EQ-Bench与MMLU等综合多领域基准测试之间有强相关性（r=0.97），可有效区分各种模型，产生高度一致的结果。
link: http://arxiv.org/abs/2312.06281v2
publish_time: 2023-12-11

title: EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models
abstract: Multimodal Large Language Models (MLLMs)通过强大的大规模语言模型（LLMs）的卓越推理和泛化能力，开辟了新的实体任务规划途径，在集成各种环境输入方面表现出色，这对于执行任务规划至关重要。我们引入了一个具有人类注释的基准，EgoPlan-Bench，以定量研究MLLMs作为实体任务规划器在现实场景中的潜力。我们进一步构建了一项指令调整数据集EgoPlan-IT，以促进在复杂的真实世界情况下学习高级任务规划。
link: http://arxiv.org/abs/2312.06722v1
publish_time: 2023-12-11

title: OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization
abstract: 自动摘要模型的性能在最近几年有了显著提升，但在实际场景中仍存在着无法满足用户具体信息需求的差距，尤其是在寻求针对性摘要时，如本文所述的有用的基于方面的摘要设置。以往针对这一设置的数据集和研究主要集中在有限的预定义方面上，仅关注于单一文档输入，或依赖于合成数据。为了推进更加真实的场景研究，我们引入了OpenAsp，一个用于多文档“开放”基于方面的摘要的基准。该基准使用一种新颖且经济高效的注释协议创建，从现有的通用多文档摘要数据集中衍生出开放方面数据集。我们分析了OpenAsp的特性，展示了其高质量内容。此外，我们还展示了OpenAsp中实现的逼真开放方面设置对当前最先进的摘要模型以及大型语言模型构成了挑战。
link: http://arxiv.org/abs/2312.04440v1
publish_time: 2023-12-07

title: LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs
abstract: LaMPilot是一个在自动驾驶领域的全新规划框架，把任务重新定义为利用已建立的行为原语来生成代码的过程，旨在解决执行类似“超车前方车辆”等临时用户指令的挑战，我们推出了LaMPilot基准测试，并在此基础上评估了一系列最新的代码生成语言模型。实验结果表明，GPT-4在人类反馈的帮助下，完成率高达92.7%，碰撞率仅为0.9%。我们将提供代码和数据集以促进这一领域的进一步研究。
link: http://arxiv.org/abs/2312.04372v1
publish_time: 2023-12-07

title: Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique
abstract: 该论文介绍了一种新颖的基准评估框架Dyport，用于评估生物医学假设生成系统。利用精心策划的数据集，我们的方法在真实条件下测试这些系统，提高了评估的相关性。我们将来自策划数据库的知识集成到一个动态图中，并伴随一种量化发现重要性的方法。这不仅评估了假设的准确性，还评估了它们在生物医学研究中的潜在影响，显著扩展了传统的链接预测基准。我们的基准评估过程可应用于应用于生物医学语义知识图上的几个链接预测系统。我们的基准评估系统设计灵活，旨在广泛应用于假设生成质量验证，旨在扩大生物医学研究社区的科学发现范围。Dyport 框架完全开源。所有代码和数据集都可在以下链接获取：https://github.com/IlyaTyagin/Dyport.
link: http://arxiv.org/abs/2312.03303v1
publish_time: 2023-12-06

title: Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models
abstract: 大规模视觉和语言模型在全监督和零样本视觉任务中取得了显著进展，而这些大型预训练架构作为目前所谓的指导调整大型视觉语言模型的基准。IT-LVLMs是通用的多模态助手，其响应受自然语言指令和任意视觉数据调节。尽管如此，由于缺乏标准化评估基准，IT-LVLM在基本计算机视觉问题上的有效性仍不清楚。这篇论文介绍了一个名为MERLIM的多模态评估基准测试，一个可伸缩的测试平台，可评估IT-LVLM在基本计算机视觉任务上的表现。 MERLIM包含超过279K个图像-问题对，并且重点关注IT-LVLM中跨模态“幻觉”事件的检测，其中语言输出指的是缺乏有效图像基础的视觉概念。我们的结果显示，最先进的IT-LVMLs在识别细粒度视觉概念方面仍受限制，对象幻觉在任务中很常见，并且他们的结果受输入查询的微小变化强烈影响，即使查询有相同的语义。我们的发现还表明，这些模型在视觉基础上有弱点，但仍可以通过LLM组件中包含的全局视觉模式或文本偏见做出合适的猜测。
link: http://arxiv.org/abs/2312.02219v1
publish_time: 2023-12-03

title: NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian
abstract: 最近对生成式语言模型（GLMs）的高级技术推动了自然语言处理（NLP）的发展，展示了利用预训练GLM知识进行多样应用的“预训练，提示和预测”范式的有效性。然而，由于缺乏全面基准测试，特别是缺乏低资源语言的基准，这些潜力仍然缺乏充分的定量评价。为弥补这些差距，我们引入了NLEBench，一个专为评估挪威人低资源语言的自然语言生成能力而设计的全面基准。NorGLM的系统评估提供了对NorGLM在各种下游任务中的能力和可伸缩性的洞见。
link: http://arxiv.org/abs/2312.01314v1
publish_time: 2023-12-03

title: A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval
abstract: 现有的长视频检索系统在段萼到视频检索范式中进行训练和测试，其中每个长视频都由一个长段落描述。这忽略了视频可能具有的丰富性和多样性的描述，可以通过逐时细节或单短语摘要来描述视频，或介于两者之间的任何形式。为了更全面评估长视频检索系统的功能，我们提出了一个管道，利用最先进的大型语言模型仔细生成多样化的长视频合成标题。我们通过严格的人工检查验证了这个管道的可靠性。然后，在几个长视频数据集上使用这些合成标题对一组代表性的视频语言模型进行基准测试，结果表明它们在变换后的数据中表现糟糕，尤其是最短标题。我们还提出了一种轻量级的微调方法，其中我们使用对比损失学习基于各种标题之间不同信息水平的分层嵌入损失。我们的方法不仅改善了下游段落到视频检索任务的性能（ActivityNet上+1.1%的R@1），而且在使用我们的合成数据计算的各种长视频检索指标上也有所提高（ActivityNet上短描述+3.6%的R@1）。有关数据访问和其他详细信息，请参阅我们的项目网站https://mgwillia.github.io/10k-words。
link: http://arxiv.org/abs/2312.00115v1
publish_time: 2023-11-30

title: TaskBench: Benchmarking Large Language Models for Task Automation
abstract: 大语言模型(LLMs)的惊人进展点燃了任务自动化的火花，这将用户指令描述的复杂任务分解为子任务，并调用外部工具来执行它们，在自主代理中扮演了核心角色。然而，缺乏系统化和标准化的基准来促进LLMs在任务自动化中的发展。因此，我们引入了TaskBench来评估LLMs在任务自动化中的能力。具体而言，任务自动化可以被分解为三个关键阶段：任务分解、工具调用和参数预测以实现用户意图。这种复杂性使得数据收集和评估比普通自然语言处理任务更具挑战性。为了生成高质量的评估数据集，我们引入了Tool Graph的概念来表示用户意图中的分解任务，并采用了一种反指导方法来模拟用户指令和注释。此外，我们提出了TaskEval来从不同方面评估LLMs的能力，包括任务分解、工具调用和参数预测。实验结果表明，TaskBench能有效地反映LLMs在任务自动化中的能力。受益于自动化数据构建和人工验证的混合，TaskBench与人工评估相比具有高一致性，可作为基于LLM的自主代理的全面和忠实的基准。
link: http://arxiv.org/abs/2311.18760v2
publish_time: 2023-11-30

title: AlignBench: Benchmarking Chinese Alignment of Large Language Models
abstract: 对于指导调优的大型语言模型来说，对齐已经成为它们成为有用的助手的关键步骤。然而，对于新兴的中文大型语言模型来说，对齐的有效评估仍然严重不足，需要有针对性地针对对齐进行真实场景基础、开放式、具有挑战性和自动化的评估。为了填补这一空白，我们引入了AlignBench，一个用于评估中文大型语言模型对齐的综合多维基准。我们的基准配备了一个人与计算机数据处理管道，利用规则校准的多维度语言模型作为评判者，并采用思维链来生成解释和最终评分，确保高可靠性和可解释性。此外，我们报道了CritiqueLLM评估的AlignBench，这是一个专门的中文评估语言模型，可以恢复95%的GPT-4的评估能力。我们将提供公共API，用于使用CritiqueLLM评估AlignBench，以促进对中文大型语言模型对齐的评估。所有评估代码、数据和语言模型生成均可在\url{https://github.com/THUDM/AlignBench}上找到。
link: http://arxiv.org/abs/2311.18743v3
publish_time: 2023-11-30

title: ArcMMLU: A Library and Information Science Benchmark for Large Language Models
abstract: 为了准确评估大型语言模型（LLMs）的能力，迫切需要开发严格的领域特定评估基准。ArcMMLU是为中国的图书情报科学（LIS）领域量身定制的专业基准，旨在衡量LLMs在档案学、数据科学、图书馆科学和信息科学四个子领域内的知识和推理能力。虽然大多数主流LLMs在ArcMMLU上的平均准确率超过50％，但仍存在明显的性能差距，为LLMs在LIS领域的能力提供改进空间。ArcMMLU的发布填补了中国LIS领域LLM评估的重要空白，并为未来定制LLMs的发展铺平了道路。
link: http://arxiv.org/abs/2311.18658v1
publish_time: 2023-11-30

title: LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models
abstract: 大型语言模型(LLMs)提供了出色的文本生成能力，但标准提示和生成方法通常并不能导致有意识或目标导向的代理，并可能需要大量的提示调整。这在多轮对话中尤为明显：即使是目前最好的LLMs也很少提出澄清问题，进行显式信息收集，或是在多轮后采取导致更好决策的行动。强化学习有可能利用LLMs强大的建模能力，以及它们对文本交互的内部表示，创建有能力的目标导向语言代理。这可以通过协调的劝说和精心设计的问题来实现有意识和时间延伸的交互，比如与人类的交互，或者通过文字游戏实现目标导向的娱乐，以达到期望的最终结果。然而，实现这一点需要社区开发稳定可靠的强化学习算法，以有效训练LLMs。开发这样的算法需要能够评估算法设计进展、为多轮交互提供易于访问和可重现的评估，并涵盖一系列任务属性和挑战以改进强化学习算法。我们的论文介绍了用于评估LLMs的多轮RL的LMRL-Gym基准，以及一个包含用于开始基于价值和策略的离线RL方法的基本工具包的开源研究框架。我们的基准包括8个不同的语言任务，这些任务需要多轮语言交互，并涵盖开放式对话和文字游戏中的一系列任务。
link: http://arxiv.org/abs/2311.18232v1
publish_time: 2023-11-30

title: UniIR: Training and Benchmarking Universal Multimodal Information Retrievers
abstract: UniIR是一个统一的指导多模式检索器，能够处理不同的检索任务需求，并通过多任务训练和指导调优实现了泛化能力，为多模态信息检索领域建立了新的标准。
link: http://arxiv.org/abs/2311.17136v1
publish_time: 2023-11-28

title: A Benchmark for Evaluating Machine Translation Metrics on Dialects Without Standard Orthography
abstract: 在自然语言处理领域取得可靠进展，对我们所使用的评估指标的局限性有清醒认识非常重要。本研究评估了这些指标对非标准方言的鲁棒性，即语言变体中拼写差异的情况。我们收集了英语到两种瑞士德语方言的人工翻译和人工评价数据集，进一步创建了一个方言变体挑战集，并评估了现有指标的表现。结果表明现有指标无法可靠地评估瑞士德语文本生成输出，特别是在段落级别。我们提出了初步的设计调整，增强对非标准方言的鲁棒性，尽管仍有很多改进空间。数据集、代码和模型可在此处获取：https://github.com/textshuttle/dialect_eval。
link: http://arxiv.org/abs/2311.16865v1
publish_time: 2023-11-28

title: CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models
abstract: 随着大型语言模型（LLMs）规模的显著增强，人们越来越关注对齐问题，以确保它们的负责任和道德使用。引入了一个新的评估文化维度的基准CDEval，旨在评估LLMs的文化维度。我们的综合实验为主流LLMs的文化带来了有趣的见解，突出了在不同维度和领域之间的一致性和变化。研究结果强调了在LLM开发中整合文化因素的重要性，特别是在不同文化设置中的应用。通过CDEval，我们旨在通过包含文化维度来扩展LLM对齐研究的视野，为未来LLMs的发展和评估提供更全面的框架。这一基准是LLMs文化研究的宝贵资源，为更多文化意识和敏感度的模型铺平道路。
link: http://arxiv.org/abs/2311.16421v2
publish_time: 2023-11-28

title: FakeWatch ElectionShield: A Benchmarking Framework to Detect Fake News for Credible US Elections
abstract: 在当今技术驱动的世界中，虚假新闻的传播，尤其是在关键事件如选举期间，对信息的完整性带来越来越大的挑战。为了解决这一挑战，我们引入了FakeWatch ElectionShield，一个精心设计用于检测虚假新闻的创新框架。我们通过先进的语言模型和彻底的人工验证，创造了一个新颖的北美选举相关新闻文章数据集，以提高准确性和相关性。我们提出了一个语言模型中心的模型用于识别虚假新闻。我们的目标是为研究界提供适应性强且准确的分类模型，以识别虚假信息动态性。对我们的数据集和基准数据集上虚假新闻分类器的广泛评估表明，尽管最先进的语言模型略优于传统机器学习模型，但传统模型在准确性、可解释性和计算效率的平衡上仍具竞争力。这项研究为未来解决与选举相关的虚假信息奠定了基础。
link: http://arxiv.org/abs/2312.03730v2
publish_time: 2023-11-27

title: Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection
abstract: 尽管数据选择方法在主动学习、数据修剪和数据增强设置中得到广泛研究，但在工业规模设置中，特别是在资源匮乏的语言中，这些方法的有效性几乎没有证据。我们的工作提出了在这些环境中评估潜在的训练样本的"有用性"或"困难性"的方法。我们还展示了如何使用这些度量来选择训练监督机器学习模型的重要样本。我们主要使用熵和错误 L2 范数（EL2N）得分进行实验。我们使用这些度量从大量的“弱信号标记”数据中筛选高质量数据集，其中在推理过程中为其分配无缺陷高置信度假设作为地面真实标签。然后，我们使用这些去标识化的数据集进行训练数据增强实验，并证明基于得分的选择相对于随机选择的基线技术可以使语义错误率减少 2%，领域分类错误率减少 4%-7%。
link: http://arxiv.org/abs/2311.16302v1
publish_time: 2023-11-27

title: How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs
abstract: 本文聚焦于视觉大型语言模型（VLLM）在视觉推理中的潜力，在评估标准性能之外引入了全面的安全评估套件，涵盖了分布外泛化和对抗鲁棒性。
link: http://arxiv.org/abs/2311.16101v1
publish_time: 2023-11-27

title: MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
abstract: MMM是一个新的基准测试，旨在评估多模态模型在需要大量学科知识和深思熟虑的大规模多学科任务上的表现。
link: http://arxiv.org/abs/2311.16502v3
publish_time: 2023-11-27

title: WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models
abstract: 通过WorldSense基准测试，我们发现目前最先进的聊天型LLMs在保持世界模型方面存在一定的局限性，即使进行细节问题学习也无法实现泛化。
link: http://arxiv.org/abs/2311.15930v1
publish_time: 2023-11-27

title: UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation
abstract: 大型语言模型(LLMs)已经成为当代自然语言处理中至关重要的贡献者，并在各行各业日益广泛应用。然而，这些大规模的概率统计模型目前无法确保在专业内容生成中必要的质量。这些模型经常产生幻觉文本，影响它们在专业背景下的实际效用。为了评估LLMs在文本生成中的真实可靠性，许多项目已经开发出了用于评估幻觉现象的基准测试。然而，这些基准测试通常因成本和时间约束而使用受限制的生成技术。这些技术包括使用导向性幻觉诱导和故意改变真实文本以产生幻觉的策略。这些方法与现实世界应用所要求的无限制文本生成不相符。此外，目前缺乏一个旨在评估中文语言模型幻觉生成的成熟的中文语言数据集。因此，我们开发了一个无限制幻觉生成评估(UHGEval)基准测试，旨在汇编LLMs以最小限制产生的输出。同时，我们建立了一个全面的基准测试评估框架，以帮助后续研究者进行可扩展和可重复的实验。我们还进行了大量实验，评估了知名的中文语言模型和GPT系列模型，以获得有关幻觉挑战的专业性能洞见。
link: http://arxiv.org/abs/2311.15296v2
publish_time: 2023-11-26

title: Benchmarking Large Language Model Volatility
abstract: 大型语言模型产生的非确定性输出对于金融文本理解任务的影响尚未得到很好的研究。通过对投资美国股市的新闻情绪分析进行一个引人入胜的案例研究，我们发现在句子级情感分类结果中存在着相当大的变化，凸显出LLM输出的固有波动性。这些不确定性会向下游传递，导致投资组合构建和回报出现更大的变化。尽管调整语言模型解码器中的温度参数可能是一个潜在的补救方法，但也会抑制创造力。同样，虽然组合多个输出可以减轻不稳定输出的影响，但这需要显著的计算投入。这项工作为从业者提供了宝贵的见解，以便在金融决策中灵活应对LLMs融入的不确定性，特别是在非确定性信息所主导的情况下。
link: http://arxiv.org/abs/2311.15180v1
publish_time: 2023-11-26

title: Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark
abstract: 当前的自动歌词转录（ALT）基准主要关注于单词内容，忽略了歌词的更细微的细节，包括格式和标点符号，这可能会导致与音乐家和词曲作者的创作产物以及听众的体验存在潜在错位。 举例来说，断句在传达有关节奏、情感强调、押韵和高层结构等信息方面至关重要。 为了解决这个问题，我们引入了Jam-ALT，一种基于JamendoLyrics数据集的新歌词转录基准。 我们的贡献是双重的。 首先，完全修订了转录，专门针对ALT评估，遵循了一个新创建的注释指南，统一了音乐行业的指导原则，涵盖了标点、断行、拼写、背景声音和非词语声音等方面。 其次，我们设计了一套评估指标，与传统的词错误率不同，旨在捕捉这些现象。 我们希望所提出的基准能够为ALT任务做出贡献，使歌词转录系统的评估更加精确可靠，并增强歌词应用的用户体验，例如实时字幕渲染或卡拉OK。
link: http://arxiv.org/abs/2311.13987v1
publish_time: 2023-11-23

title: Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark
abstract: 大型语言模型如ChatGPT的普及显著推进了语言理解和生成，影响了广泛的应用。然而，这些模型主要擅长文本任务，忽视了真实世界多模态信息的复杂性。本研究介绍了MultiAPI，这是一个旨在扩展LLMs在多模态环境中熟练度的开创性大规模API基准数据集。通过ChatGPT的合作开发，MultiAPI包括235个多样化的API调用和2,038个上下文提示，提供了一个独特的平台评估工具增强的LLMs处理多模态任务。通过全面的实验，我们的研究结果显示，LLMs在API调用决策方面表现出色，但在领域识别、功能选择和参数生成方面面临挑战。此外，令人惊讶的是，辅助上下文实际上可能影响性能。深入的错误分析为解决这些挑战开辟了一条新的道路，为未来LLM研究提出了潜在方向。
link: http://arxiv.org/abs/2311.13053v1
publish_time: 2023-11-21

title: GAIA: a benchmark for General AI Assistants
abstract: GAIA是一个通用人工智能助手基准测试，如果解决了，将代表人工智能研究的一个里程碑。
link: http://arxiv.org/abs/2311.12983v1
publish_time: 2023-11-21

title: IMGTB: A Framework for Machine-Generated Text Detection Benchmarking
abstract: 在大型语言模型生成高质量文本的时代，发展机器生成文本检测方法以避免有害使用或仅仅出于注释目的是必要的。然而，对于这些开发的方法进行适当评估和比较也同样重要。最近，已经提出了一些基准来实现这一目的；然而，随着每月出现的新方法提供稍有不同的评估流程，新方法的整合变得相当具有挑战性。在本文中，我们提出了IMGTB框架，通过简化定制（新）方法和评估数据集的轻松集成，使得机器生成文本检测方法的基准测试更简单。其可配置性和灵活性使得研究和开发新的检测方法更容易，尤其是它们与现有最先进检测器的比较。工具提供的默认分析、指标和可视化遵循了机器生成文本检测基准测试在最新文献中的被认可的做法。
link: http://arxiv.org/abs/2311.12574v1
publish_time: 2023-11-21

title: GPQA: A Graduate-Level Google-Proof Q&A Benchmark
abstract: GPQA是一个由生物学、物理学和化学领域专家撰写的448道多项选择题组成的挑战性数据集，确保高质量且极具挑战性。即使是拥有或正在攻读相应学科博士学位的专家，准确率也只有65%（在回顾时排除明显错误后提高至74%），而高技能非专家验证者的准确率仅为34%，尽管他们平均花费超过30分钟并能自由使用互联网（即这些问题是“防谷歌”的）。这些问题也对最先进的人工智能系统具有挑战性，我们基于最强的GPT-4基础线准确率只有39%。如果我们将未来的人工智能系统用于帮助回答非常困难的问题，例如在发展新的科学知识时，我们需要开发可扩展的监督方法，使人类能够监督其输出，即使监督者本身也是熟练和具备知识。对于熟练非专家和尖端人工智能系统而言，GPQA的难度应该能够启动现实可扩展的监督实验，我们希望这些实验能帮助设计人类专家可靠地从超越人类能力的人工智能系统中获得真实信息的方法。
link: http://arxiv.org/abs/2311.12022v1
publish_time: 2023-11-20

title: FinanceBench: A New Benchmark for Financial Question Answering
abstract: FinanceBench是用于评估LLMs在开放式金融问题回答上性能的首个测试套件，包括10,231个关于上市公司的问题，对应答案和证据字符串。我们测试了16种最新模型配置，发现现有LLMs在金融QA上存在明显局限性，不适合企业使用。
link: http://arxiv.org/abs/2311.11944v1
publish_time: 2023-11-20

title: Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters
abstract: 写作辅助是与人类生活密切相关的应用程序，也是自然语言处理（NLP）研究领域的基础。其目的是提高输入文本的正确性和质量，其中字符检查在检测和纠正错误字符方面至关重要。 Visual-C$^3$是为汉字检查场景设计的第一个真实世界视觉和最大人工制作的数据集，包含了伪造和拼写错误的中文字符。
link: http://arxiv.org/abs/2311.11268v1
publish_time: 2023-11-19

title: Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language
abstract: 本文研究了孟加拉地区方言翻译成标准孟加拉语的机器翻译模型，并成功填补了这一领域的空白。
link: http://arxiv.org/abs/2311.11142v1
publish_time: 2023-11-18

title: Sinhala-English Word Embedding Alignment: Introducing Datasets and Benchmark for a Low Resource Language
abstract: 儘管多語言嵌入式已被廣泛應用在自然語言處理任務中，但由於資料匱乏，低資源語言如僧伽羅語更傾向於使用單語言嵌入式。面對多語言任務時，由於對其進行訓練的嵌入式空間未對齊，因而利用這些單語言嵌入式是具有挑戰性的。為解決這個問題，嵌入對齊任務應運而生，然而，高資源語言受到較多關注，而低資源語言如僧伽羅語似乎被忽略。本文嘗試基於現有的對齊技術來對齊僧伽羅語和英語嵌入式空間，並引入僧伽羅語語言嵌入式對齊的基準。此外，為了促進監督式對齊，我們還引入了僧伽羅語-英語對齊數據集，這些數據集作為我們進行監督式詞嵌入式對齊的錨定數據集。儘管我們的結果不及法語、德語或中文等高資源語言，但我們相信我們的工作為更專業的英語和僧伽羅語嵌入式對齊奠定了基礎。
link: http://arxiv.org/abs/2311.10436v1
publish_time: 2023-11-17

title: PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models
abstract: 伴随着大型语言模型（LLMs）在各个领域普遍应用，迫切需要改进的自然语言处理基准，涵盖个别学科所需的所有知识。psybench 是首个涵盖研究生入学考试所需全部知识的综合性中文评估套件，深度评估心理学模型的优势和劣势。
link: http://arxiv.org/abs/2311.09861v2
publish_time: 2023-11-16

title: AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL
abstract: LLM被越来越多地用于规划式任务，但其规划和推理能力仍未被很好理解，AutoPlanBench 是一种新颖的方法，可将PDDL中的规划基准自动转换为文本描述，并提供了我们的方法创建的基准数据集。我们展示了最优的LLM规划器在某些规划任务上表现出色，但对于其他任务，当前方法仍无法达到。
link: http://arxiv.org/abs/2311.09830v2
publish_time: 2023-11-16

title: FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models
abstract: 评估大型语言模型的遵循指令能力非常重要，FollowEval基准测试旨在通过多种维度评估模型的能力，同时还需进一步提高模型的表现。
link: http://arxiv.org/abs/2311.09829v1
publish_time: 2023-11-16

title: Investigating Data Contamination in Modern Benchmarks for Large Language Models
abstract: 最近的观察突显了难以匹配的基准分数和LLMs实际性能之间的差异，引发了对评估基准可能受污染的担忧，特别是针对缺乏训练数据透明度的封闭源模型和某些开源模型。我们在本文中研究了数据污染问题，提出了两种适用于开源和专有LLMs的方法。通过介绍一个检索系统来探索评估基准和预训练语料库之间的潜在重叠，以及引入一种名为TS-Guessing的新颖调查协议。我们发现某些商业LLMs能在各种测试集中出人意料地猜出缺失选项。我们希望这些结果强调了该领域需要更严格的评估方法和基准的必要性。
link: http://arxiv.org/abs/2311.09783v1
publish_time: 2023-11-16

title: MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification
abstract: 通过使用谬论来传播虚假信息、假新闻和宣传，强调了检测它们的重要性。然而，自动检测和分类谬论仍然具有挑战性，主要是因为任务的固有主观性以及现有研究中需要全面、统一的方法。我们的研究解决了这些局限性，引入了一个新的谬论分类法，对以前的分类进行了修改和精炼，设计了一个专为主观自然语言处理任务定制的新标注方案，并为应对主观性调整了精度、召回率和F1分数度量的新评估方法。使用我们的标注方案，本文引入了MAFALDA（多级注释谬论数据集），这是一个基于各种先前存在的谬论数据集示例的金标准数据集，根据我们的统一分类法在三个粒度级别上进行。然后，我们在零样本学习环境下使用MAFALDA评估了几种语言模型，以评估它们的谬论检测和分类能力。我们的全面评估不仅对这些模型的性能进行了基准测试，还为了解它们在应对谬论推理中的优势和局限性提供了有价值的见解。
link: http://arxiv.org/abs/2311.09761v1
publish_time: 2023-11-16

title: GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding
abstract: 语言模型可以作为软件开发者提高生产力的宝贵工具，大型生成模型可用于代码生成和代码补全，而较小的仅编码器模型则能够通过自然语言查询执行代码搜索任务。
link: http://arxiv.org/abs/2311.09707v1
publish_time: 2023-11-16

title: TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction
abstract: 事件提取因其广泛的应用而引起了极大的兴趣，但最近的研究引起了评估问题的关注，表明报告的分数可能无法准确反映真正的性能。本研究识别并解决了评估中的挑战，包括由不同数据假设或预处理步骤引起的不一致性，当前评估框架的不足，可能引入数据集或数据分割偏差，以及一些先前方法的低可重现性。为了解决这些挑战，我们提出了TextEE，一个标准化、公平和可重现的事件提取基准。TextEE包括标准化的数据预处理脚本和14个跨七个不同领域的数据集的数据拆分，并涵盖14种最新的方法，进行全面的基准重新评估。我们还在TextEE基准上评估了五种不同的大型语言模型，并展示它们在实现令人满意的性能方面存在困难。受我们重新评估结果和发现的启发，我们讨论了事件提取在当前自然语言处理时代的作用，以及从TextEE中得出的未来挑战和见解。我们相信TextEE，作为第一个标准化的全面基准工具，将极大地促进未来的事件提取研究。
link: http://arxiv.org/abs/2311.09562v2
publish_time: 2023-11-16

title: PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health
abstract: 最近，利用大型语言模型（LLMs）在心理健康研究中的应用引起了人们的浓厚兴趣，并展示了其在疾病检测等方面的卓越能力。然而，目前缺乏针对该领域评估LLMs能力的全面基准，因此我们填补了这一空白，引入了首个针对心理健康领域特性量身定制的全面基准。该基准包括总共六个子任务，覆盖三个维度，以系统评估LLMs在心理健康领域的能力。我们为每个子任务设计了简明的提示，并使用我们的基准全面评估了八个先进的LLMs。实验结果不仅显示当前LLMs在心理健康方面有很大改进空间，还揭示了未来模型优化的潜在方向。
link: http://arxiv.org/abs/2311.09189v1
publish_time: 2023-11-15

title: Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization
abstract: 大型语言模型在标准的常规总结基准上已经取得了强大的性能，但它们在更复杂的总结任务设置上的表现还没有得到充分研究。因此，我们对指导可控文本总结进行基准测试，其中模型输入包括源文章和所需摘要特性的自然语言要求。我们针对该任务制定了一个仅供评估的数据集，并对基于LLM的5个摘要系统进行人类评估。然后，我们使用4种不同的评估协议和11个LLM对该任务的LLM-based自动评估进行基准测试，共计40种评估方法。我们的研究显示，对于LLM来说，指导可控文本总结仍然是一项具有挑战性的任务，因为（1）所有评估的LLM仍然在摘要中出现事实和其他类型的错误；（2）所有基于LLM的评估方法在判断候选摘要的质量时无法与人类注释者强有力地对齐；（3）不同的LLM在摘要生成和评估中表现出较大的性能差距。我们公开了我们收集的基准数据集InstruSum，以推动未来在这个方向的研究。
link: http://arxiv.org/abs/2311.09184v1
publish_time: 2023-11-15

title: AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph
abstract: 这篇论文提出了AbsPyramid，这是一个包含22.1K抽象知识描述的统一推理图，用于评估语言模型在开放领域中的抽象能力。实验证明，当前的语言模型在零样本和少样本情况下面临着理解抽象知识的挑战。通过训练我们的丰富抽象知识，我们发现语言模型可以获得基本的抽象能力，并推广到未见过的事件。同时，我们在实验中显示，我们的基准测试可以全面提升语言模型在两个以前的抽象任务中的表现。
link: http://arxiv.org/abs/2311.09174v2
publish_time: 2023-11-15

title: Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark
abstract: UNER是一个开放的、社区驱动的项目，旨在开发多语言金标准NER基准测试。其目标是提供高质量、跨语言一致的注释，以促进和规范多语言NER研究。UNER v1包含18个数据集，在12种不同语言中以跨语言一致的架构注释命名实体。我们详细介绍了UNER数据集的创建过程和组成，并提供了在不同语言和跨语言学习设置上的初始建模基线。我们将数据、代码和模型释放给公众。
link: http://arxiv.org/abs/2311.09122v2
publish_time: 2023-11-15

title: Social Bias Probing: Fairness Benchmarking for Language Models
abstract: 大型语言模型已被证明存在各种社会偏见，可能带来下游危害。我们提出了一个新的框架来探查语言模型的社会偏见，并通过一个基于困惑度的公平分数来分析大规模基准数据集，发现不同身份表达对模型的不同对待。
link: http://arxiv.org/abs/2311.09090v2
publish_time: 2023-11-15

title: GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models
abstract: GRASP是一个新颖的基准，用于评估视频多模态大型语言模型（LLMs）的语言基础和物理理解能力，通过Unity模拟进行两级评估，揭示了现有多模态LLMs在语言基础和直觉物理能力方面存在显著缺陷，并强调了使用GRASP等基准来监测未来模型发展这些能力的重要性。
link: http://arxiv.org/abs/2311.09048v2
publish_time: 2023-11-15

title: CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
abstract: 大型语言模型在编码相关任务中表现出色，尤其是在协助人类编程和促进编程自动化方面。然而，现有用于评估LLMs的代码理解和生成能力的基准存在严重局限性，需要更具多语言、多任务、多维度的评估基准来全面检验LLMs在编码任务上的性能。
link: http://arxiv.org/abs/2311.08588v2
publish_time: 2023-11-14

title: TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
abstract: 文本风格是高度抽象的，涵盖了演讲者的特征、习惯、逻辑思维和表达的内容。然而，先前的文本风格转移任务主要侧重于数据驱动的方法，缺乏从语言学和认知科学角度进行深入分析和研究。本文介绍了一个名为文本语音风格转移（TSST）的新任务。主要目标是基于现有LLMs的能力，进一步探讨与人类认知相关的主题，如个性和情感。考虑到我们任务的目标和现实场景中口头表达的显著特征，我们为TSST训练了多维度（如填充词、生动性、互动性、情感性）评估模型，并验证了它们与人类评估的相关性。我们对几个大型语言模型（LLMs）的表现进行了彻底分析，并确定需要进一步改进的领域。此外，受评估模型的驱动，我们发布了一个新的语料库，提高了LLMs生成具有语音风格特征的文本的能力。总之，我们提出了TSST任务，作为风格转移的新基准，并强调以人为导向的评估，探索和提高当前LLMs的性能。
link: http://arxiv.org/abs/2311.08389v1
publish_time: 2023-11-14

title: All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction
abstract: 从科学论文中提取关键信息有助于研究人员更高效地工作，加快科学进展的速度。 提出了一种半监督管道用于标注文本中的实体以及表格中的实体和关系，释放了新资源，包括高质量基准测试、大规模语料库和半监督注释管道，验证了半监督管道的有效性和效率。
link: http://arxiv.org/abs/2311.08189v3
publish_time: 2023-11-14

title: RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge
abstract: LMs和AI聊天机器人在各个领域中提高了人们的效率，但回答问题所需的知识可能超出了模型的知识边界。为了缓解这一问题，许多研究人员尝试将外部知识（如知识图和互联网内容）引入LLMs以获取最新信息。然而，互联网上的外部信息可能包含违背事实的信息，这会使模型困惑并导致不正确的回应。因此，LLMs需要具备区分可靠信息和外部知识的能力。因此，为了评估LLMs识别外部知识可靠性的能力，我们从现有知识库中创建了一个基准。我们的基准包括两项任务，问答和文本生成，对于每个任务，我们为模型提供了一个包含违背事实信息的上下文。评估结果表明，现有的LLMs容易受到包含违背事实信息的不可靠外部知识的干扰，简单的干预方法对减轻这一问题的贡献有限。
link: http://arxiv.org/abs/2311.08147v1
publish_time: 2023-11-14

title: A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases
abstract: 大型语言模型（LLMs）在企业应用中具有回答企业SQL数据库问题的潜力。然而，鉴于缺乏针对企业环境的文本到SQL基准测试，LLMs在此类数据库中准确回答企业问题的程度尚不清楚。此外，知识图谱（KGs）通过提供业务背景来增强基于LLMs的问答系统的潜力尚不明确。本研究旨在评估LLM驱动的问答系统在企业问题和SQL数据库环境中的准确性，同时探讨知识图谱在提高准确性方面的作用。为实现这一目标，我们引入了一个基准测试，其中包括保险领域的企业SQL架构、涵盖报告到指标的一系列企业查询，以及一个包含本体和映射的上下文层，定义了一个知识图谱。我们的主要发现是，使用GPT-4直接在SQL数据库上进行零提示的问答，在企业环境中的准确率为16％。值得注意的是，当问题在企业SQL数据库的知识图表示上提出时，准确率提高至54％。因此，投资知识图谱为LLM驱动的问答系统提供更高的准确性。
link: http://arxiv.org/abs/2311.07509v1
publish_time: 2023-11-13

title: MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks
abstract: 最近，大型语言模型（LLMs）的研究取得了快速进展，在几个自然语言处理（NLP）任务中取得了显著进展，从而导致LLM评估研究激增，以了解模型的能力和局限性。然而，很大一部分研究仍局限在英文语言中，使得非英语语言的LLM构建和评估相对未被探索。因此，近期引入了一些新的LLMs，需要在非英语语言上进行评估。本研究旨在通过包括六个新数据集形成MEGAVERSE基准套件，扩展我们的MEGA基准套件。该基准包括22个数据集，涵盖81种语言，包括低资源的非洲语言。我们评估了几种最先进的LLMs，如GPT-3.5-Turbo、GPT4、PaLM2和Llama2在MEGAVERSE数据集上的表现。此外，我们在基准中包括了两个多模态数据集，并评估了LLaVa-v1.5模型的性能。我们的实验表明，GPT4和PaLM2在各种任务上表现优于Llama模型，特别是在低资源语言上，GPT4在更多数据集上胜过PaLM2。然而，必须解决数据污染等问题，才能准确评估LLM在非英语语言上的表现。
link: http://arxiv.org/abs/2311.07463v1
publish_time: 2023-11-13

title: AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation
abstract: 当前多模态大语言模型在多模态任务上取得了显著进展，但也面临幻觉挑战，评估幻觉变得愈发重要。本文提出了一个多维度基准AMBER，用于评估各类任务中包括存在、属性和关系幻觉。通过AMBER设计了低成本高效评估流程，并对主流MLLMs进行了全面评估和分析，提供了缓解幻觉的指导建议。
link: http://arxiv.org/abs/2311.07397v2
publish_time: 2023-11-13

title: ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models
abstract: 随着预训练视频语言模型（VidLMs）日益普及，迫切需要开发健壮的评估方法，深入挖掘它们的视觉语言能力。ViLMA（Video Language Model Assessment）是一个任务不可知的基准，旨在将这些模型的细粒度能力评估稳固地根基。任务驱动的评估虽有价值，但无法捕捉VidLMs处理移动图像的复杂性和具体时间方面。通过精心策划的反事实情况，ViLMA提供了一个受控评估套件，揭示了这些模型的真正潜力以及其与人类水平理解的性能差距。ViLMA还包括熟练度测试，评估解决主要反事实测试所需的基本能力。我们发现当前VidLMs的基准能力不比使用静态图像的视觉语言模型好。特别是当考虑到熟练度测试的表现时，这一点尤为引人注目。我们的基准将成为未来VidLMs研究的催化剂，帮助突出仍需探索的领域。
link: http://arxiv.org/abs/2311.07022v1
publish_time: 2023-11-13

title: Flames: Benchmarking Value Alignment of Chinese Large Language Models
abstract: 大语言模型（LLMs）在各个地区的广泛采用凸显了迫切需要评估它们与人类价值观的一致性。当前的基准不足以有效揭示LLMs的安全漏洞。尽管许多模型在这些评估中取得了高分并“榜上有名”，但在LLMs与人类价值观的深层一致性和实现真正无害方面仍存在显着差距。因此，本文提出了第一个高度敌对的基准 Flames，包含 2,251 个手动制作的提示、~18.7K 模型响应和细粒度注释，以及一个指定的得分系统。我们的框架包含了公共的无害原则，如公平、安全、合法性和数据保护，以及一个独特的道德维度，整合了中国特有的价值观，如和谐。根据该框架，我们精心设计了敌对提示，包括复杂情景和越狱方法，大部分带有隐含恶意。通过用这些敌对构造的提示来提示主流的LLMs，我们得到模型响应，随后对其进行严格评估。我们的研究结果表明，所有评估的LLMs在Flames上表现相对较差，尤其在安全和公平性维度上。Claude 是整体表现最佳的模型，但其无害率仅为 63.08%，而 GPT-4 仅得到 39.04% 的分数。Flames 的复杂性远远超出了现有的基准，为当代LLMs设立了新的挑战，凸显了LLMs更进一步对齐的必要性。为了有效评估基准上的新模型，我们开发了一个指定的得分系统，能够跨多个维度评分LLMs，准确率达到 77.4%。Flames基准已在https://github.com/AIFlames/Flames 上公开提供。
link: http://arxiv.org/abs/2311.06899v1
publish_time: 2023-11-12

title: BizBench: A Quantitative Reasoning Benchmark for Business and Finance
abstract: 商业和金融领域的问题需要推理、精确和广泛的技术知识，这使得大型语言模型在这个领域中变得困难。我们引入了BizBench，一个用于评估模型推理关于现实金融问题的能力的基准。BizBench包括八个定量推理任务，重点关注通过程序合成对财务数据进行问答（QA）。我们包含了三个从新收集和扩充的QA数据中提取的具有金融主题的代码生成任务。此外，我们分离了金融QA所需的推理能力：读取财务文本和表格以提取中间值的阅读理解，以及理解计算复杂解决方案所需的金融概念和公式。总体上，这些任务评估了模型的金融背景知识、解析财务文件的能力以及解决问题的编程能力。我们对开源和商业LLM进行了深入评估，并比较了以代码为中心和以语言为中心模型的行为。我们表明目前性能瓶颈是由于LLM对商业和金融理解的局限性，突显了在该领域内定量推理的挑战性基准的价值。
link: http://arxiv.org/abs/2311.06602v2
publish_time: 2023-11-11

title: THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech
abstract: 社交媒体上检测有害内容的困难在于简单的二元分类掩盖了大量的复杂性，并且目前缺乏细粒度目标类别和特定目标的标记数据集。我们介绍了一个包含8.3k条带有目标细粒度注释的推文的THOS数据集，展示了这个数据集使得能够基于大型语言模型训练分类器实现这种粒度级别的分类。
link: http://arxiv.org/abs/2311.06446v1
publish_time: 2023-11-11

title: Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text
abstract: NLP领域的数据质量问题是一个长期存在的难题，无论是任务、领域还是架构都会不断浮现，尤其对于资源较少的语言而言，这一问题尤为严重。重复和主要由缺乏语言趣味的模板数据是一个典型且隐蔽的问题，影响了训练数据和模型输出。在许多网络抓取的文集中，这一问题普遍存在，却缺乏标准来检测，或者系统研究来找出能够横跨多语言并符合人类数据质量判断的简单指标。在本研究中，我们创建并发布了BREAD，一个关于重复模板和合理语言内容人工标注的基准，涵盖了360种语言。随之发布了几个基准CRED（字符冗余）分数，并评估它们在BREAD上的效果。我们希望社区能够利用这一资源来开发更好的过滤方法，而我们的CRED分数的参考实现也能成为标准的文集评估工具，推动了更清洁的语言建模文集的发展，特别是在资源匮乏的语言中。
link: http://arxiv.org/abs/2311.06440v1
publish_time: 2023-11-11

title: Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications
abstract: 大型语言模型在各种自然语言任务上表现出卓越性能，但容易受到过时数据和领域特定限制的影响。为了解决这些挑战，研究人员追求了两种主要策略，知识编辑和检索增强，通过整合不同方面的外部信息来增强LLMs。然而，目前仍然缺少一个全面的调查。本文提出了一个回顾，讨论知识与大型语言模型集成的趋势，包括方法的分类、基准和应用。此外，我们对不同方法进行了深入分析，并指出未来的潜在研究方向。我们希望这份调查为社区提供快速访问和全面概述这一研究领域，以激发未来的研究努力。
link: http://arxiv.org/abs/2311.05876v2
publish_time: 2023-11-10

title: CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model
abstract: 大型语言模型在金融领域展现出巨大潜力，因此评估LLMs在金融任务中的表现变得至关重要。CFBenchmark旨在评估LLMs在中文金融辅助中的性能，结果显示现有模型在金融文本处理的基本任务仍有巨大改进空间。我们计划进一步探索CFBenchmark的高级版本，以更深入地探究语言模型在作为中文金融助理的广泛能力。
link: http://arxiv.org/abs/2311.05812v1
publish_time: 2023-11-10

title: Rethinking Benchmark and Contamination for Language Models with Rephrased Samples
abstract: 语言模型在训练过程中可能受到测试数据的影响，需要采用更强的去污染方法，并呼吁社区积极开发新的一次性考试来准确评估模型。
link: http://arxiv.org/abs/2311.04850v2
publish_time: 2023-11-08

title: P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models
abstract: 语言模型的快速发展为模型和用户带来了前所未有的便利和使用。对于强大的语言模型来说，其使用大规模文本数据训练，在多项下游自然语言处理任务中取得了最先进的性能。然而，对于不受限制的模型访问越来越受到关注，可能带来数据泄露的恶意隐私风险。为了解决这些问题，许多最近的工作提出了采用差分隐私的隐私保护语言模型(PPLM)。然而，不同的差分隐私实现使得在现有PPLM之间进行公平比较变得困难。本文提出了P-Bench，一个多角度隐私评估基准，以经验并直观地量化语言模型的隐私泄露。P-Bench不仅仅保护和测量受保护数据的隐私，还关注了实际使用中的推理数据隐私，首先清晰定义了私有微调过程中的多方面隐私目标。然后，P-Bench构建了一个统一的流程来执行私有微调。最后，P-Bench对具有预定义隐私目标的语言模型执行现有隐私攻击作为实验评估结果。实验攻击结果用于公平而直观地评估各种PPLMs的隐私泄露。我们在GLUE的三个数据集上进行了广泛的实验。
link: http://arxiv.org/abs/2311.04044v1
publish_time: 2023-11-07

title: Don't Make Your LLM an Evaluation Benchmark Cheater
abstract: 大型语言模型（LLMs）极大地推动了人工智能的边界，取得了模型容量方面的显著改进。为了评估模型性能，一个典型的方法是构建评估基准，以衡量LLMs在不同方面的能力水平。尽管已发布了许多高质量的基准，但对这些基准的恰当使用和对不同模型的公平比较的关注日益增多。考虑到这些问题，在本文中我们讨论了不恰当使用评估基准和误解评估结果的潜在风险和影响。具体而言，我们关注可能导致不恰当评估的特殊问题，即\emph{基准泄漏}，指的是与评估集相关的数据有时被用于模型训练。这种现象现在变得更加普遍，因为预训练数据往往是在进行模型测试之前准备好的。我们进行了广泛的实验来研究基准泄漏的影响，并发现它可以极大地提升评估结果，最终导致对模型性能的不可靠评估。为了改进现有评估基准的使用，我们最终提出了一些针对LLM开发者和基准维护者的指导方针。希望这项工作能够引起对LLM的恰当训练和评估的重视。
link: http://arxiv.org/abs/2311.01964v1
publish_time: 2023-11-03

title: PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion
abstract: LLMs在单轮对话中表现优异，但在完成整个会话时面临挑战，主要问题包括多轮会话中的错误积累、长PPT模板处理和多模态感知。
link: http://arxiv.org/abs/2311.01767v2
publish_time: 2023-11-03

title: An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction
abstract: 总结：Aspect sentiment quad prediction (ASQP) 是方面级情感分析的一个关键子任务。当前ASQP数据集的大小较小且四元组密度较低，限制了技术发展。我们构建了两个大型的中文ASQP数据集，具有更大的规模、丰富的方面类别、更长的句子、更高的密度，比现有的ASQP数据集更有价值。此外，我们首次评估了GPT系列模型在ASQP上的性能，并展示潜在问题。实验结果强调了探索额外技术来解决ASQP问题的必要性，以及改进GPT性能的重要性的重要性。
link: http://arxiv.org/abs/2311.01713v1
publish_time: 2023-11-03

title: A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers
abstract: 在线阅读文章的许多用户可能很难明确区分文本中的隐含意图，本研究致力于通过理解文本上下文来自动识别在线报纸中的政治意图。
link: http://arxiv.org/abs/2311.01712v1
publish_time: 2023-11-03

title: Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia
abstract: 本研究分析了印度尼西亚的四种低资源当地语言——爪哇语、巽他语、民南加保语和巴厘语的NMT系统训练，通过采用各种方法和数据规模，初步研究了使用大型语言模型生成合成低资源语言平行数据。研究表明，尽管计算资源和文本数据有限，我们的一些NMT系统表现出色，与零-shot gpt-3.5-turbo的翻译质量相媲美，这些发现显著推进了低资源语言的NMT，为相似背景的研究人员提供了宝贵的指导。
link: http://arxiv.org/abs/2311.00998v1
publish_time: 2023-11-02

title: IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems
abstract: 本文介绍了IndoToD，这是一个面向印尼语的端到端多领域任务导向对话系统基准。通过将两个英文ToD数据集扩展到印尼语，以消除词汇信息，有效减少注释大小。通过聘请母语者手动翻译对话，确保高质量的数据采集。这些新的印尼语数据集与原始的英文数据集一起，为评估印尼语和英语ToD系统提供了有效的基准，同时探索跨语言和双语转移学习方法的潜在优势。
link: http://arxiv.org/abs/2311.00958v1
publish_time: 2023-11-02

title: FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models
abstract: 本文提出了一个多级细化约束遵循基准测试FollowBench，以评估大型语言模型在遵循各种真实应用中的能力，弥补了现有基准主要关注纯响应质量而非遵循指令的不足。
link: http://arxiv.org/abs/2310.20410v2
publish_time: 2023-10-31

title: CreoleVal: Multilingual Multitask Benchmarks for Creoles
abstract: CreoleVal是一个涵盖多项NLP任务的基准数据集，旨在为克里奥尔语言的研究与包容性提供支持。
link: http://arxiv.org/abs/2310.19567v1
publish_time: 2023-10-30

title: M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models
abstract: LLMs面对长序列的管理已成为重要且必要的功能，但如何对LLMs进行全面系统的长序列能力评估仍是一个开放问题。M4LE基于多样化的NLP任务池，旨在提供多种能力、多范围、多任务、多领域的长上下文评估基准。
link: http://arxiv.org/abs/2310.19240v1
publish_time: 2023-10-30

title: SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks
abstract: SALMA是第一个阿拉伯语带有语义标注的语料库，包含约34K个标记，所有标记都有语义标注。该语料库使用两种不同的语义清单（现代和Ghani）进行标注。SALMA的创新之处在于如何关联标记和语义。与其将标记仅链接到一个预期的语义，SALMA将标记链接到多个语义，并为每个语义提供一个分数。开发了一个智能的基于网络的标注工具，以支持针对给定单词进行多个语义的评分。除了语义标注，我们还使用六种类型的命名实体进行了标注。使用各种指标（Kappa、线性加权Kappa、二次加权Kappa、平均误差和均方根误差）对我们的标注质量进行了评估，结果显示出很高的标注者间一致性。为了建立一个使用我们的SALMA语料库的词义消歧基线，我们使用目标语义验证开发了一个端到端的词义消歧系统。我们使用该系统评估了文献中提供的三个目标语义验证模型。我们最好的模型在使用现代模型时达到了84.2%的准确率，在使用Ghani模型时达到了78.7%的准确率。完整的语料库和标注工具都是开源的，并且可以在https://sina.birzeit.edu/salma/上公开获取。
link: http://arxiv.org/abs/2310.19029v1
publish_time: 2023-10-29

title: LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection
abstract: 本文比较了不同的预训练和微调的大型语言模型(LLMs)在仇恨言论检测方面的表现。我们的研究突显了LLMs在跨领域有效性和过拟合风险方面的挑战。通过评估，我们强调了需要通过更多标签的异质性来把握仇恨言论的微妙之处的微调模型。最后，我们总结了对未来仇恨言论检测的展望，强调跨领域的普适性和适当的基准测试实践。
link: http://arxiv.org/abs/2310.18964v1
publish_time: 2023-10-29

title: MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments
abstract: 对印度司法系统来说，自动摘要判例裁决是一个实际重要且吸引了许多国家的研究努力的问题。由于印度法律案件判决大多是用复杂英语书写的，但印度有相当一部分人口并不擅长英语，因此在印度语言中总结法律文件以确保平等获取司法权利至关重要。这项研究是对将英语法律文件跨语言总结成印地语的开创性努力，通过构建首个由英语编写的高质量印度法庭案件裁决文献集合，包括法律从业者撰写的英语和印地语摘要，在该集合上评估了几种不同的摘要方法的表现，并展示了在法律领域跨语言摘要需要进一步研究的必要性。
link: http://arxiv.org/abs/2310.18600v1
publish_time: 2023-10-28

title: Evaluating Cross-Domain Text-to-SQL Models and Benchmarks
abstract: 文中指出，Text-to-SQL基准测试在评估领域进展和模型排名中起着至关重要的作用。然而，由于自然语言查询不足、模型生成和参考查询的固有假设，以及SQL输出在某些条件下的非确定性等原因，模型生成的SQL查询与基准测试中的参考SQL查询精确匹配的失败。文章通过对几个知名跨领域文本到SQL基准测试进行广泛研究，并通过手动评估SQL查询并用等效表达式重写这些查询来重新评估这些基准测试中表现最好的模型。我们的评估揭示，由于提供的样本可能有多种解释，完美地在这些基准测试中表现的难以实现。此外，我们发现模型的真实性能被低估，并在重新评估后它们的相对表现发生变化。最明显的是，我们的评估发现，最近基于GPT4的模型在我们的人工评估中超越了Spider基准测试中的黄金标准参考查询。这一发现强调了在谨慎解释基准测试评估的重要性，同时也承认了独立额外评估在推动领域进展中的关键作用。
link: http://arxiv.org/abs/2310.18538v1
publish_time: 2023-10-27

title: NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark
abstract: 在这篇立场论文中，我们认为使用标注基准对自然语言处理(NLP)任务进行经典评估存在问题，数据污染的最严重形式是在基准的测试数据上训练大型语言模型(LLM)，然后在同一基准上进行评估。问题的程度目前尚不清楚，因为很难直接衡量。数据污染会导致对目标基准和相关任务中受到污染的模型的性能进行高估。后果可能非常严重，会导致错误的科学结论被发表，而其他正确的结论被丢弃。这篇立场论文定义了不同程度的数据污染，并呼吁社区努力，包括开发自动和半自动措施来检测基准数据是否暴露给模型，以及建议标记通过数据污染受损的论文。
link: http://arxiv.org/abs/2310.18018v1
publish_time: 2023-10-27

title: The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks
abstract: 近年来，根据多个旨在评估性能的数据集显示，自然语言处理模型取得了巨大的进展，然而，仍有疑问关于特定数据集设计选择如何影响我们对模型能力的结论。本研究在组合概括领域探讨了这个问题，通过4个数据集根据8种组合分割策略拆分，总共对18种组合概括分割对六种建模方法的表现进行了考察。我们的结果显示：i）尽管所有数据集都设计用于评估组合概括，但它们对建模方法进行了不同的排序；ii）人类生成的数据集之间比它们与合成数据集之间或合成数据集之间更吻合；iii）总的来说，数据集是否来自相同的来源更有可能预测结果模型的排序，而不是它们是否保持了相同的组合性定义；iv）数据中使用的词汇项目会对结论产生强烈影响。总的来说，我们的结果显示，当评估数据集是否测量其意图时，仍需要做大量工作，建议明确更严格的标准来验证评估集的有效性可能对该领域有益。
link: http://arxiv.org/abs/2310.17514v1
publish_time: 2023-10-26

title: CL-MASR: A Continual Learning Benchmark for Multilingual ASR
abstract: 现代多语种自动语音识别（ASR）系统使得可以使用单一模型转录多种语言的音频，但当前最先进的ASR模型通常在单个语言或多任务设置下评估，忽视了不断学习新语言的挑战。缺乏研究如何添加新语言而不丢失之前数据的宝贵信息。此外，现有的持续学习基准主要集中在视觉和语言任务上，对于多语种ASR的持续学习几乎未被探索。为了弥补这一空白，我们提出了CL-MASR，一个专为研究多语种ASR的持续学习环境而设计的基准。CL-MASR提供了多种持续学习方法，应用于大规模预训练的ASR模型，并提供共同的指标来评估学习新语言的效果，同时解决了灾难性遗忘的问题。据我们所知，CL-MASR是多语种ASR任务的第一个持续学习基准。该代码可在https://github.com/speechbrain/benchmarks获取。
link: http://arxiv.org/abs/2310.16931v1
publish_time: 2023-10-25

title: HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models
abstract: 理论心智（ToM）是理性思考自己和他人心智状态的能力，在智力、语言理解和认知过程的发展中发挥着关键作用。我们探索了涉及对他人信念进行递归推理的高阶ToM，引入了HI-TOM作为高阶心理理论基准。我们的实验评估表明，基于各种大型语言模型（LLMs）的性能在高阶ToM任务上有所下降，展示了当前LLMs的局限性。我们对LLMs的不同失败案例进行了彻底分析，并就我们的发现对NLP未来的影响分享了思考。
link: http://arxiv.org/abs/2310.16755v1
publish_time: 2023-10-25

title: HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis
abstract: 作者分析，也被称为文体测度学，长期以来一直是自然语言处理（NLP）中的重要方面。大语言模型（LLMs）的最新进展使得作者分析在区分人类写作和人工智能生成的文本方面变得越来越关键。然而，这些作者分析任务主要集中在书面文本上，而忽视了口语文本。因此，我们推出了口语文本的最大基准 - HANSEN（Human ANd ai Spoken tExt beNchmark）。HANSEN包括对现有语音数据集进行精心筛选，并配有文字转录，同时创建新的人工智能生成口语文本数据集。总共包括了17个人类数据集，以及使用ChatGPT、PaLM2和Vicuna13B等3个知名LLM创建的人工智能生成口语文本。为了评估和展示HANSEN的实用性，我们在人类口语数据集上进行作者归属和作者验证，并使用最先进的模型进行人类对人工智能口语文本检测。虽然最先进的方法，如字符ngram或基于Transformer的模型，在人类口语数据集上的AA＆AV性能与书面文本相似，但在人工智能生成口语文本检测方面还有很大的改进空间。HANSEN基准可在以下网址找到：https://huggingface.co/datasets/HANSEN-REPO/HANSEN。
link: http://arxiv.org/abs/2310.16746v1
publish_time: 2023-10-25

title: A Multi-Modal Multilingual Benchmark for Document Image Classification
abstract: 文档图像分类与纯文本文档分类不同，是通过理解表单、电子邮件等文档的内容和结构来对文档进行分类。我们发现现有的数据集存在限制，引入了两个新的多语言数据集WIKI-DOC和MULTIEURLEX-DOC来克服这些限制。我们对以往未经测试的文档图像分类中流行的视觉丰富的文档理解或文档AI模型进行了全面研究，例如1) 多标签分类，以及2) 零-shot跨语言转移设置。实验结果显示多语言文档AI模型在跨语言转移中存在局限性。我们的数据集和研究结果为未来改进文档AI模型打开了大门。
link: http://arxiv.org/abs/2310.16356v1
publish_time: 2023-10-25

title: CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts
abstract: 当前，存在着迫切的需求自动评估人类感知的文本连贯性，以评估生成文本的连贯性，为此我们引入了一个新的标准来评估文本连贯性。
link: http://arxiv.org/abs/2310.16329v1
publish_time: 2023-10-25

title: DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment
abstract: 对话评估在开放领域对话系统的发展中起着至关重要的作用。 DiQAD 是一个大规模的对话质量评估数据集，旨在提供端到端和基于人类认知的评估数据，以此自动评估开放领域对话质量。
link: http://arxiv.org/abs/2310.16319v1
publish_time: 2023-10-25

title: This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models
abstract: 尽管大型语言模型在语法知识和概括能力方面具有一定水平，但它们在理解否定时存在困难，进一步表明了对负面句子的处理仍然是大型语言模型面临的挑战之一。
link: http://arxiv.org/abs/2310.15941v1
publish_time: 2023-10-24

title: BLESS: Benchmarking Large Language Models on Sentence Simplification
abstract: 我们提出了BLESS，这是对最新一代最先进的大型语言模型（LLMs）在文本简化（TS）任务上的全面性能基准。我们检查现成的LLMs在解决这一具有挑战性任务时表现如何，评估了总共44个模型，在大小、架构、预训练方法和可访问性上有所不同，在来自不同领域（维基百科、新闻和医学）的三个测试集上进行了少样本设置。我们的分析考虑了一套自动度量标准以及对不同模型执行的常见编辑操作类型的大规模定量调查。此外，我们对一部分模型输出进行了手动的质量分析，以更好地评估生成的简化质量。我们的评估表明，最好的LLMs，尽管没有在TS上进行训练，性能与最先进的TS基线相当。此外，我们发现某些LLMs表现出更广泛和多样化的编辑操作。我们的性能基准将作为未来TS方法和评估指标的资源。
link: http://arxiv.org/abs/2310.15773v1
publish_time: 2023-10-24

title: DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding
abstract: 社会智能对于理解和推理人类表达、意图和互动至关重要。通过研究脸书智能查询（Social-IQ）这一代表性基准，我们定义了一种全面的方法论来检验其严谨性，因为对于研究问题的深入探讨来说，这类基准数据集的严谨性至关重要。我们的分析表明，Social-IQ存在着相当大的偏见，这些偏见可以被中等强度的语言模型利用，学习到伪相关性以达到完美的表现，而不需要给出上下文甚至问题本身。我们引入了DeSIQ，这是一个通过对Social-IQ进行简单扰动构建的全新挑战数据集。我们的经验证明，DeSIQ显著减少了原始Social-IQ数据集中的偏见。此外，我们对新基准表现的影响因素包括模型规模、模型风格、学习设置、常识知识和多模态等进行了调查和阐述。我们的新数据集、观察和发现为社会智能研究打开了重要的研究问题。
link: http://arxiv.org/abs/2310.18359v1
publish_time: 2023-10-24

title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions
abstract: 现有的心灵理论（ToM）评估主要集中在使用缺乏互动性的被动叙述来测试模型。我们引入了FANToM，一个旨在通过问答在信息不对称对话环境中对ToM进行压力测试的新基准。我们的基准结合了心理学的重要理论要求以及评估大型语言模型（LLMs）时必要的经验考虑。特别是，我们制定了多种类型的问题，要求相同的基本推理，以识别LLMs中的虚幻或错误的ToM能力。我们展示了FANToM对最先进的LLMs来说是具有挑战性的，即使进行了思维链推理或微调，它们的表现也远远不及人类。
link: http://arxiv.org/abs/2310.15421v3
publish_time: 2023-10-24

title: CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks
abstract: 最近在自然语言处理（NLP）常识推理研究领域取得了大量新数据集和基准结果，但大多数数据集制定的常识推理挑战在人工场景中，与真实世界的NLP系统任务不符。我们提出了CRoW，这是一个手工策划的、多任务评估模型在六个真实世界NLP任务中应用常识推理能力的基准测试。CRoW采用多阶段数据收集管道构建，通过对现有数据集的例子进行常识违规扰动来生成。我们使用CRoW研究NLP系统在常识知识（如物理、时间、社交推理等）不同维度上的表现。我们发现在CRoW上评估时NLP系统与人类相比存在显著的性能差距，显示在真实任务环境中常识推理仍远未解决。我们将我们的数据集和排行榜提供给研究社区，网址为https://github.com/mismayil/crow。
link: http://arxiv.org/abs/2310.15239v1
publish_time: 2023-10-23

title: The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models
abstract: 尽管预训练的语言与视觉模型在下游任务中取得了令人印象深刻的表现，但这是否反映了对图像-文本交互的正确理解仍是一个悬而未决的问题。本文探讨了它们在处理基本语言结构方面的能力，例如主动被动语态、并列和关系从句，即使是学龄前儿童通常都能掌握。我们提出了BLA，一个新颖的、自动生成的基准，用于评估多模态模型在这些基本语言能力上的表现。我们发现，基于Transformer的不同类型系统，如CLIP、ViLBERT和BLIP2，在零样本设置下通常在BLA上遇到困难，这与之前的研究结果一致。我们的实验特别显示，大多数测试的模型只有在经过微调或提示具体结构样本后才能略微受益。然而，生成式的BLIP2在一个基于上下文的学习设置中表现出有希望的趋势。这为将BLA用作评估基准不仅打开了大门，还为提高模型的基本语言能力提供了可能。
link: http://arxiv.org/abs/2310.15061v1
publish_time: 2023-10-23

title: SLOG: A Structural Generalization Benchmark for Semantic Parsing
abstract: SLOG旨在评估模型对新复杂语言表达的泛化能力，强调了模型在结构泛化任务上的性能，揭示了模型在词汇和结构泛化能力之间存在的巨大差异。
link: http://arxiv.org/abs/2310.15040v1
publish_time: 2023-10-23

title: SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research
abstract: 尽管与通用模型、指标和基准相比，社交媒体自然语言处理的成熟度还不足，这种分散的情况使得社群很难知道对于一个任务，哪种模型表现最好以及它与其他模型的比较。通过引入一个统一的社交媒体自然语言处理评估基准SuperTweetEval来缓解这个问题，其中包括从头开始组合、适应和构建的异质任务和数据集。我们在SuperTweetEval上对各种模型的表现进行了基准测试，结果显示，尽管在语言建模方面取得了最新进展，社交媒体仍然具有挑战性。
link: http://arxiv.org/abs/2310.14757v1
publish_time: 2023-10-23

title: Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models
abstract: 当前大型语言模型(Large Language Models, LLMs)如Llama、Mistral和GPT等普遍忽视了曾经作为语言建模评估基石的词汇测试。然而，我们认为词汇测试作为一种评估LLM表现的宝贵工具仍值得重视，通过评估七种LLM在两种语言下使用两种词汇测试格式，揭示了它们在词汇知识方面的令人惊讶的差距。这些发现不仅揭示了LLM单词表示、学习机制和不同模型及语言间性能差异的复杂性，而且利用自动生成和执行词汇测试的能力为扩展评估方法和提供LLM语言技能更全面的画面提供了新的机会。
link: http://arxiv.org/abs/2310.14703v2
publish_time: 2023-10-23

title: CITB: A Benchmark for Continual Instruction Tuning
abstract: 不断学习是一种模式，旨在模仿人类不断学习和积累知识的能力，不会忘记先前的知识并将其转移到新任务中。最近的指导微调涉及微调模型，使其更适应解决自然语言处理任务。然而，在CL任务中如何使用指导微调仍不确定。这一具有挑战性但实用的问题被形式化为持续指导微调。在这项工作中，我们建立了一个包含学习和评估协议的CIT基准。我们整理了两种不同类型的长对话任务流，InstrDialog和InstrDialog ++，以系统地研究各种CL方法。我们的实验表明，现有的CL方法并未有效利用丰富的自然语言指令，按顺序微调经过指导微调的模型可能会产生类似或更好的结果。我们进一步探讨可能影响CIT学习的不同方面。希望这个基准可以促进进一步的研究。
link: http://arxiv.org/abs/2310.14510v1
publish_time: 2023-10-23

title: InstructExcel: A Benchmark for Natural Language Instruction in Excel
abstract: 随着大型语言模型（LLMs）的发展，我们可以在各个领域解决越来越复杂的自然语言处理任务，包括表格处理。本研究探讨LLMs能否生成能够通过自然语言用户指令解决Excel特定任务的代码（Excel OfficeScripts，一种用于执行Excel中许多任务的TypeScript API）。为此，我们引入了一个新的大规模基准，InstructExcel，通过利用Excel中的“自动化”功能从用户的操作中自动生成OfficeScripts。我们的基准包括超过10,000个样本，涵盖了2000多个公开可用的Excel电子表格中的170多项Excel操作。各种零样本和少样本设置的实验表明，InstructExcel对于像GPT-4这样的最先进模型来说是一个难以应付的基准。我们观察到，（1）使用GPT-4而不是GPT-3.5，（2）提供更多的上下文示例，以及（3）使用动态提示可以帮助提高在该基准上的性能。
link: http://arxiv.org/abs/2310.14495v1
publish_time: 2023-10-23

title: Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques
abstract: GPT-3模型在地理编码地址解析任务中表现出潜力，可通过进一步微调实现改进，值得进一步研究和开发。
link: http://arxiv.org/abs/2310.14360v4
publish_time: 2023-10-22

title: DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias
abstract: 许多去偏见技术已被提出来减轻在预训练语言模型中普遍存在的性别偏见，然而评估通常基于检测模型在预测中的性别中立程度的数据集，忽视了偏见减轻可能对有用性别知识的负面影响。为填补这一空白，我们提出了DiFair，一种基于掩码语言建模目标的手动筛选的数据集。DiFair允许我们引入一个统一的指标，即性别不变性评分，不仅量化模型的偏见行为，还检查有用性别知识是否保留。我们使用DiFair作为一些广泛使用的预训练语言模型和去偏见技术的基准。实验结果证实了现有性别偏见的先前发现，同时表明尽管去偏见技术改善了性别偏见问题，但这种改进通常会以降低模型的有用性别知识为代价。
link: http://arxiv.org/abs/2310.14329v1
publish_time: 2023-10-22

title: PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain
abstract: 生物医药语言理解基准是推动具有大型语言模型后端的人工智能应用的动力，但大多数当前的基准存在限制，这使得在其他语言中复制英文的成功变得具有挑战性。 为促进医学语言模型的研究，我们重新构建了中文生物医学语言理解评估（CBLUE）基准，并将其打造为一个大规模的提示调节基准，PromptCBLUE。 我们的基准是一个适合的测试平台和在线平台，用于评估中文语言模型在各种生物医学任务上的多任务能力，包括医学实体识别、医学文本分类、医学自然语言推理、医学对话理解和医学内容/对话生成。 为了对这些任务建立评估，我们已经进行了实验，并报告了目前使用不同微调技术微调的9个中文语言模型的结果。
link: http://arxiv.org/abs/2310.14151v1
publish_time: 2023-10-22

title: Sentiment Analysis Across Multiple African Languages: A Current Benchmark
abstract: 情感分析是自然语言处理中的一个基础和有价值的任务。然而，由于数据和技术可用性的限制，对非洲语言的情感分析研究存在碎片化和缺乏。最近发布的AfriSenti-SemEval Shared Task 12提供了对14种非洲语言的情感分析注释数据。我们对12种语言的当前最先进的transformer模型进行了基准测试和比较，比较了单语言单模型与单模型多语言训练的性能。我们还评估了标准多语言模型的性能以及其学习和跨语言表征从非洲语言到非洲语言的能力。我们的结果显示，尽管在低资源建模方面工作，但更多的数据仍会产生更好的模型。专门为非洲语言开发的模型在所有任务上表现出色。此外，对于一些样本较少的语言，一个更大的多语言模型可能会比专门为情感分类而设计的单语言模型表现更好。
link: http://arxiv.org/abs/2310.14120v1
publish_time: 2023-10-21

title: MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation
abstract: MedEval是一个多层次、多任务、多领域的医学基准数据集，为医疗领域语言模型的开发提供支持，展现出对于大型语言模型在医疗领域应用的潜力和限制。
link: http://arxiv.org/abs/2310.14088v3
publish_time: 2023-10-21

title: Benchmarking and Improving Text-to-SQL Generation under Ambiguity
abstract: 研究显示，在文本到SQL转换中，现有的基准数据集通常是每个文本查询对应一个正确的SQL，但是真实生活中的自然语言查询往往涉及对所需SQL的意图存在重大歧义，因为存在重叠的模式名称和多条混淆的关系路径。为了弥合这一差距，我们开发了一个名为AmbiQT的新型基准测试，包含3000多个示例，其中每个文本都可解释为两个合理的SQL，因为存在词汇或结构上的歧义。在面对歧义时，理想的前$k$解码器应生成所有有效的解释，以便用户进行可能的消歧。我们评估了几种文本到SQL系统和解码算法，包括使用最先进的LLM，发现它们远未达到这种理想。主要原因是常见的束搜索算法及其变体将SQL查询视为一个字符串，并在前$k$的水平上产生无用的标记级多样性。我们提出了LogicalBeam，一种新的解码算法，通过计划型模板生成和受限填充的混合来导航SQL逻辑空间。通过生成事实反证计划，模板多样化，同时通过仅在模式名称上分支的束搜索提供值多样性。LogicalBeam在生成所有候选SQLs的前$k$名输出方面比最先进模型高效多达2.5倍，还提高了SPIDER和Kaggle DBQA的前5名准确度和执行匹配准确度。
link: http://arxiv.org/abs/2310.13659v1
publish_time: 2023-10-20

title: MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark
abstract: 目前关于最近LLM在非英语语言中生成具有说服力文本的能力和多语言环境下机器生成文本检测器的性能研究不足，而现有的基准测试缺乏非英语真实文本数据，并主要涵盖较老的生成器。为了填补这一空白，我们引入了MULTITuDE，一个新颖的用于多语言机器生成文本检测的基准数据集，包括由8个多语种LLM生成的11种语言（ar、ca、cs、de、en、es、nl、pt、ru、uk和zh）的74,081条真实文本和机器生成文本。利用这个基准测试，我们比较了零-shot（统计和黑盒）和微调检测器的性能。考虑到多语种性，我们评估了1）这些检测器如何泛化到未见过的语言（语言上相似和不相似）和未见过的LLM，以及2）当在多种语言上进行训练时，检测器是否提高了性能。
link: http://arxiv.org/abs/2310.13606v1
publish_time: 2023-10-20

title: FactCHD: Benchmarking Fact-Conflicting Hallucination Detection
abstract: 尽管LLM的生成能力令人印象深刻，但在实际应用中却受到事实矛盾的幻觉的阻碍。 FactCHD是一个专门用于检测LLMs中事实冲突幻觉的基准，具有丰富的数据集和深度评估检测器解释的特殊元素。Truth-Triangulator通过整合ChatGPT和Llama2的预测结果和证据来提供更可信的检测结果。
link: http://arxiv.org/abs/2310.12086v2
publish_time: 2023-10-18

title: LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation
abstract: 合并实体代理和大型语言模型(LLMs)的发展为实体指导带来了显著进步。特别是LLMs的强大推理能力使得机器人能够在不需要昂贵标注示范的情况下执行长时间跨度任务。然而，目前缺乏用于测试语言驱动机器人在各种场景中长时间跨度推理能力的公共基准。为了填补这一空白，本研究关注桌上操作任务并发布了一个模拟基准，名为\textit{LoHoRavens}，涵盖了跨颜色、大小、空间、算术和参照的各种长时间跨度推理方面。此外，对于LLMs与长时间跨度操作任务之间的关键模态桥接问题：如何在机器人执行过程中将观察反馈融入LLM的闭环规划，这一问题却受之前的研究较少关注。我们研究了两种解决模态差距的方法：生成标题和可学习接口，分别将显式和隐式的观察反馈引入LLM。这些方法构成了我们提出的基准的两个基线。实验证明，这两种方法在解决某些任务时都遇到困难，表明长时间跨度操作任务对当前流行模型仍然具有挑战性。我们希望提出的公共基准和基线方法能帮助社区发展出更好的模型来解决长时间跨度桌面操作任务。
link: http://arxiv.org/abs/2310.12020v2
publish_time: 2023-10-18

title: A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs
abstract: 本文提出了一个基于Wikidata5M的大规模基准测试来评估半归纳链路预测模型，并发现半归纳LP性能远远低于传统性能，为进一步研究如何在半归纳LP模型中整合上下文和文本信息提供了一个测试平台。
link: http://arxiv.org/abs/2310.11917v1
publish_time: 2023-10-18

title: CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion
abstract: 代码完成模型在最近几年取得了显著进展，但当前流行的评估数据集，如HumanEval和MBPP，主要集中在单个文件内的代码完成任务上。这种过于简化的设置无法很好地代表现实世界中软件开发的情景，其中存储库跨越多个文件，并具有许多文件间的依赖关系，访问和理解跨文件上下文通常是必要的才能正确完成代码。为了填补这一不足，我们提出了CrossCodeEval，这是一个多样化且多语言的代码完成基准测试，需要深入的跨文件上下文理解才能准确完成代码。CrossCodeEval是建立在四种流行编程语言（Python，Java，TypeScript和C#）的一组多样化的开源许可库上。通过一个简单而高效的基于静态分析的方法来创造严格需要跨文件上下文的示例。对于像CodeGen和StarCoder这样的最先进的代码语言模型进行了广泛的实验，结果表明当相关的跨文件上下文缺失时，CrossCodeEval极具挑战性，并且当将这些上下文添加到提示中时，明显出现了改善。然而，尽管有这些改进，即使采用了最高性能的模型，仍未达到性能的巅峰，这表明CrossCodeEval也能够评估模型利用大量上下文进行更好代码完成的能力。最后，我们对检索跨文件上下文的各种方法进行了基准测试，并表明CrossCodeEval也可以用来衡量代码检索器的能力。
link: http://arxiv.org/abs/2310.11248v2
publish_time: 2023-10-17

title: NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain
abstract: LLM已经被广泛应用于各个领域，但在评估其在特定科学领域效果方面存在挑战。
link: http://arxiv.org/abs/2310.10920v1
publish_time: 2023-10-17

title: BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali
abstract: 本文介绍了我们在解决孟加拉语暴力煽动文本检测共享任务时开发的系统。我们解释了我们使用的传统和最新方法，以使我们的模型学习。我们提出的系统有助于分类给定的文本是否含有威胁。我们研究了在数据集有限时数据增强的影响。我们的定量结果显示，微调多语言-e5基础模型在我们的任务中表现最佳，优于其他基于Transformer架构。我们在测试集中获得了68.11\%的宏平均F1值，并在排行榜上排名第23位。
link: http://arxiv.org/abs/2310.10781v1
publish_time: 2023-10-16

title: TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models
abstract: 最近成功的生成语言模型在探索自动定理证明领域中的推理能力方面变得愈发吸引。
link: http://arxiv.org/abs/2310.10180v2
publish_time: 2023-10-16

title: Improving Access to Justice for the Indian Population: A Benchmark for Evaluating Translation of Legal Text to Indian Languages
abstract: 由于历史原因，印度司法机构的大部分法律文本均以复杂的英语书写。然而，只有大约10%的印度人口能够舒服地阅读英语。因此，法律文本需要以各种印度语言的形式提供，可能通过从英语翻译可用的法律文本。尽管对印度语言之间的翻译已经有大量研究，但据我们所知，在法律领域中很少有这种翻译的先前工作。在本研究中，我们构建了第一个包含英语和九种印度语言对齐文本单元的高质量法律平行语料库，其中包括一些低资源语言。我们还对各种机器翻译系统在此语料库上的性能进行了基准测试，包括商用机器翻译系统、开源机器翻译系统和大型语言模型。通过法律从业者的广泛调查，我们检查了他们对某些机器翻译系统的翻译满意程度，以及自动机器翻译评估指标与法律从业者意见的一致程度。
link: http://arxiv.org/abs/2310.09765v1
publish_time: 2023-10-15

title: BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts
abstract: 孟加拉语是全球第七大使用人数最多的语言，拥有惊人的2.34亿母语使用者，主要来自印度和孟加拉国。该语言形态丰富，拥有丰富的文学传统，包括多样的方言和语言特定挑战。尽管孟加拉语在语言学上富有特色并具有悠久历史，但在自然语言处理和语音领域仍被归类为低资源语言。在BLP研讨会的Task 2（孟加拉社交媒体帖子情感分析）中，我们提交了我们的研究成果。我们尝试了基于Transformer的各种架构来解决这个任务。我们的定量结果显示，迁移学习确实有助于在这种低资源语言的情况下更好地学习模型。当我们进一步微调一个已经在twitter数据上进行过情感分析任务微调的模型时，这一点变得显而易见，并且这个微调模型在所有其他模型中表现最好。我们还进行了详细的错误分析，发现有一些地方需要重新审视实际标签。我们在测试集上获得了67.02\%的微平均F1分数，我们在这个共享任务中的表现在排行榜上位列第21位。
link: http://arxiv.org/abs/2310.09238v2
publish_time: 2023-10-13

title: xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark
abstract: 当前针对开域对话评估的无参考学习度量的最新进展主要受益于预训练语言模型的进步和具有高质量人类标注的对话数据的可用性。然而，目前的研究主要集中在英语对话上，这些度量的泛化性能到其他语言尚未得到充分检验，这在很大程度上是由于缺乏多语言对话评估基准所致。
link: http://arxiv.org/abs/2310.08958v1
publish_time: 2023-10-13

title: Welfare Diplomacy: Benchmarking Language Model Cooperation
abstract: 随着人工智能系统能力增长，部署范围扩大，需要强有力的基准来衡量它们的合作能力。我们提出了一种新的基准游戏——Welfare Diplomacy，旨在促进对合作能力的清晰评估和更强的训练动力。
link: http://arxiv.org/abs/2310.08901v1
publish_time: 2023-10-13

title: GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language Models
abstract: 尽管多模态模型已成功整合了图像、视频和音频等不同模态的信息，但将图模态整合到大型语言模型(LLMs)中仍未被探索。这种差异主要源自结构化图数据和非结构化文本数据之间的固有差异。整合图知识提供了可靠的信息来源，为解决文本生成中的问题(如幻觉和缺乏领域知识)提供了潜在解决方案。为评估将图知识整合到语言模型中，需要专门的数据集。然而，目前还没有专门为多模态图语言模型设计的基准数据集。为填补这一空白，我们提出了GraphextQA，这是一个问题回答数据集，其中包含了从Wikidata检索出的配对子图，以促进图-语言模型的评估和未来发展。此外，我们介绍了一个名为CrossGNN的基线模型，通过在解码时跨注意问题感知图特征，将答案生成的条件化于配对图上。所提出的数据集旨在评估图-语言模型理解图的能力和利用图进行答案生成的能力。我们进行了语言模型和提出的图-语言模型实验，以验证配对图的有用性，并展示任务的难度。
link: http://arxiv.org/abs/2310.08487v1
publish_time: 2023-10-12

title: Who Said That? Benchmarking Social Media AI Detection
abstract: AI生成文本在各种在线平台上广泛传播，提供了转变前景，但也带来了与错误信息和操纵相关的重大风险。【SAID：社交媒体AI检测】基准评估了AI文本检测模型在真实社交媒体平台上的能力，提供了更真实和具有挑战性的评估环境。【研究发现】显示，注释员能够以96.5%的平均准确率区分AI生成和人类生成的文本，这需要重新评估人类在识别今天广泛受AI影响环境中的AI生成文本方面的能力。conducting detection tasks on actual social media platforms proves to be more challenging compared to traditional simulated AI-text detection, resulting in a decreased accuracy. On the other hand, user-oriented AI-generated text detection significantly improve the accuracy of detection.
link: http://arxiv.org/abs/2310.08240v1
publish_time: 2023-10-12

title: TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models
abstract: 总结：对齐的大型语言模型（LLMs）在任务解决、遵循指令和确保安全方面展现出卓越能力，但这些LLMs的持续学习方面已被大多数人所忽视。TRACE基准是用来评估LLMs持续学习的，显示在TRACE训练后，LLMs在普遍能力和遵循指令能力方面显著下降，指出需要在特定任务表现和保留LLMs原始能力之间找到平衡。
link: http://arxiv.org/abs/2310.06762v1
publish_time: 2023-10-10

title: A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection
abstract: 大型语言模型已显示出在现实场景中与人类有效合作的能力，但是LLMs往往会产生幻觉，即生成不正确的文本和未经验证的信息，当用于任务关键型工作时可能造成重大破坏。本文提出了一种基于逆向验证的自检方法，以零资源方式自动检测事实错误。为了促进未来研究和评估不同方法，我们构建了一个由ChatGPT生成并由人类标注的幻觉检测基准数据集PHD。与以往的零资源幻觉检测研究相反，我们的方法和基准数据集集中在段落级别的检测而不是句子级别。实验证实了我们的方法在两个数据集上明显优于基线方法，消耗更少的token和时间。此外，我们手动分析了LLM未能捕捉的一些幻觉情况，揭示了零资源方法的共同局限。
link: http://arxiv.org/abs/2310.06498v2
publish_time: 2023-10-10

title: SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese
abstract: 大型语言模型，如ChatGPT和GPT-4，在自然语言理解和生成方面表现出色。然而，除了对我们日常任务产生积极影响外，它们也可能产生有害内容，对社会认知产生负面影响。通过引入SuperCLUE-Safety（SC-Safety）这一多轮对抗基准测试，系统评估中文LLM的安全性，我们旨在促进协作努力，创建更安全、更可信赖的LLM。
link: http://arxiv.org/abs/2310.05818v1
publish_time: 2023-10-09

title: Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution
abstract: 本文提出了一项新任务，即基于知识的语言模型归因（KaLMA），以改进传统属性LMs的三个核心问题。
link: http://arxiv.org/abs/2310.05634v1
publish_time: 2023-10-09

title: LAiW: A Chinese Legal Large Language Models Benchmark
abstract: 总结: 目前的通用和法律领域LLMs在LegalAI的各种任务中表现出色，但其在法律实践逻辑之外的专家评估下，难以判断其实际能力。为解决这一挑战，我们首次建立了基于法律实践逻辑的中国法律LLMs基准LAiW，以确保全面评估其法律能力，结果显示当前的LLMs在某些基本任务上表现不佳，还需要加强法律逻辑。
link: http://arxiv.org/abs/2310.05620v2
publish_time: 2023-10-09

title: Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus
abstract: 社会性别不平等根植于我们的交流方式，并在翻译技术中得以延续。特别是当翻译成语法性别语言时，机器翻译往往会默认为男性和刻板印象，做出不当的二元性别假设。我们的工作直面应对包容性语言需求的上升，聚焦于从英文到意大利文的性别中性翻译。我们从基础开始：提出一个专门的基准，并探讨自动化评估方法。首先，我们介绍了GeNTE，一个专门用于性别中性翻译的自然、双语测试集，其创建受到了有关中立语言的感知和使用的调查的启发。基于GeNTE，我们回顾了现有的基于参考的评估方法，强调了它们的局限性，并提出了一种无参考方法，更适合评估性别中性翻译。
link: http://arxiv.org/abs/2310.05294v1
publish_time: 2023-10-08

title: Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction
abstract: 信息抽取是自然语言处理中的重要任务，传统方法依赖于简单粗糙的抽取指令，但随着大型语言模型的出现，需要调整抽取技术以发挥这些模型的能力。本文介绍了针对大型语言模型定制的细粒度信息抽取基准数据集，采用增强的指令来指导每种信息类型的抽取，包括任务描述、抽取规则、输出格式和示例。通过广泛的评估，我们发现编码-解码模型，特别是T5和FLAN-T5，在泛化到未见信息类型时表现良好，而ChatGPT对新的任务形式具有更大的适应性。我们的结果还表明，性能不仅取决于模型规模，还突出了架构、数据多样性和学习技术的重要性。这项工作为在信息抽取中更精细和多功能地利用大型语言模型铺平了道路。
link: http://arxiv.org/abs/2310.05092v1
publish_time: 2023-10-08

title: FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets
abstract: 本文介绍了一种基于指令调整范式的独特方法，专门针对金融领域定制的开源大型语言模型，旨在提高其适用性和相关性，同时保证在不同任务领域的无缝和透明整合。
link: http://arxiv.org/abs/2310.04793v2
publish_time: 2023-10-07

title: A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks
abstract: 最近，大型语言模型(LLM)展示了惊人的解决各种任务的能力，尤其在生物医学领域。
link: http://arxiv.org/abs/2310.04270v3
publish_time: 2023-10-06

title: Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report
abstract: 本研究引入了大型语言模型(LLMs)概念，根据美国物理医学协会(AAPM)任务组(TG)-263标准重新标记结构名称，并为未来研究建立了基准。
link: http://arxiv.org/abs/2310.03874v1
publish_time: 2023-10-05

title: Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning
abstract: LLMs在任务中取得了显著成功，但在需要长期规划和空间推理的场景中仍存在局限性，我们提出了PPNL基准以评估LLMs的空间-时间推理能力，并发现少量GPT-4在空间推理方面具有潜力，但长期时间推理上仍存在挑战，而经过微调的LLMs在内部推理任务上表现出色，但在更大环境或更多障碍的环境中泛化能力较弱。
link: http://arxiv.org/abs/2310.03249v2
publish_time: 2023-10-05

title: MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use
abstract: 大型语言模型(LLMs)因其令人印象深刻的自然语言处理(NLP)能力而受到重视，然而最近许多研究集中在LLMs的工具利用能力上。这篇论文介绍了MetaTool基准，用于评估LLMs是否具有工具使用意识并能够正确选择工具。通过实验发现，大多数LLMs仍然在有效选择工具方面存在困难，突显了LLMs与真正智能代理之间的现有差距。
link: http://arxiv.org/abs/2310.03128v5
publish_time: 2023-10-04

title: Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages
abstract: 本文介绍了一种新的零资源代码切换语音基准，旨在直接评估自监督语音编码器的代码切换能力。我们展示了一个基准系统，采用离散单元的语言建模，以演示如何以零资源方式评估语音编码器的代码切换能力。我们的实验涵盖了各种知名的语音编码器，包括Wav2vec 2.0、HuBERT、XLSR等。我们分析了预训练语言和模型大小对基准性能的影响。尽管我们的结果表明，在代码切换场景中，具有多语言预训练的语音编码器（如XLSR）胜过单语种变体（Wav2vec 2.0、HuBERT），但它们的代码切换语言能力仍有相当大的改进空间。
link: http://arxiv.org/abs/2310.03018v3
publish_time: 2023-10-04

title: From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference
abstract: 大型语言模型（LLMs）因其超越以往最新技术的新生成能力而倍受青睐，在法律、金融和医学等各个领域越来越受到利用。然而，这些模型带来了显著的计算挑战，尤其是推断所需的计算和能源成本。推断能源成本已经比训练LLMs的能源成本更少受到关注，尽管这些大模型在现实中被频繁调用进行推断（例如ChatGPT）。随着这些最新技术的LLMs在各个领域中的使用和部署日益增加，对它们的资源利用情况有更好的理解对于节约成本、提高性能、有效使用硬件和优化推断策略至关重要。 本文描述了进行推断的计算和能源利用研究所进行的实验。我们对Meta AI最新的LLM之一LLaMA的推断性能和推断能源成本进行基准测试和初步分析，该LLM是在两代流行GPU（NVIDIA V100和A100）和两个数据集（Alpaca和GSM8K）上开发的，以反映研究和实践中LLMs的多样化任务/基准集。我们展示了使用模型分片在最多32个GPU上进行的多节点、多GPU推断的结果。据我们所知，我们的工作是在这一规模上研究LLM推断性能的第一批之一。
link: http://arxiv.org/abs/2310.03003v1
publish_time: 2023-10-04

title: T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation
abstract: 最近的文本到3D方法利用强大的预训练扩散模型来优化NeRF，能够在不训练3D数据的情况下生成高质量的3D场景，但如何定量评估当前文本到3D的进展仍然是一个挑战。我们引入了T$^3$Bench作为首个全面的文本到3D基准测试，提出了两个基于多视角图像的自动指标来评估文本和图像的质量和一致性，这为有效评估文本到3D模型提供了范例。基准测试结果显示了六种主流文本到3D方法之间的性能差异，同时也凸显了当前方法在生成环境和多对象场景方面的共同困难，以及利用2D指导进行3D生成的瓶颈。
link: http://arxiv.org/abs/2310.02977v1
publish_time: 2023-10-04

title: LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models
abstract: 传统自动语音识别模型输出小写单词且不包含标点符号，降低可读性并需额外的文本处理模型来转换成正确格式；同时，基于端到端的ASR模型具有预测标点和大写字母的能力，但由于数据有限和现有评估方法不足所面临的挑战，如标点预测评估不足等。本文引入LibriSpeech-PC基准，用于评估端到端ASR模型的标点和大写字母预测能力，包括LibriSpeech-PC数据集、Punctuation Error Rate（PER）评估指标及初始基准模型，所有代码、数据和模型均公开可用。
link: http://arxiv.org/abs/2310.02943v1
publish_time: 2023-10-04

title: Benchmarking and Improving Generator-Validator Consistency of Language Models
abstract: 研究提出了一种新的框架来度量生成和验证的一致性，称之为生成-验证一致性，在调研中发现即使是GPT-4这样最先进的语言模型，在这方面的一致性只有76%。为了提升语言模型的一致性，我们建议在过滤后的生成器和验证器响应上进行微调，称之为一致性微调。我们发现这种方法将Alpaca-30B的一致性从60%提升至93%，这种改进能够延伸到未知任务和域（例如积极风格转换的一致性延伸到未知风格，如幽默）。一致性微调不仅提高了一致性，还在不使用任何标记数据的情况下提高了生成器质量和验证器准确率。在包括数学问题、知识密集型问答和指令遵循在内的6个任务上评估，我们的方法将生成器质量提高了16%，验证器准确率提高了6.3%。
link: http://arxiv.org/abs/2310.01846v1
publish_time: 2023-10-03

title: Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench
abstract: 大型语言模型(LLMs)展示出非凡的能力，不仅在自然语言处理任务中表现出色，还跨越多个领域，如临床医学、法律咨询和教育。LLMs不仅仅是应用程序，而是能够处理各种用户请求的助手，进一步缩小了人类和人工智能代理之间的差距，引发了关于LLMs在个性、性格和情感方面潜在展现的有趣问题。在本文中，我们提出了一个用于评估LLMs多种心理方面的框架PsychoBench，包括了临床心理学中常用的十三个量表，进一步将这些量表分类为四个不同的类别：人格特征、人际关系、动机测试和情绪能力。我们的研究考察了五个流行的模型，即text-davinci-003、gpt-3.5-turbo、gpt-4、LLaMA-2-7b和LLaMA-2-13b。此外，我们采用了越狱方法绕过安全对齐协议，测试了LLMs的内在属性。我们已经通过https://github.com/CUHK-ARISE/PsychoBench 开放了PsychoBench。
link: http://arxiv.org/abs/2310.01386v2
publish_time: 2023-10-02

title: ARN: A Comprehensive Framework and Benchmark for Analogical Reasoning on Narratives
abstract: 人类的模拟推理能力是与创造力和科学发现密切相关的主要能力之一，尽管这一能力已在自然语言处理和认知心理学领域得到广泛研究，但对叙事中的模拟推理却并未受到广泛关注。我们建立了一个全面的评估框架，利用叙事元素创建低阶和高阶映射，进而开发了覆盖远距（跨领域）/近距（领域内）类比和远距/近距非类比四类类别的叙事模拟推理（ARN）基准，使我们能够研究LLMs在不同场景下的类比推理。我们的结果表明，LLMs在没有低阶映射的情况下（远类比）很难识别高阶映射，而在所有映射同时形成时（近类比）表现更好。我们观察到，在所有情景下，LLMs的模拟推理能力很容易受到近非类比中的低阶映射的影响。
link: http://arxiv.org/abs/2310.00996v2
publish_time: 2023-10-02

title: Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models
abstract: 人类的基本逻辑推理对于人类至关重要，但在人工智能领域却面临着重大挑战。最近，大语言模型的出现展示了克服形式知识表示系统各种限制的能力，因此人们对使用大型语言模型通过自然语言进行逻辑推理产生了浓厚的兴趣。通过LogiGLUE基准测试和LogiT5模型，研究逻辑推理任务在大语言模型中的有效性，并探讨提高大型语言模型逻辑推理能力的潜在途径。
link: http://arxiv.org/abs/2310.00836v2
publish_time: 2023-10-02

title: TRAM: Benchmarking Temporal Reasoning for Large Language Models
abstract: 时间推理对于理解自然语言中描述的事件细微差别至关重要。本文介绍了TRAM，一个包含十个数据集的时间推理基准测试，涵盖了事件的各种时间方面，如顺序、算术、频率和持续时间，旨在促进大型语言模型（LLMs）在时间推理能力方面的全面评估。我们使用流行的LLMs（例如GPT-4和Llama2）在零迁移和少量迁移学习场景下进行了广泛评估。此外，我们还使用基于BERT的模型建立了基准评估。我们的发现表明，这些模型在时间推理任务中仍然落后于人类表现。我们希望TRAM能够促进进一步提升LLMs的时间推理能力。
link: http://arxiv.org/abs/2310.00835v2
publish_time: 2023-10-02

title: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models
abstract: 大型语言模型（LLMs）的出现为角色扮演等复杂任务打开了大门，通过让模型模仿各种角色来增强用户互动。但目前最先进的LLMs是封闭源码的，通用训练方式限制了角色扮演的优化。本文介绍了RoleLLM框架，用于评估、引发和增强LLMs的角色扮演能力，通过四个阶段构建角色概况、生成基于上下文的指导、使用GPT进行角色提示和进行细化调优。通过Context-Instruct和RoleGPT，我们创建了RoleBench，这是第一个系统化和细粒度的角色层面基准数据集，RoCIT在RoleBench上产生了RoleLLaMA（英文）和RoleGLM（中文），显著增强了角色扮演能力，甚至取得了与使用GPT-4的RoleGPT相当的结果。
link: http://arxiv.org/abs/2310.00746v1
publish_time: 2023-10-01

title: FELM: Benchmarking Factuality Evaluation of Large Language Models
abstract: 评估大型语言模型生成的文本的事实性是一个新兴但关键的研究领域，旨在警示用户可能出现的错误并引导更可靠LLM的发展。然而，评估事实性的评估者本身需要适当的评估来衡量进展并促进进步。这个方向尚未得到充分开发，导致事实性评估的进展受到重大阻碍。为了减轻这一问题，我们引入了一个大型语言模型事实性评估的基准，称为felm。在这个基准中，我们收集了从LLM生成的响应，并以细致的方式注释了事实性标签。与以往主要集中在世界知识事实性（例如，来自维基百科的信息）的研究相反，felm关注的是跨越各种领域的事实性，从世界知识到数学和推理。我们的标注基于文本段落，可以帮助准确定位特定的事实错误。事实性标注进一步由预定义的错误类型和支持或反驳陈述的参考链接补充。在我们的实验中，我们研究了几种基于LLM的事实性评估器在felm上的性能，包括既有检索机制又有思维链的基础LLM和那些增强型LLM。我们的研究结果显示，虽然检索有助于事实性评估，但当前的LLM远未能准确检测事实错误。
link: http://arxiv.org/abs/2310.00741v2
publish_time: 2023-10-01

title: The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes
abstract: 手语识别和翻译技术有潜力增加聋人手语社区的融入和包容性，但由于缺乏代表性数据，研究进展受到限制。我们引入了一个新的美国手语（ASL）建模资源，Sem-Lex Benchmark。这一Benchmark是目前规模最大的，包括来自聋人ASL手语者的超过84,000个孤立手语制作视频，他们已经同意并接受了补偿。人类专家将这些视频与其他手语资源（包括ASL-LEX，SignBank和ASL Citizen）进行了对齐，为手语和音韵特征识别提供了有用的扩展。我们展示了一系列利用ASL-LEX中的语言信息的实验，评估Sem-Lex Benchmark在孤立手语识别（ISR）中的实用性和公正性。我们使用SL-GCN模型显示，音韵特征可达到85%的识别率，并且作为ISR的辅助目标是有效的。与gloss一起学习识别音韵特征结果可提高few-shot ISR准确率6%，总体ISR准确率提高2%。下载数据的指南可在https://github.com/leekezar/SemLex找到。
link: http://arxiv.org/abs/2310.00196v1
publish_time: 2023-09-30

title: Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?
abstract: 大型语言模型（LLMs）在自然语言处理和编码任务方面取得了显著进展，但它们在处理代表数据的形式语言，特别是在知识图工程领域方面，仍未受到深入研究。我们创建了一组五个任务，评估各种LLMs的能力，这些任务通过Turtle语法序列化知识图来考察它们的解析、理解、分析和创建能力。我们将这些任务集成到我们的自动化评估系统LLM-KG-Bench中，对四个商业可用的LLMs进行了评估 - GPT-3.5、GPT-4、Claude 1.3和Claude 2.0，以及两个免费在线模型GPT4All Vicuna和GPT4All Falcon 13B。这项分析深入了解了LLMs在利用Turtle表示的RDF知识图工程工作流中的优势和不足。我们的研究结果显示，最新的商业模型在Turtle语言方面的表现优于以往的模型，但也揭示了一个明显的弱点。在严格遵守输出格式约束方面，这些模型存在不足，而这在这一背景下是一个关键要求。
link: http://arxiv.org/abs/2309.17122v1
publish_time: 2023-09-29

title: Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection
abstract: MUStARD数据集及其情感识别扩展MUStARD++的引入，揭示了讽刺是一种多模态现象，不仅表现在自然语言文本中，还通过言语方式（如语调和语气）和视觉线索（面部表情）来表达。我们旨在通过考虑最先进的语言、语音和视觉编码器，对MUStARD++数据集进行严格的基准测试，充分利用其提供的多模态丰富性，使宏-F1值较现有基准提高2％。此外，为了解决MUStARD++中`sarcasm type'类别的不平衡问题，我们提出一种名为\emph{MUStARD++ Balanced}的扩展。在扩展中的实例分布在训练和测试集上，进一步提高2.4%的宏-F1值。新片段来源于电视剧《豪斯医生》，增加了数据集的多样性，并由多名标注者进行手动标注，达到了较高的一致性。我们的代码、扩展数据和最先进的基准模型已经公开发布。
link: http://arxiv.org/abs/2310.01430v1
publish_time: 2023-09-29

title: Benchmarking Cognitive Biases in Large Language Models as Evaluators
abstract: 最近的研究表明，大型语言模型（LLMs）在简单提示和上下文学习方面已被证明有效作为自动评估器。LLMs存在着对文本质量的偏见，导致其评估结果与人类偏好不一致，因此可能仍无法用于与人类偏好一致的自动注释。
link: http://arxiv.org/abs/2309.17012v1
publish_time: 2023-09-29

title: GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond
abstract: 随着大型语言模型(LLMs)的迅速发展，迫切需要一个全面的评估套件来评估它们的能力和局限性。
link: http://arxiv.org/abs/2309.16583v5
publish_time: 2023-09-28

title: A Benchmark for Learning to Translate a New Language from One Grammar Book
abstract: 大型语言模型可以通过上下文学习或轻量微调实现令人印象深刻的壮举。对于这些模型如何适应真正的新任务，如何找到在互联网规模训练集中看不到的任务，是一个很自然的疑问。我们转向一个明确受到网络数据稀缺问题启发和瓶颈制约的领域：低资源语言。通过使用几百页的现场语言学参考材料，我们引入了MTOB（从一本书的机器翻译）这个基准。我们希望MTOB可以帮助衡量LLM在一个新维度上的能力，并且解决它的方法可以通过利用与传统机器翻译不同类型的数据，帮助扩大语言技术对服务不足社区的覆盖。
link: http://arxiv.org/abs/2309.16575v2
publish_time: 2023-09-28

title: KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models
abstract: 最近，定位-编辑范式已经成为改变语言模型中存储事实知识的主要方法之一，然而，目前的定位方法能否精确定位所需知识的参数仍缺乏研究。此外，尽管许多研究人员质疑事实知识的局部性假设的有效性，但并没有提供一种方法来测试该假设以进行更深入的讨论和研究。因此，我们介绍了KLoB，一个检查可靠知识定位方法应具备的三个基本属性的基准。KLoB可以作为评估现有语言模型中定位方法的基准，并为重新评估事实知识的局部性假设提供一种方法。我们的方法公开在 \url{https://github.com/juyiming/KLoB} 上。
link: http://arxiv.org/abs/2309.16535v1
publish_time: 2023-09-28

title: Toloka Visual Question Answering Benchmark
abstract: 本文介绍了Toloka视觉问答，这是一个新的众包数据集，允许比较机器学习系统在视觉问答任务中的表现与人类专业水平。在这个任务中，给定一幅图像和一个文本问题，需要正确地在物体周围画出边界框以回答问题。数据集包含45,199对图像和问题，分为英文训练集和两个测试集，并提供了准确的边界框。虽然在WSDM Cup举办了一次多阶段比赛，吸引了来自全球48个参与者，但截至论文提交时，没有任何机器学习模型能够超过非专家众包基线的交并比评分。
link: http://arxiv.org/abs/2309.16511v1
publish_time: 2023-09-28

title: LawBench: Benchmarking Legal Knowledge of Large Language Models
abstract: 总结：大型语言模型在各方面展示出强大的能力，但在将它们应用于高度专业化、安全关键的法律领域时，对它们具有多少法律知识以及它们是否能够可靠地执行法律相关任务仍存在不确定性。为了解决这一问题，我们提出了一个全面的评估基准LawBench，以对LLMs的法律能力进行精确评估。
link: http://arxiv.org/abs/2309.16289v1
publish_time: 2023-09-28

title: Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems
abstract: 近期大规模语言模型（LLMs）数量和可用性激增，为评估知识图谱问答（KGQA）系统提供大规模和真实的基准数据变得愈发重要。现有的基准数据往往依赖基于模式的SPARQL查询生成方法，而非常规自然语言问题不一定适用于现实环境中人类提出的模糊和语言多样的问题。Spider4SPARQL提供了9,693个之前手动生成的自然语言问题和4,721个独特且复杂的SPARQL查询，以及对应的166个知识图谱和本体。这一挑战性基准数据使得现代KGQA系统的优势和劣势得以新颖评估，而现有的最先进KGQA系统和LLMs执行准确率仅达到45\%，证明了Spider4SPARQL对未来研究有着挑战性。
link: http://arxiv.org/abs/2309.16248v2
publish_time: 2023-09-28

title: Large Language Model Routing with Benchmark Datasets
abstract: 众多开源大语言模型和基准数据集的快速增长，挑选最佳LLM模型面临挑战，提出一种新的解决方案。
link: http://arxiv.org/abs/2309.15789v1
publish_time: 2023-09-27

title: Updated Corpora and Benchmarks for Long-Form Speech Recognition
abstract: 研究表明，在大多数真实环境的语音识别应用中，模型训练和测试数据并没有被切分成独立的utterances，导致了训练和推断之间的不匹配。通过更新转录和对齐数据，我们重新发布了三个标准ASR语料库，并使用它们研究了长篇ASR模型的训练-测试不匹配问题，证实了AEDs对此问题更为敏感。最后，我们对这些模型进行了简单的长篇训练基准测试，展示了在这种领域转移情况下模型稳健性的效果。
link: http://arxiv.org/abs/2309.15013v1
publish_time: 2023-09-26

title: BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models
abstract: LLMs在NLP任务上取得了惊人的成绩，研究最近致力于扩展上下文长度并增强LLMs对长文本的建模能力。我们提出了BAMBOO，一个多任务长上下文基准，用于全面评估LLMs的长上下文能力。
link: http://arxiv.org/abs/2309.13345v3
publish_time: 2023-09-23

title: Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam
abstract: 尽管大型语言模型（LLMs）代表与计算机交互的革命，允许构建复杂问题和推理一系列陈述的能力，但由于需要专用硬件才能执行，它们的使用受到了限制。
link: http://arxiv.org/abs/2309.12071v1
publish_time: 2023-09-21

title: CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation
abstract: 本文针对手动在线广告制作的局限性，进行了广告文本自动生成领域的重要研究。然而，由于缺乏囊括整个领域的基准和缺乏清晰的模型输入和输出的明确定义问题集，不同方法的比较一直是一项挑战。为了解决这些挑战，本文旨在通过引入重新设计的任务和构建一个基准来推进ATG领域。具体来说，我们将ATG定义为一个跨应用任务，涵盖Internet广告的各个方面。作为我们的贡献的一部分，我们提出了第一个基准数据集，CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA)，精心设计用于ATG，以利用多模态信息并进行产业化评估。此外，我们通过使用多个基线模型进行评估实验，这些模型在使用预训练语言模型的类型和多模态信息的整合方面存在差异，展示了我们提出的基准的实用性。我们还讨论了当前任务的现状和未来挑战。
link: http://arxiv.org/abs/2309.12030v1
publish_time: 2023-09-21

title: XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates
abstract: 本文介绍了XATU，这是第一个专门设计用于细粒度指令驱动可解释文本编辑的基准测试。
link: http://arxiv.org/abs/2309.11063v2
publish_time: 2023-09-20

title: Benchmarks for Pirá 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change
abstract: Pir'a是一个专注于海洋、巴西海岸和气候变化的阅读理解数据集，从这些主题的一系列科学摘要和报告构建而成。 这个数据集代表了一种多功能的语言资源，特别适用于测试当前机器学习模型获取专业科学知识的能力。 尽管具有潜力，但尚未为Pir\'a开发详细的基线。 通过创建这些基线，研究人员可以更容易地利用Pir\'a作为测试机器学习模型在各种问题回答任务中的资源。 本文中，我们在Pir\'a数据集上定义了六个基准，涵盖了封闭的生成式问题回答、机器阅读理解、信息检索、开放式问题回答、答案触发和多项选择问题回答。 作为这一努力的一部分，我们还制作了原始数据集的筛选版本，其中修复了一些语法问题、重复和其他不足之处。 此外，该数据集还在几个新方向上进行了扩展，以面对上述基准：将支持文本从英语翻译成葡萄牙语，为可回答性添加分类标签，自动生成问题和答案的改写，以及多项选择候选项。 本文描述的结果为对探索Pir'a数据集提供的挑战感兴趣的研究人员提供了几个参考点。
link: http://arxiv.org/abs/2309.10945v1
publish_time: 2023-09-19

title: Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark
abstract: 现有命题往往依赖逻辑常数进行分类，相比英语等偏向假设的西方语言，汉语在日常表达中常常依赖语义或逻辑理解而非逻辑连接词，呈现出并列句的特点。然而，现有研究很少关注这一问题。准确分类这些命题对自然语言理解和推理至关重要。本文提出了显式和隐式命题的概念，并提出了基于语言学和逻辑的多层命题分类系统。相应地，我们从多个领域创建了一个大规模的汉语命题数据集PEACE，涵盖了所有与命题相关的类别。为了评估现有模型的汉语命题分类能力并探讨其局限性，我们使用基于规则的方法、SVM、BERT、RoBERTA和ChatGPT等多种方法对PEACE进行评估。结果表明正确建模命题的语义特征的重要性。BERT具有相对较好的命题分类能力，但缺乏跨领域的可迁移性。ChatGPT表现不佳，但通过提供更多命题信息可以改善其分类能力。许多问题仍未得到解决，需要进一步研究。
link: http://arxiv.org/abs/2309.09602v1
publish_time: 2023-09-18

title: Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles
abstract: 以前的多文档新闻摘要研究通常集中在整理所有来源都同意的信息上。然而，摘要多篇关于同一事件的文章中分散的多样信息仍未得到充分探索。本文提出了一个新的任务，即总结涵盖相同事件的多篇新闻文章中的多样信息。为了推动这一任务，我们提出了一个用于识别多样信息的数据收集模式，并整理了一个名为DiverseSumm的数据集。数据集包括245篇新闻报道，每个报道包含10篇新闻文章，并且配有经过人工验证的参考。接下来，为了实现一致的自动评估，我们进行了全面分析，找出了使用大型语言模型（LLM）指标评估摘要覆盖度和忠实度时存在的位置和冗长偏见。通过相关性分析，我们总结了在DiverseSumm数据集上有效使用自动LLM指标的最佳实践。最后，我们研究了LLM如何通过分析多篇新闻文章来总结，从而分析了LLM能够识别哪种类型的多样信息。我们的分析表明，尽管LLM在单一文档摘要中具有非凡的能力，但由于其覆盖范围有限，GPT-4只能平均覆盖不到40%的多样信息，所以提出的任务对它们来说仍然是一个复杂的挑战。
link: http://arxiv.org/abs/2309.09369v2
publish_time: 2023-09-17

title: A Benchmark for Text Expansion: Datasets, Metrics, and Baselines
abstract: 本工作提出了一个新的文本扩展任务（Text Expansion，TE），旨在将细粒度的修饰语插入到普通文本的适当位置，以具体化或生动化人类写作。TE与现有的基于插入的写作辅助任务不同，需要模型在定位和生成方面更加灵活，并在保持基本语义方面更加谨慎。我们利用四种互补的方法构建了一个包含1200万个自动生成实例和2K个英文和中文人工注释参考的数据集。为了便于自动评估，我们设计了多角度的多种度量标准。特别是，我们提出了Info-Gain来有效衡量扩展的信息量，这是TE中一个重要的质量维度。在预训练文本插填模型的基础上，我们构建了管道式和定位与填充的联合模型，展示出对Text2Text基线的优越性，尤其是在扩展信息量方面。实验证实了TE任务的可行性，并指出未来研究更好的自动文本扩展的潜在方向。
link: http://arxiv.org/abs/2309.09198v1
publish_time: 2023-09-17

title: ODSum: New Benchmarks for Open Domain Multi-Document Summarization
abstract: 开放领域多文档摘要（ODMDS）是将大量文档压缩成连贯简洁摘要的关键工具。提出了一种基于规则的方法，将基于查询的文档摘要数据集处理成ODMDS数据集。我们通过“检索-摘要”方法解决ODMDS问题，并调查一系列检索器和摘要器的性能。我们发现LLMs在检索错误时性能大幅下降，尝试改进性能方法并探究其对不完美检索的鲁棒性。通过广泛实验，我们发现了评估指标的差异并提供了可靠的见解。我们将在https://github.com/yale-nlp/ODSum上释放我们的数据和代码。
link: http://arxiv.org/abs/2309.08960v1
publish_time: 2023-09-16

title: Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite
abstract: 大型语言模型的评估是语言理解和生成领域中至关重要的任务，随着语言模型的不断发展，评估它们性能的有效基准需求变得迫切。尽管存在一些基准数据集如DRCD、TTQA、CMDQA和FGC数据集，但在传统汉语的背景下，评估语言模型能力的全面和多样化基准数据集缺乏。为了填补这一空白，我们提出了一套新的基准数据集，利用现有的英文数据集，特别设计以评估传统汉语语言模型。这些基准数据集涵盖了广泛的任务，包括上下文问答、摘要、分类和表格理解。提出的基准数据集提供了一个全面的评估框架，使得能够评估语言模型在不同任务上的能力。本文中，我们在这些基准数据集上评估了GPT-3.5、Taiwan-LLaMa-v1.0和Model 7-C，我们的专有模型。评估结果表明，我们的模型Model 7-C 在一部分评估能力方面与GPT-3.5 的性能相当。为了推动传统汉语语言模型评估的进一步发展并激发该领域更多的研究，我们已开源我们的基准数据集并开放了该模型供试用。
link: http://arxiv.org/abs/2309.08448v2
publish_time: 2023-09-15

title: Anchor Points: Benchmarking Models with Much Fewer Examples
abstract: 现代语言模型通常表现出强大但脆弱的行为，导致发展更大更多样的基准来可靠地评估它们的行为。我们建议可以用更小的评估集来评估和阐明模型的性能。通过对六个流行的语言分类基准的分析，我们首先展示了许多点对中模型对正确类别的信心在各个模型之间强相关。我们基于这一现象提出了Anchor Point Selection技术，该技术可选择捕获整个数据集中模型行为的小子集。Anchor points可靠地对模型进行排名：在87个不同的语言模型-提示对中，使用1-30个anchor points评估模型优于均匀采样和其他基准，在准确排名模型方面表现得更好。此外，只需几个anchor points就可以用较低的平均绝对误差估计数据集上其他点上模型的每类预测，足以判断模型可能失败的地方。最后，我们提出了Anchor Point Maps来可视化这些洞察力，促进对不同模型在数据集分布的各个区域的性能的比较。
link: http://arxiv.org/abs/2309.08638v2
publish_time: 2023-09-14

title: VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue
abstract: 视觉对话系统已经成为一个越来越受欢迎的研究领域，但缺乏标准化的评估框架给这个领域的发展带来了挑战。为此，我们提出了VDialogUE，一个用于统一评估的视觉对话基准测试，旨在加速视觉对话系统的发展。
link: http://arxiv.org/abs/2309.07387v1
publish_time: 2023-09-14

title: Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish
abstract: 通过研究土耳其的程序性文本，我们发现使用语言特定模型的性能始终优于多语言模型，并为大多数程序语言理解任务提供了重要贡献。
link: http://arxiv.org/abs/2309.06698v2
publish_time: 2023-09-13

title: FIND: A Function Description Benchmark for Evaluating Interpretability Methods
abstract: 将神经网络子模块标记为人类可读的描述对许多下游任务都很有用：这些描述能够凸显失败，指导干预，甚至解释重要的模型行为。到目前为止，大多数训练网络的机理描述涉及小模型、狭义现象和大量人力。为了标记越来越大和复杂的模型中所有可人类解释的子计算，几乎肯定需要可以自动生成和验证描述的工具。最近，利用学习模型进行标记的技术开始受到关注，但评估其有效性的方法有限且是临时的。我们应该如何验证和比较无限制的标记工具？本文介绍了FIND（Function INterpretation and Description），一个用于评估自动可解释性方法组件的基准套件。FIND包含类似于训练神经网络组件的函数，并提供了我们想要生成的描述。这些函数涵盖了文本和数字领域，并涉及各种现实世界的复杂性。我们评估了使用预训练语言模型（LMs）生成自然语言和代码中的函数行为描述的方法。此外，我们还介绍了一种新的交互方法，其中自动可解释性代理（AIA）生成函数描述。我们发现，一个由具有对函数的黑盒访问的LM构建的AIA可以推断函数结构，就像一个科学家形成假设，提出实验，并根据新数据更新描述。然而，AIA描述往往捕捉全局函数行为，忽略了局部细节。这些结果表明，在将更复杂的解释性方法应用于真实世界模型之前，FIND将对评估非常有用。
link: http://arxiv.org/abs/2309.03886v3
publish_time: 2023-09-07

title: The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties
abstract: 五种流行的句子编码器在下游任务性能和捕捉基本语义属性方面进行了比较，发现虽然在SentEval基准测试中表现出色，但仍然在一些基本语义属性上存在困难，这对自然语言处理研究提出了严峻问题。
link: http://arxiv.org/abs/2309.03747v1
publish_time: 2023-09-07

title: Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media
abstract: 在社交媒体领域，用户经常传达个人情感，有些可能表明认知失真或自杀倾向，及时识别这些迹象对有效干预至关重要。我们提出了两个新颖的中文社交媒体数据集，分别关注认知失真和自杀风险分类。我们提出了一个综合的基准，使用监督学习和大语言模型，尤其是GPT系列，来评估这些数据集的性能。为了评估大语言模型的能力，我们采用了零-shot、少-shot和微调三种策略。此外，我们从心理学角度深入探讨和分析了这些大语言模型在识别和理解复杂人类情绪方面的表现，突显出两种方法之间的性能差异，这些模型往往受到微妙类别区分的挑战。尽管GPT-4始终传递出强大的结果，但在微调后，GPT-3.5在自杀风险分类方面表现出显著的改进。这项研究在评估大语言模型在中文社交媒体任务中的应用方面具有开创性，突显了模型在心理学背景下的潜力。所有数据集和代码均可供使用。
link: http://arxiv.org/abs/2309.03564v2
publish_time: 2023-09-07

title: AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models
abstract: 大语言模型（LLMs）如ChatGPT展示出惊人的智能，评估其解题能力和智能程度是一个热点但具有挑战性的问题。AGIBench提出了一个多粒度、多模态、以人为参照和自动评分的基准测试方法，支持多维度度量，并可从\url{https://www.benchcouncil.org/agibench}公开获取。
link: http://arxiv.org/abs/2309.06495v1
publish_time: 2023-09-05

title: CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models
abstract: 大型语言模型（LLMs）的出现显著提高了模型的编程能力，吸引了研究人员越来越多的关注。 CodeApex提出了一个双语基准数据集，重点测试LLMs的编程理解、代码生成和代码纠正能力，希望能够促进LLMs的发展和进步。
link: http://arxiv.org/abs/2309.01940v4
publish_time: 2023-09-05

title: Benchmarking Large Language Models in Retrieval-Augmented Generation
abstract: 通过研究不同大型语言模型对检索增强生成的影响，我们发现当前大语言模型在负面拒绝、信息整合和处理虚假信息方面仍存在挑战，表明在将检索增强生成应用于大型语言模型方面仍有相当的挑战。
link: http://arxiv.org/abs/2309.01431v2
publish_time: 2023-09-04

title: The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants
abstract: Belebele是一个包括122种语言变体的多项选择机器阅读理解(MRC)数据集，大大扩展了自然语言理解(NLU)基准的语言覆盖范围，使得对文本模型在高、中和低资源语言方面的评估成为可能。
link: http://arxiv.org/abs/2308.16884v1
publish_time: 2023-08-31

title: Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering
abstract: 大语言模型(LLMs)领域正在快速发展，对其性能进行评估和监测变得至关重要。我们引入了一个以知识图谱工程为重点的基准测试框架，包括语法和错误校正、事实提取和数据集生成三个挑战。我们展示了LLMs虽然是一个有用的工具，但尚不适合用于零提示的知识图生成。因此，我们的LLM-KG-Bench框架提供了LLM响应的自动评估和存储，以及统计数据和可视化工具，以支持提示工程和模型性能的跟踪。
link: http://arxiv.org/abs/2308.16622v1
publish_time: 2023-08-31

title: BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge
abstract: 预训练的大型语言模型显著改善了代码生成，我们通过针对生物信息学的 BioCoder 基准测试展示了其在生成生物信息学特定代码方面的应用和效果。
link: http://arxiv.org/abs/2308.16458v4
publish_time: 2023-08-31

title: Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations
abstract: 介绍了情感视觉对话(Affective Visual Dialog)任务，作为研究理解视觉对话中情感形成的测试基础。该任务涉及三种技能：(1)基于对话的问答，(2)基于对话的情感预测，(3)基于对话生成情感解释。我们的主要贡献是收集了一个大规模数据集AffectVisDial，包含50K个10轮视觉对话以及情感归因和基于对话的文本情感解释，总计耗时27,180个工作小时。我们解释了在收集数据集时的设计决策，并介绍了与对话参与者相关的提问者和回答者任务。我们训练并展示了基于最先进模型的可靠情感视觉对话基线。值得注意的是，我们模型生成的回答表现出了在视觉对话中有希望的情感推理能力。我们的项目页面可在https://affective-visual-dialog.github.io 上访问。
link: http://arxiv.org/abs/2308.16349v2
publish_time: 2023-08-30

title: Benchmarking Multilabel Topic Classification in the Kyrgyz Language
abstract: 本文介绍了一个基于从新闻网站24.KG收集和注释的数据的公开基准测试，为吉尔吉斯语的主题分类提供了一个新的数据集，并提出了多标签设置下的几种基准模型。我们训练和评估了传统统计模型和神经模型，报告了得分，讨论了结果，并提出了未来工作的方向。Kyrgyz在现代自然语言处理资源方面是一个非常欠代表的语言。
link: http://arxiv.org/abs/2308.15952v1
publish_time: 2023-08-30

title: Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation
abstract: 大型语言模型（LLMs）已经成为文本到SQL任务的一种新范式。然而，缺乏系统性的基准测试阻碍了设计有效、高效和经济的基于LLM的文本到SQL解决方案的发展。为了应对这一挑战，在本文中，我们首先对现有的提示工程方法进行了系统和广泛的比较，包括问题表示、示例选择和示例组织，并通过这些实验结果，详细阐述它们的优缺点。基于这些发现，我们提出了一种名为DAIL-SQL的新一体化解决方案，刷新了Spider排行榜，达到了86.6%的执行准确率，开创了新局面。为了探索开源LLM的潜力，我们在各种场景中进行了调查，并通过监督微调进一步提高了它们的性能。我们的探索突显了开源LLM在文本到SQL中的潜力，以及监督微调的优势和劣势。此外，为了实现高效和经济的基于LLM的文本到SQL解决方案，我们强调了提示工程中的令牌效率，并根据此度量比较了之前的研究。 我们希望我们的工作能够深入了解基于LLMs的文本到SQL，并激发进一步的研究和广泛的应用。
link: http://arxiv.org/abs/2308.15363v4
publish_time: 2023-08-29

title: Benchmarking the Generation of Fact Checking Explanations
abstract: 打击虚假信息是一项具有挑战性但至关重要的任务，自动化这一过程是必要的以帮助遏制虚假信息的传播。本文研究了对断言进行证明的生成，并通过新颖的数据集和先进的基准进行了基准测试。研究表明，在证明生成摘要中，利用断言信息可以提高抽取式和生成式摘要的性能。而处理不同风格和结构的两个数据集时，性能可能会有所下降，但一个结合了两个数据集的模型能够有效地保留风格信息。
link: http://arxiv.org/abs/2308.15202v1
publish_time: 2023-08-29

title: LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding
abstract: 尽管大型语言模型（LLMs）在许多语言任务中展现出令人印象深刻的性能，但大多数模型只能处理几千个标记的文本，这限制了它们在更长的序列输入（如书籍、报告和代码库）上的应用。最近的研究提出了改进LLMs长上下文能力的方法，通过扩展上下文窗口和更复杂的记忆机制。然而，缺乏针对评估长上下文理解能力的综合基准。在本文中，我们介绍了LongBench，这是第一个针对长上下文理解的双语、多任务基准，可以更严格地评估长上下文理解。LongBench包括21个数据集，涵盖了6个任务类别，其中包括英文和中文，平均长度为6711个单词（英文）和13386个字符（中文）。这些任务涵盖了关键的长文本应用领域，包括单文档问答、多文档问答、摘要、少样本学习、合成任务和代码补全。LongBench中的所有数据集都被标准化成统一的格式，使得能够轻松自动评估LLMs。在LongBench上对8个LLMs进行全面评估后，我们发现：（1）商用模型（GPT-3.5-Turbo-16k）在表现上超越其他开源模型，但仍然在长上下文方面遇到困难。（2）扩展位置嵌入和对更长序列的微调会在长上下文理解上带来显著改进。（3）像检索这样的上下文压缩技术对那些长上下文理解能力较弱的模型带来一定的改进，但在性能上仍然落后于具有强大长上下文理解能力的模型。 该代码和数据集可在https://github.com/THUDM/LongBench上获取。
link: http://arxiv.org/abs/2308.14508v1
publish_time: 2023-08-28

title: ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
abstract: 大语言模型的卓越表现需要全面准确的评估，我们提出了朱九基准，并通过多维度覆盖、多方位评价方法合作、全面中文基准以及避免潜在数据泄露等强项来综合评估大语言模型，确保评估结果的权威性和准确性。
link: http://arxiv.org/abs/2308.14353v1
publish_time: 2023-08-28

title: Goodhart's Law Applies to NLP's Explanation Benchmarks
abstract: 尽管基于显著性的解释越来越受欢迎，但研究界仍在陷入僵局，对其目的、效力和倾向相互矛盾存在疑虑。近期一些工作提出了评估指标，试图团结社区力量共同达成目标。然而，我们对ERASER指标（全面性和足够性）和EVAL-X指标进行了批判性审查，重点关注自然语言处理领域。我们展示了可以显著提高模型的全面性和足够性得分，而不需要改变其对分布测试数据的预测或解释。我们的策略利用了提取解释及其补充之间相对不一致以及测试数据集的特点。接着，我们展示了EVAL-X指标可以通过简单的方法任意夸大，即使EVAL-X旨在解决此类欺骗行为。我们的结果对当前指标引导可解释性研究的能力提出了疑问，强调了对这些指标确切意图的更广泛重新评估的必要性。
link: http://arxiv.org/abs/2308.14272v1
publish_time: 2023-08-28

title: How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection
abstract: 近年来，在计算机视觉领域，目标检测（OD）取得了显著进展，从封闭标签过渡到基于大规模视觉语言预训练（VLP）的开放词汇检测（OVD）。然而，当前的评估方法和数据集仅限于测试目标类型和指称表达的泛化能力，这并不能提供对OVD模型能力进行系统、细粒度和准确评估的基准。本文提出了一个名为OVDEval的新的基准，包括9个子任务，并引入对常识知识、属性理解、位置理解、物体关系理解等的评估。数据集经过精心创建，提供了挑战模型对视觉和语言输入的真正理解的难点负例。此外，我们发现在这些细粒度标签数据集上基准模型时，流行的平均精度（AP）指标存在问题，因此提出了一个新指标叫做非极大值抑制平均精度（NMS-AP）来解决这个问题。大量实验结果表明，除了简单的物体类型外，现有的顶级OVD模型在新任务上都失败了，这证明了提出数据集在指出当前OVD模型弱点并指导未来研究的价值。此外，实验证实，所提出的NMS-AP度量提供了对OVD模型更为真实的评估，而传统的AP度量会产生误导性结果。数据可在\url{https://github.com/om-ai-lab/OVDEval}获取。
link: http://arxiv.org/abs/2308.13177v2
publish_time: 2023-08-25

title: SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research
abstract: 最近，对于使用大型语言模型（LLMs）进行科学研究的兴趣日益增加，提出了许多评估LLMs能力的基准，但当前的基准大多基于预先收集的客观问题，存在数据泄漏问题和缺乏主观问答能力评估的问题。本文提出了SciEval，一个全面多学科评估基准，涵盖四个维度系统评估科学研究能力，设计了基于科学原则的“动态”子集以防止潜在数据泄漏的评估，包含客观和主观问题，这些特点使得SciEval成为更有效的评估LLMs科学研究能力的基准。对最先进的LLMs进行的综合实验表明，尽管GPT-4在与其他LLMs相比实现了SOTA性能，但仍有许多改进的空间，特别是在动态问题方面。数据和代码现已公开。
link: http://arxiv.org/abs/2308.13149v1
publish_time: 2023-08-25

title: CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias
abstract: 随着语言模型（LMs）变得越来越强大和广泛应用，量化它们可能会造成的社会人口统计偏见是非常重要的。CALM可以有效地衡量两种普遍相关的社会人口统计偏见——性别和种族。
link: http://arxiv.org/abs/2308.12539v2
publish_time: 2023-08-24

title: Efficient Benchmarking of Language Models
abstract: 语言模型的多功能性不断增强，引发了一类全面评估各种能力的新基准，但在文献中对评估效率方面的讨论不多。在这项工作中，我们提出了“高效基准测试”的问题，即在不影响可靠性的情况下智能地降低语言模型评估的计算成本。通过以HELM基准测试为案例，我们研究了不同基准设计选择如何影响计算-可靠性权衡。我们提出使用一个新的度量标准DIoR来评估这些决策对可靠性的影响。根据我们的研究结果，我们提出了一系列关于更高效的基准设计和利用实践的具体建议，实现了大幅的成本节约，同时最小限度地损失基准的可靠性，通常可将计算量减少100倍或更多。
link: http://arxiv.org/abs/2308.11696v4
publish_time: 2023-08-22

title: StoryBench: A Multifaceted Benchmark for Continuous Story Visualization
abstract: 视频故事生成是一个复杂的任务，视频不仅需要具有高视觉质量，还需要与文本提示序列保持一致，同时在各帧上保持一致。 StoryBench旨在鼓励未来在这一激动人心的新领域的研究努力。
link: http://arxiv.org/abs/2308.11606v2
publish_time: 2023-08-22

title: BELB: a Biomedical Entity Linking Benchmark
abstract: 生物医学实体链接(BEL)是将实体提及与知识库关联的任务，在生命科学文献信息抽取管道中起着至关重要的作用。最近的研究发现，由于该任务在现有的生物医学文本挖掘基准中缺失，不同的研究采用不同的实验设置，使得基于已发表的数字进行比较变得困难。此外，神经系统主要在与广泛覆盖知识库UMLS相关的实例上进行了测试，而对于更专门的实体(例如基因或变异体)的性能研究很少。因此，我们开发了BELB，一个生物医学实体链接基准，以统一格式提供了与7个知识库相连的11个语料库，涵盖了基因、疾病、化学物质、物种、细胞系和变异体六种实体类型。BELB极大地减少了在多个语料库上测试BEL系统时的预处理工作量，为可重复实验提供了标准化的测试平台。利用BELB，我们对六种基于规则的实体特定系统和三种最近利用预训练语言模型的神经方法进行了全面评估。我们的结果显示出一个复杂的情况，即神经方法无法在不同实体类型之间保持一致的表现，突显了需要进一步研究面向实体的模型的必要性。
link: http://arxiv.org/abs/2308.11537v1
publish_time: 2023-08-22

title: LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles
abstract: LLM经过不断的演变和完善，赋予了它们令人印象深刻的逻辑推理或纵向思维能力。但它们能够"跳出框框"吗？它们是否具备优秀的横向思维能力？我们提出了一种新颖的评估基准LatEval，通过Lateral Thinking Puzzles的设定，评估模型的横向思维能力在交互框架中。在我们的基准中，我们挑战LLM面对两个方面：模型提出的问题质量以及模型整合信息解决问题的能力。我们发现几乎所有LLM在交互过程中都难以运用横向思维。例如，即使是最先进的模型GPT-4，在一定程度上具备优势，但与人类相比仍存在明显差距。这一评估基准为LLM提供了一项极具挑战性和独特性的任务，对于创建一个有效的人工智能助手至关重要。
link: http://arxiv.org/abs/2308.10855v3
publish_time: 2023-08-21

title: LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models
abstract: 大型语言模型（LLM）的出现和被法律界采用引发了一个问题：LLM能执行哪些类型的法律推理？为了更好地研究这个问题，我们提出了LegalBench：一个由合作构建的法律推理基准，包括162个任务，涵盖六种不同类型的法律推理。LegalBench是通过跨学科过程构建的，我们收集了由法律专业人士设计和手工制作的任务。由于这些学科专家在构建中发挥了主导作用，任务要么衡量了实用的法律推理能力，要么衡量了律师们感兴趣的推理技能。为了促进跨学科对LLM在法律领域的讨论，我们还展示了描述法律推理的流行法律框架与LegalBench任务的对应关系，从而让律师和LLM开发者拥有共同的词汇表。本文描述了LegalBench，对20款开源和商业LLM进行了实证评估，并展示了LegalBench能够启发的研究探索类型。
link: http://arxiv.org/abs/2308.11462v1
publish_time: 2023-08-20

title: FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models
abstract: 大型语言模型(LLMs)在各种自然语言处理任务中表现出色，但它们在更具挑战性和特定领域任务中的有效性仍未被充分探索。该论文提出了FinEval，这是一个专门为LLMs中的金融领域知识而设计的基准。FinEval是一个包含金融、经济、会计和证书等高质量多项选择题的集合。它包括4,661个涵盖34个不同学术学科的问题。为了确保全面评估模型性能，FinEval采用了一系列提示类型，包括零-shot和少-shot提示，以及仅答案和思维链提示。在FinEval上评估最先进的中文和英文LLMs，结果表明只有GPT-4在不同提示设置下实现了接近70%的准确率，表明LLMs在金融领域知识中具有显着的增长潜力。我们的工作提供了一个更全面的金融知识评估基准，利用了模拟考试数据，并涵盖了广泛评估的LLMs。
link: http://arxiv.org/abs/2308.09975v1
publish_time: 2023-08-19

title: Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs
abstract: 大型语言模型（LLMs）具有丰富的知识，但这些知识可能随着时间而过时或不适用。因此，对LLMs进行知识编辑和评估其效果引起了越来越多的兴趣。现有研究主要集中在使用事实三元组进行知识编辑，但这不仅收集成本高，还难以表达复杂的事实。此外，这些研究在评估角度上也存在局限。本文提出了Eva-KELLM，一个用于评估LLMs知识编辑的新基准。该基准包括一个评估框架和相应的数据集。在我们的框架下，我们首先要求LLM使用原始文档进行知识编辑，这相对于使用事实三元组更加方便和通用。然后，我们从多个角度评估更新后的LLM。除了评估知识编辑的效果和传统研究中无关知识的保留情况外，我们还测试了LLM在两个方面的能力：1）推理修改后的知识，旨在使LLM真正学习修改后的知识而不仅仅是记住它。2）跨语言知识转移，更新为原始文档的LLM应能够处理来自另一种语言的查询。为促进进一步研究，我们构建并发布了相应的数据集。利用此基准，我们调查了几种常用的知识编辑方法的有效性。实验结果表明，当前使用原始文档进行知识编辑的方法在产生令人满意的结果方面并不有效，尤其是在处理修改后的知识推理和跨语言知识转移方面。
link: http://arxiv.org/abs/2308.09954v1
publish_time: 2023-08-19

title: EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding
abstract: EgoSchema是一个非常长形式的视频问答数据集，用于评估现代视觉和语言系统的长视频理解能力，并通过Ego4D获取。
link: http://arxiv.org/abs/2308.09126v1
publish_time: 2023-08-17

title: CMB: A Comprehensive Medical Benchmark in Chinese
abstract: 通过建立本土化的中文医学基准CMB，有望促进大型语言模型在医疗领域的普及和提升。
link: http://arxiv.org/abs/2308.08833v1
publish_time: 2023-08-17

title: Benchmarking Neural Network Generalization for Grammar Induction
abstract: 神经网络的泛化能力如何？我们提供了基于完全指定的形式语言的神经网络泛化度量，结果显示使用MDL目标训练的网络比使用标准损失函数训练的网络更好地泛化，并且需要更少的数据。
link: http://arxiv.org/abs/2308.08253v2
publish_time: 2023-08-16

title: VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use
abstract: VisIT-Bench是一个用于评估视觉语言模型在真实世界应用中遵循指示的基准测试，包含涵盖70个指令类型的数据集，旨在超越VQAv2和COCO等任务范围，并能够自动评估候选多模态生成的质量差距。
link: http://arxiv.org/abs/2308.06595v4
publish_time: 2023-08-12

title: EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis
