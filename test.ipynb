{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidURL",
     "evalue": "URL can't contain control characters. '/api/query?search_query=cat:nlp AND cat:cs.CL AND submittedDate:[2020-01-01 TO 2020-05-31] AND title:benchmark&start=0&max_results=10' (found at least ' ')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m arxiv_feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://export.arxiv.org/api/query?search_query=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&start=0&max_results=10\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 解析arXiv的数据源\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[43mfeedparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43marxiv_feed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 输出搜索结果\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mentries:\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/site-packages/feedparser/api.py:216\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, response_headers, resolve_relative_uris, sanitize_html)\u001b[0m\n\u001b[1;32m    208\u001b[0m result \u001b[39m=\u001b[39m FeedParserDict(\n\u001b[1;32m    209\u001b[0m     bozo\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m     entries\u001b[39m=\u001b[39m[],\n\u001b[1;32m    211\u001b[0m     feed\u001b[39m=\u001b[39mFeedParserDict(),\n\u001b[1;32m    212\u001b[0m     headers\u001b[39m=\u001b[39m{},\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     data \u001b[39m=\u001b[39m _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    218\u001b[0m     result\u001b[39m.\u001b[39mupdate({\n\u001b[1;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbozo\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    220\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbozo_exception\u001b[39m\u001b[39m'\u001b[39m: error,\n\u001b[1;32m    221\u001b[0m     })\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/site-packages/feedparser/api.py:115\u001b[0m, in \u001b[0;36m_open_resource\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m url_file_stream_or_string\u001b[39m.\u001b[39mread()\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(url_file_stream_or_string, \u001b[39mstr\u001b[39m) \\\n\u001b[1;32m    114\u001b[0m    \u001b[39mand\u001b[39;00m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39murlparse(url_file_stream_or_string)[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mftp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfeed\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m http\u001b[39m.\u001b[39;49mget(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\n\u001b[1;32m    117\u001b[0m \u001b[39m# try to open with native open function (if url_file_stream_or_string is a filename)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/site-packages/feedparser/http.py:171\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[1;32m    169\u001b[0m opener \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mbuild_opener(\u001b[39m*\u001b[39m\u001b[39mtuple\u001b[39m(handlers \u001b[39m+\u001b[39m [_FeedURLHandler()]))\n\u001b[1;32m    170\u001b[0m opener\u001b[39m.\u001b[39maddheaders \u001b[39m=\u001b[39m []  \u001b[39m# RMK - must clear so we only send our custom User-Agent\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m f \u001b[39m=\u001b[39m opener\u001b[39m.\u001b[39;49mopen(request)\n\u001b[1;32m    172\u001b[0m data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m    173\u001b[0m f\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/urllib/request.py:1377\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1377\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPConnection, req)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[1;32m   1281\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1282\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/http/client.py:1294\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maccept-encoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m header_names:\n\u001b[1;32m   1292\u001b[0m     skips[\u001b[39m'\u001b[39m\u001b[39mskip_accept_encoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1294\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mputrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mskips)\n\u001b[1;32m   1296\u001b[0m \u001b[39m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m# the caller passes encode_chunked=True or the following\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39m# conditions hold:\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m# 1. content-length has not been explicitly set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39m# 2. the body is a file or iterable, but not a str or bytes-like\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[39m# 3. Transfer-Encoding has NOT been explicitly set by the caller\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m header_names:\n\u001b[1;32m   1304\u001b[0m     \u001b[39m# only chunk body if not explicitly set for backwards\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[39m# compatibility, assuming the client code is already handling the\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     \u001b[39m# chunking\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/http/client.py:1128\u001b[0m, in \u001b[0;36mHTTPConnection.putrequest\u001b[0;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_method \u001b[39m=\u001b[39m method\n\u001b[1;32m   1127\u001b[0m url \u001b[39m=\u001b[39m url \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1128\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_path(url)\n\u001b[1;32m   1130\u001b[0m request \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (method, url, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn_str)\n\u001b[1;32m   1132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_request(request))\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek-moe/lib/python3.10/http/client.py:1228\u001b[0m, in \u001b[0;36mHTTPConnection._validate_path\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   1226\u001b[0m match \u001b[39m=\u001b[39m _contains_disallowed_url_pchar_re\u001b[39m.\u001b[39msearch(url)\n\u001b[1;32m   1227\u001b[0m \u001b[39mif\u001b[39;00m match:\n\u001b[0;32m-> 1228\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mURL can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain control characters. \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1229\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(found at least \u001b[39m\u001b[39m{\u001b[39;00mmatch\u001b[39m.\u001b[39mgroup()\u001b[39m!r}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidURL\u001b[0m: URL can't contain control characters. '/api/query?search_query=cat:nlp AND cat:cs.CL AND submittedDate:[2020-01-01 TO 2020-05-31] AND title:benchmark&start=0&max_results=10' (found at least ' ')"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "# 设置arXiv的查询参数\n",
    "query = 'cat:nlp AND cat:cs.CL AND submittedDate:[2020-01-01 TO 2020-05-31] AND title:benchmark'\n",
    "arxiv_feed = f'http://export.arxiv.org/api/query?search_query={query}&start=0&max_results=10'\n",
    "\n",
    "# 解析arXiv的数据源\n",
    "feed = feedparser.parse(arxiv_feed)\n",
    "\n",
    "# 输出搜索结果\n",
    "for entry in feed.entries:\n",
    "    print('Title:', entry.title)\n",
    "    print('Authors:', ', '.join(author.name for author in entry.authors))\n",
    "    print('Summary:', entry.summary)\n",
    "    print('Link:', entry.link)\n",
    "    print('Publish Date:', entry.published)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "import feedparser\n",
    "\n",
    "# Base api query url\n",
    "base_url = 'http://export.arxiv.org/api/query?';\n",
    "\n",
    "# Search parameters\n",
    "search_query = 'all:electron&cat:cs.CL'  # search for electron in all fields\n",
    "start = 0  # retreive the first 5 results\n",
    "max_results = 5\n",
    "\n",
    "query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                     start,\n",
    "                                                     max_results)\n",
    "\n",
    "# Opensearch metadata such as totalResults, startIndex,\n",
    "# and itemsPerPage live in the opensearch namespase.\n",
    "# Some entry metadata lives in the arXiv namespace.\n",
    "# This is a hack to expose both of these namespaces in\n",
    "# feedparser v4.1\n",
    "# feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "# feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "# perform a GET request using the base_url and query\n",
    "response = urllib.request.urlopen(base_url + query).read()\n",
    "print(\"respnse:\",response)\n",
    "# parse the response using feedparser\n",
    "feed = feedparser.parse(response)\n",
    "\n",
    "# print out feed information\n",
    "print('Feed title: %s' % feed.feed.title)\n",
    "print('Feed last updated: %s' % feed.feed.updated)\n",
    "\n",
    "# print opensearch metadata\n",
    "print('totalResults for this query: %s' % feed.feed.opensearch_totalresults)\n",
    "\n",
    "print('itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage)\n",
    "\n",
    "print('startIndex for this query: %s' % feed.feed.opensearch_startindex)\n",
    "\n",
    "\n",
    "# Run through each entry, and print out information\n",
    "for entry in feed.entries:\n",
    "    print('e-print metadata')\n",
    "\n",
    "    print('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "\n",
    "    print('Published: %s' % entry.published)\n",
    "\n",
    "    print('Title:  %s' % entry.title)\n",
    "\n",
    "\n",
    "    # feedparser v4.1 only grabs the first author\n",
    "    author_string = entry.author\n",
    "\n",
    "    # grab the affiliation in <arxiv:affiliation> if present\n",
    "    # - this will only grab the first affiliation encountered\n",
    "    #   (the first affiliation for the first author)\n",
    "    # Please email the list with a way to get all of this information!\n",
    "    try:\n",
    "        author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    print('Last Author:  %s' % author_string)\n",
    "\n",
    "\n",
    "    # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "    try:\n",
    "        print('Authors:  %s' % ', '.join(author.name for author in entry.authors))\n",
    "\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == 'alternate':\n",
    "            print('abs page link: %s' % link.href)\n",
    "\n",
    "        elif link.title == 'pdf':\n",
    "            print('pdf link: %s' % link.href)\n",
    "\n",
    "\n",
    "    # The journal reference, comments and primary_category sections live under\n",
    "    # the arxiv namespace\n",
    "    try:\n",
    "        journal_ref = entry.arxiv_journal_ref\n",
    "    except AttributeError:\n",
    "        journal_ref = 'No journal ref found'\n",
    "    print('Journal reference: %s' % journal_ref)\n",
    "\n",
    "\n",
    "    try:\n",
    "        comment = entry.arxiv_comment\n",
    "    except AttributeError:\n",
    "        comment = 'No comment found'\n",
    "    print('Comments: %s' % comment)\n",
    "\n",
    "\n",
    "    # Since the <arxiv:primary_category> element has no data, only\n",
    "    # attributes, feedparser does not store anything inside\n",
    "    # entry.arxiv_primary_category\n",
    "    # This is a dirty hack to get the primary_category, just take the\n",
    "    # first element in entry.tags.  If anyone knows a better way to do\n",
    "    # this, please email the list!\n",
    "    print('Primary Category: %s' % entry.tags[0]['term'])\n",
    "\n",
    "\n",
    "    # Lets get all the categories\n",
    "    all_categories = [t['term'] for t in entry.tags]\n",
    "    print('All Categories: %s' % (', ').join(all_categories))\n",
    "\n",
    "\n",
    "    # The abstract is in the <summary> element\n",
    "    print('Abstract: %s' % entry.summary)\n",
    "————————————————\n",
    "\n",
    "                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。\n",
    "                        \n",
    "原文链接：https://blog.csdn.net/ye6pipipihou/article/details/127170216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benchmarking Graph Data Management and Processing Systems: A Survey\n",
      "Authors: Toyotaro Suzumura\n",
      "Summary: The development of scalable, representative, and widely adopted benchmarks\n",
      "for graph data systems have been a question for which answers has been sought\n",
      "for decades. We conduct an in-depth study of the existing literature on\n",
      "benchmarks for graph data management and processing, covering 20 different\n",
      "benchmarks developed during the last 15 years. We categorize the benchmarks\n",
      "into three areas focusing on benchmarks for graph processing systems, graph\n",
      "database benchmarks, and bigdata benchmarks with graph processing workloads.\n",
      "This systematic approach allows us to identify multiple issues existing in this\n",
      "area, including i) few benchmarks exist which can produce high workload\n",
      "scenarios, ii) no significant work done on benchmarking graph stream processing\n",
      "as well as graph based machine learning, iii) benchmarks tend to use\n",
      "conventional metrics despite new meaningful metrics have been around for years,\n",
      "iv) increasing number of big data benchmarks appear with graph processing\n",
      "workloads. Following these observations, we conclude the survey by describing\n",
      "key challenges for future research on graph data systems benchmarking.\n",
      "Published Date: 2020-05-26T17:07:29Z\n",
      "arXiv Link: http://arxiv.org/abs/2005.12873v4\n",
      "\n",
      "Title: AIBench: An Agile Domain-specific Benchmarking Methodology and an AI\n",
      "  Benchmark Suite\n",
      "Authors: Hainan Ye\n",
      "Summary: Domain-specific software and hardware co-design is encouraging as it is much\n",
      "easier to achieve efficiency for fewer tasks. Agile domain-specific\n",
      "benchmarking speeds up the process as it provides not only relevant design\n",
      "inputs but also relevant metrics, and tools. Unfortunately, modern workloads\n",
      "like Big data, AI, and Internet services dwarf the traditional one in terms of\n",
      "code size, deployment scale, and execution path, and hence raise serious\n",
      "benchmarking challenges.\n",
      "  This paper proposes an agile domain-specific benchmarking methodology.\n",
      "Together with seventeen industry partners, we identify ten important end-to-end\n",
      "application scenarios, among which sixteen representative AI tasks are\n",
      "distilled as the AI component benchmarks. We propose the permutations of\n",
      "essential AI and non-AI component benchmarks as end-to-end benchmarks. An\n",
      "end-to-end benchmark is a distillation of the essential attributes of an\n",
      "industry-scale application. We design and implement a highly extensible,\n",
      "configurable, and flexible benchmark framework, on the basis of which, we\n",
      "propose the guideline for building end-to-end benchmarks, and present the first\n",
      "end-to-end Internet service AI benchmark.\n",
      "  The preliminary evaluation shows the value of our benchmark suite---AIBench\n",
      "against MLPerf and TailBench for hardware and software designers,\n",
      "micro-architectural researchers, and code developers. The specifications,\n",
      "source code, testbed, and results are publicly available from the web site\n",
      "\\url{http://www.benchcouncil.org/AIBench/index.html}.\n",
      "Published Date: 2020-02-17T07:29:05Z\n",
      "arXiv Link: http://arxiv.org/abs/2002.07162v1\n",
      "\n",
      "Title: ESBM: An Entity Summarization BenchMark\n",
      "Authors: Yuzhong Qu\n",
      "Summary: Entity summarization is the problem of computing an optimal compact summary\n",
      "for an entity by selecting a size-constrained subset of triples from RDF data.\n",
      "Entity summarization supports a multiplicity of applications and has led to\n",
      "fruitful research. However, there is a lack of evaluation efforts that cover\n",
      "the broad spectrum of existing systems. One reason is a lack of benchmarks for\n",
      "evaluation. Some benchmarks are no longer available, while others are small and\n",
      "have limitations. In this paper, we create an Entity Summarization BenchMark\n",
      "(ESBM) which overcomes the limitations of existing benchmarks and meets\n",
      "standard desiderata for a benchmark. Using this largest available benchmark for\n",
      "evaluating general-purpose entity summarizers, we perform the most extensive\n",
      "experiment to date where 9~existing systems are compared. Considering that all\n",
      "of these systems are unsupervised, we also implement and evaluate a supervised\n",
      "learning based system for reference.\n",
      "Published Date: 2020-03-08T07:12:20Z\n",
      "arXiv Link: http://arxiv.org/abs/2003.03734v1\n",
      "\n",
      "Title: Benchmarking Unsupervised Outlier Detection with Realistic Synthetic\n",
      "  Data\n",
      "Authors: Klemens Böhm\n",
      "Summary: Benchmarking unsupervised outlier detection is difficult. Outliers are rare,\n",
      "and existing benchmark data contains outliers with various and unknown\n",
      "characteristics. Fully synthetic data usually consists of outliers and regular\n",
      "instance with clear characteristics and thus allows for a more meaningful\n",
      "evaluation of detection methods in principle. Nonetheless, there have only been\n",
      "few attempts to include synthetic data in benchmarks for outlier detection.\n",
      "This might be due to the imprecise notion of outliers or to the difficulty to\n",
      "arrive at a good coverage of different domains with synthetic data. In this\n",
      "work we propose a generic process for the generation of data sets for such\n",
      "benchmarking. The core idea is to reconstruct regular instances from existing\n",
      "real-world benchmark data while generating outliers so that they exhibit\n",
      "insightful characteristics. This allows both for a good coverage of domains and\n",
      "for helpful interpretations of results. We also describe three instantiations\n",
      "of the generic process that generate outliers with specific characteristics,\n",
      "like local outliers. A benchmark with state-of-the-art detection methods\n",
      "confirms that our generic process is indeed practical.\n",
      "Published Date: 2020-04-15T08:55:47Z\n",
      "arXiv Link: http://arxiv.org/abs/2004.06947v1\n",
      "\n",
      "Title: Decisions and Performance Under Bounded Rationality: A Computational\n",
      "  Benchmarking Approach\n",
      "Authors: Anthony Strittmatter\n",
      "Summary: This paper presents a novel approach to analyze human decision-making that\n",
      "involves comparing the behavior of professional chess players relative to a\n",
      "computational benchmark of cognitively bounded rationality. This benchmark is\n",
      "constructed using algorithms of modern chess engines and allows investigating\n",
      "behavior at the level of individual move-by-move observations, thus\n",
      "representing a natural benchmark for computationally bounded optimization. The\n",
      "analysis delivers novel insights by isolating deviations from this benchmark of\n",
      "bounded rationality as well as their causes and consequences for performance.\n",
      "The findings document the existence of several distinct dimensions of\n",
      "behavioral deviations, which are related to asymmetric positional evaluation in\n",
      "terms of losses and gains, time pressure, fatigue, and complexity. The results\n",
      "also document that deviations from the benchmark do not necessarily entail\n",
      "worse performance. Faster decisions are associated with more frequent\n",
      "deviations from the benchmark, yet they are also associated with better\n",
      "performance. The findings are consistent with an important influence of\n",
      "intuition and experience, thereby shedding new light on the recent debate about\n",
      "computational rationality in cognitive processes.\n",
      "Published Date: 2020-05-26T11:39:39Z\n",
      "arXiv Link: http://arxiv.org/abs/2005.12638v2\n",
      "\n",
      "Title: AIBench Scenario: Scenario-distilling AI Benchmarking\n",
      "Authors: Zihan Jiang\n",
      "Summary: Modern real-world application scenarios like Internet services consist of a\n",
      "diversity of AI and non-AI modules with huge code sizes and long and\n",
      "complicated execution paths, which raises serious benchmarking or evaluating\n",
      "challenges. Using AI components or micro benchmarks alone can lead to\n",
      "error-prone conclusions. This paper presents a methodology to attack the above\n",
      "challenge. We formalize a real-world application scenario as a Directed Acyclic\n",
      "Graph-based model and propose the rules to distill it into a permutation of\n",
      "essential AI and non-AI tasks, which we call a scenario benchmark. Together\n",
      "with seventeen industry partners, we extract nine typical scenario benchmarks.\n",
      "We design and implement an extensible, configurable, and flexible benchmark\n",
      "framework. We implement two Internet service AI scenario benchmarks based on\n",
      "the framework as proxies to two real-world application scenarios. We consider\n",
      "scenario, component, and micro benchmarks as three indispensable parts for\n",
      "evaluating. Our evaluation shows the advantage of our methodology against using\n",
      "component or micro AI benchmarks alone. The specifications, source code,\n",
      "testbed, and results are publicly available from\n",
      "\\url{https://www.benchcouncil.org/aibench/scenario/}.\n",
      "Published Date: 2020-05-06T01:24:25Z\n",
      "arXiv Link: http://arxiv.org/abs/2005.03459v4\n",
      "\n",
      "Title: An Empirical Study on Benchmarks of Artificial Software Vulnerabilities\n",
      "Authors: Bing Mao\n",
      "Summary: Recently, various techniques (e.g., fuzzing) have been developed for\n",
      "vulnerability detection. To evaluate those techniques, the community has been\n",
      "developing benchmarks of artificial vulnerabilities because of a shortage of\n",
      "ground-truth. However, people have concerns that such vulnerabilities cannot\n",
      "represent reality and may lead to unreliable and misleading results.\n",
      "Unfortunately, there lacks research on handling such concerns.\n",
      "  In this work, to understand how close these benchmarks mirror reality, we\n",
      "perform an empirical study on three artificial vulnerability benchmarks -\n",
      "LAVA-M, Rode0day and CGC (2669 bugs) and various real-world memory-corruption\n",
      "vulnerabilities (80 CVEs). Furthermore, we propose a model to depict the\n",
      "properties of memory-corruption vulnerabilities. Following this model, we\n",
      "conduct intensive experiments and data analyses. Our analytic results reveal\n",
      "that while artificial benchmarks attempt to approach the real world, they still\n",
      "significantly differ from reality. Based on the findings, we propose a set of\n",
      "strategies to improve the quality of artificial benchmarks.\n",
      "Published Date: 2020-03-21T02:40:44Z\n",
      "arXiv Link: http://arxiv.org/abs/2003.09561v1\n",
      "\n",
      "Title: Benchmark Design and Prior-independent Optimization\n",
      "Authors: Yingkai Li\n",
      "Summary: This paper compares two leading approaches for robust optimization in the\n",
      "models of online algorithms and mechanism design. Competitive analysis compares\n",
      "the performance of an online algorithm to an offline benchmark in worst-case\n",
      "over inputs, and prior-independent mechanism design compares the expected\n",
      "performance of a mechanism on an unknown distribution (of inputs, i.e., agent\n",
      "values) to the optimal mechanism for the distribution in worst case over\n",
      "distributions. For competitive analysis, a critical concern is the choice of\n",
      "benchmark. This paper gives a method for selecting a good benchmark. We show\n",
      "that optimal algorithm/mechanism for the optimal benchmark are equal to the\n",
      "prior-independent optimal algorithm/mechanism.\n",
      "  We solve a central open question in prior-independent mechanism design,\n",
      "namely we identify the prior-independent revenue-optimal mechanism for selling\n",
      "a single item to two agents with i.i.d. and regularly distributed values. Via\n",
      "this solution and the above equivalence of prior-independent mechanism design\n",
      "and competitive analysis (a.k.a. prior-free mechanism design) we show that the\n",
      "standard method for lower bounds of prior-free mechanisms is not generally\n",
      "tight for the benchmark design program.\n",
      "Published Date: 2020-01-28T04:06:00Z\n",
      "arXiv Link: http://arxiv.org/abs/2001.10157v2\n",
      "\n",
      "Title: SHOP-VRB: A Visual Reasoning Benchmark for Object Perception\n",
      "Authors: Krystian Mikolajczyk\n",
      "Summary: In this paper we present an approach and a benchmark for visual reasoning in\n",
      "robotics applications, in particular small object grasping and manipulation.\n",
      "The approach and benchmark are focused on inferring object properties from\n",
      "visual and text data. It concerns small household objects with their\n",
      "properties, functionality, natural language descriptions as well as\n",
      "question-answer pairs for visual reasoning queries along with their\n",
      "corresponding scene semantic representations. We also present a method for\n",
      "generating synthetic data which allows to extend the benchmark to other objects\n",
      "or scenes and propose an evaluation protocol that is more challenging than in\n",
      "the existing datasets. We propose a reasoning system based on symbolic program\n",
      "execution. A disentangled representation of the visual and textual inputs is\n",
      "obtained and used to execute symbolic programs that represent a 'reasoning\n",
      "process' of the algorithm. We perform a set of experiments on the proposed\n",
      "benchmark and compare to results for the state of the art methods. These\n",
      "results expose the shortcomings of the existing benchmarks that may lead to\n",
      "misleading conclusions on the actual performance of the visual reasoning\n",
      "systems.\n",
      "Published Date: 2020-04-06T13:46:54Z\n",
      "arXiv Link: http://arxiv.org/abs/2004.02673v1\n",
      "\n",
      "Title: Towards Realistic Optimization Benchmarks: A Questionnaire on the\n",
      "  Properties of Real-World Problems\n",
      "Authors: Boris Naujoks\n",
      "Summary: Benchmarks are a useful tool for empirical performance comparisons. However,\n",
      "one of the main shortcomings of existing benchmarks is that it remains largely\n",
      "unclear how they relate to real-world problems. What does an algorithm's\n",
      "performance on a benchmark say about its potential on a specific real-world\n",
      "problem? This work aims to identify properties of real-world problems through a\n",
      "questionnaire on real-world single-, multi-, and many-objective optimization\n",
      "problems. Based on initial responses, a few challenges that have to be\n",
      "considered in the design of realistic benchmarks can already be identified. A\n",
      "key point for future work is to gather more responses to the questionnaire to\n",
      "allow an analysis of common combinations of properties. In turn, such common\n",
      "combinations can then be included in improved benchmark suites. To gather more\n",
      "data, the reader is invited to participate in the questionnaire at:\n",
      "https://tinyurl.com/opt-survey\n",
      "Published Date: 2020-04-14T10:04:38Z\n",
      "arXiv Link: http://arxiv.org/abs/2004.06395v1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "# 定义搜索关键词\n",
    "query = 'benchmark'\n",
    "\n",
    "# 定义arXiv搜索的URL\n",
    "url = f'http://export.arxiv.org/api/query?search_query=all:{query}+AND+submittedDate:[20200101+TO+20200531]&start=0&max_results=10'\n",
    "\n",
    "# 使用feedparser库解析URL\n",
    "feed = feedparser.parse(url)\n",
    "\n",
    "# 输出搜索结果\n",
    "for entry in feed.entries:\n",
    "    print(\"Title:\", entry.title)\n",
    "    print(\"Authors:\", entry.author)\n",
    "    print(\"Summary:\", entry.summary)\n",
    "    print(\"Published Date:\", entry.published)\n",
    "    print(\"arXiv Link:\", entry.link)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('deepseek-moe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6b80dcee1fdfa01b80b2dd44a21962caf8db036253777b45ed91dbd77686682"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
